{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.png","path":"favicon.png","modified":0,"renderable":0},{"_id":"source/avatar/avatar.png","path":"avatar/avatar.png","modified":0,"renderable":0},{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"45a10314948be16dc2bf219164fcf74e723394bc","modified":1466517506000},{"_id":"source/404.html","hash":"310fc37341c13586d09c4d1d82ca51ed082d6481","modified":1463216861000},{"_id":"source/favicon.png","hash":"3ab3634e3d2820eef373319e07364263269cb2d5","modified":1463216893000},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1463214729000},{"_id":"themes/landscape/.npmignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1463218758000},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1463218758000},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1463218758000},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1463218758000},{"_id":"themes/landscape/_config.yml","hash":"fb8c98a0f6ff9f962637f329c22699721854cd73","modified":1463218758000},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1463218758000},{"_id":"source/_drafts/tags","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1463216855000},{"_id":"source/_drafts/.DS_Store","hash":"7e3834049503a25e55ce0f220d92f67630a42534","modified":1466428771000},{"_id":"source/_drafts/captcha.md","hash":"8c3fef56d8f7827d10314b2eec7be0b1a3356e3a","modified":1466519842000},{"_id":"source/_drafts/esl-2a.md","hash":"6823e99bce785c5e30678af6d2c3bffdeb68c5b9","modified":1466519845000},{"_id":"source/_drafts/wishful-thinking.md","hash":"546f3f1f2b989e1744aa39bc945e0c9b67a653dd","modified":1466609807000},{"_id":"source/_drafts/机器学习概念类比.md","hash":"8d56a2351508494cc3b2d117f73987ba68bb5019","modified":1466519851000},{"_id":"source/_posts/.DS_Store","hash":"c198a79ec657eaa667b17119af0d606ae6f69247","modified":1466517573000},{"_id":"source/_posts/Ein-Eout.md","hash":"8eeb0398eed68e5bfd5380efa90f527502a3a459","modified":1463768050000},{"_id":"source/_posts/complex.md","hash":"f78cf2559f41fb6bbd9403dd55854ed01152ca3e","modified":1463768070000},{"_id":"source/_posts/convolution.md","hash":"79966736a8def138f06548a60b4dcaee990eb13b","modified":1463768061000},{"_id":"source/_posts/gabor.md","hash":"75c08264970eb1a31c39f42afcfdcc2afed41438","modified":1463768080000},{"_id":"source/_posts/linear-algebra-1.md","hash":"cffa6133aa1a4c0be59208549afceb23bb55d001","modified":1463801818000},{"_id":"source/_posts/linear-algebra-2.md","hash":"d6533d1949804a177f199f40ab7f8a0ffab5bd77","modified":1463840393000},{"_id":"source/_posts/linear-algebra-3.md","hash":"cf87ff646eea18a47233449354e32597161999f1","modified":1466522145000},{"_id":"source/_posts/linear-algebra-4.md","hash":"0ce53842abdda162fc9b4bfb5f3811f37ecbe6d2","modified":1466505654000},{"_id":"source/_posts/linear-algebra-5.md","hash":"0d2d6c5c4ad1ff738fc7d99f964ec62661263b88","modified":1466592313000},{"_id":"source/_posts/logistic-regression.md","hash":"df2b77e90f480403e5b68888b3526ecec5c84e98","modified":1463768088000},{"_id":"source/_posts/tags","hash":"4d8bacf533fcd2ae84f1ace6434108ea493d45af","modified":1444317119000},{"_id":"source/_posts/think-statistics-note.md","hash":"734434d5886fdf91d0b57ef79a9d0384533d30c2","modified":1463768095000},{"_id":"source/categories/index.md","hash":"5da51fab93dd9d5e69e2e6634cdb8c56ae15780d","modified":1463216872000},{"_id":"source/avatar/.DS_Store","hash":"ed7655b20c030d5b28ad422a197250e312452e1d","modified":1463216843000},{"_id":"source/images/.DS_Store","hash":"7f268512caa87cd33449d1c52a0901f5bb1e26c3","modified":1463216885000},{"_id":"source/tags/.DS_Store","hash":"aee7081639622d40d0bab03149aab01ecdedc505","modified":1466428678000},{"_id":"source/tags/index.md","hash":"4832992dbae570198ba8493a77751427ed9cd035","modified":1463216879000},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1463218758000},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1463218758000},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1463214729000},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1463218758000},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1463218758000},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1463218758000},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1463218758000},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1463218758000},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1463218758000},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1463218758000},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1463218758000},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1463218758000},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1463218758000},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1463218758000},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1463218758000},{"_id":"source/_drafts/captcha/binary.png","hash":"3d8dbdeb3db483d86579e22832b83753c9d45b34","modified":1463216885000},{"_id":"source/_drafts/captcha/contour.png","hash":"82633e86612aef7084f9221635bdbdfd4a876b4e","modified":1463216885000},{"_id":"source/_drafts/captcha/edge.png","hash":"dff118a972976d3fb59d7cd0b478c508b318c7ea","modified":1463216885000},{"_id":"source/_drafts/captcha/grayscale.png","hash":"6dd4c0ae0adfdacd4ff5afad6ea3d12d03d811b9","modified":1463216885000},{"_id":"source/_drafts/captcha/hough-line.png","hash":"d45e565707289c37e4bed1ac6d1473e26d40d27c","modified":1463216885000},{"_id":"source/_drafts/captcha/hough.png","hash":"54e1fb4e8e323553c3991a54bdc7fead210f9555","modified":1463216885000},{"_id":"source/_drafts/captcha/kmeans.png","hash":"eae66e922b577ac0a2a207ffae3ec42c79950795","modified":1463216885000},{"_id":"source/_drafts/captcha/lsd-line.png","hash":"7b6b49621720cd7ea273715e1e459e7aeb1f3c95","modified":1463216885000},{"_id":"source/_drafts/captcha/lsd-post.png","hash":"88d33e105f26a3bb75d409a25ece3414c75f66c0","modified":1463216885000},{"_id":"source/_drafts/captcha/lsd-pre.png","hash":"f4fbd35d889dd4146cbcebbc7da0cdf3715f8568","modified":1463216885000},{"_id":"source/_drafts/captcha/median-contour.png","hash":"2bdff07582ef5ff72ea3fceb966a0db040f47e8d","modified":1463216885000},{"_id":"source/_drafts/captcha/median.png","hash":"1f1d5880b10b4fced82092021690d0538c19b4d2","modified":1463216885000},{"_id":"source/_drafts/captcha/region-grow.png","hash":"91d2c2ace86c3ef3dea181efc6c1097b027854c7","modified":1463216885000},{"_id":"source/_drafts/captcha/sample.jpg","hash":"d717ebcfa01468ec2d72245fb857bcf463958cae","modified":1463216885000},{"_id":"source/_posts/complex/complex-plane.png","hash":"37c1f7fa4e367a309c4e8a2c1da2b67087ad8322","modified":1442370925000},{"_id":"source/_posts/complex/complex-polar-plane.png","hash":"c4d0670923007855080409a05f269a4504e7bf32","modified":1444109229000},{"_id":"source/_posts/complex/complex-polar-transform.png","hash":"d79c29c88744af188546615312c0bcd14a39d477","modified":1444109870000},{"_id":"source/_posts/complex/real-wave-complex-wave.png","hash":"84783ed468776e1900219fbea57f05cf4e65d2c6","modified":1442370924000},{"_id":"source/_posts/convolution/.DS_Store","hash":"40de575227a91622bb10d8acbd59b5e2dfacac02","modified":1463239183000},{"_id":"source/_posts/convolution/conv-effect-day1.png","hash":"b02983a82fabafb55e2f3db82162bb1effb25edd","modified":1444233030000},{"_id":"source/_posts/convolution/conv-effect-day2.png","hash":"0f866382a23216f85d58aab2cd53079d25e3d582","modified":1444233038000},{"_id":"source/_posts/convolution/conv-effect-day3.png","hash":"5a069afbbe380e8c2d7b5fb064b8e4254059dc9e","modified":1444233046000},{"_id":"source/_posts/convolution/conv-effect-function.png","hash":"f4b237d53344de1836d5cfbe454a3fbe59a24c97","modified":1444215478000},{"_id":"source/_posts/gabor/.DS_Store","hash":"d08ee26404b5143abc243792660274963bfdffb9","modified":1466431355000},{"_id":"source/_posts/gabor/2d-gaussian.png","hash":"5a32a61b1b8c6a1c13737be9c9894509067e9cf2","modified":1444489088000},{"_id":"source/_posts/gabor/gabor-filter-banks.png","hash":"1f2e5ebedec061ce27e25e2318413434a3793442","modified":1444553566000},{"_id":"source/_posts/gabor/gabor-filter-frequency.png","hash":"725f6129c7e23d146d7743cdc04eca20e238dffd","modified":1444548697000},{"_id":"source/_posts/linear-algebra-1/.DS_Store","hash":"0e6527f024c13145b83fd16a1411bbd90ccf8118","modified":1463239292000},{"_id":"source/_posts/linear-algebra-2/.DS_Store","hash":"e8da3f08aa072a77e2ff6166968cf985cf7b9c02","modified":1463239301000},{"_id":"source/_posts/linear-algebra-2/r_less_than_n.png","hash":"b6783a84a0fd330c33d92bb3be8e86ddb4cc06ce","modified":1463150289000},{"_id":"source/_posts/linear-algebra-3/linear-transformation.png","hash":"99e973a90ca32a614c30ee35939f16410825d8e4","modified":1463767353000},{"_id":"source/_posts/linear-algebra-3/rotation.png","hash":"748bee183a6a36d11e70d36e387504b110b91bc6","modified":1463767353000},{"_id":"source/_posts/linear-algebra-3/uniqueness.png","hash":"cc4e277091d4a108caeae3112079a6f6bfec8d60","modified":1463767353000},{"_id":"source/_posts/linear-algebra-3/existence.png","hash":"e7eff385c02952a99190a1e7a44bb5c1d9835aaf","modified":1463767353000},{"_id":"source/_posts/linear-algebra-4/.DS_Store","hash":"0e6527f024c13145b83fd16a1411bbd90ccf8118","modified":1466354164000},{"_id":"source/_posts/linear-algebra-4/banner.jpeg","hash":"e4e399b7a53e7904ab7bd66223185f713ea3b668","modified":1466354391000},{"_id":"source/_posts/logistic-regression/Logistic-curve.png","hash":"c4a6c61c2b3411a9cd4d428203fa1058b9751450","modified":1463216885000},{"_id":"source/_posts/think-statistics-note/nonlinear-correlation-example.png","hash":"0174a52b9a31028a8118d7420f18a82f07992055","modified":1463216885000},{"_id":"source/_posts/think-statistics-note/pmf-cdf-pdf.jpg","hash":"df6e750e5d601e01a8e0d7b6ba4960773bf4f1d7","modified":1463216885000},{"_id":"source/avatar/avatar.png","hash":"894829df3059055b41c6a72283de4fa3f9fca25e","modified":1463216843000},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"82a30f81c0e8ba4a8af17acd6cc99e93834e4d5e","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"4fe8853e864d192701c03e5cd3a5390287b90612","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"c21ca56f419d01a9f49c27b6be9f4a98402b2aa3","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1463218758000},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1463218758000},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1463218758000},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1463218758000},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1463218758000},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1463218758000},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1463218758000},{"_id":"themes/landscape/source/css/_variables.styl","hash":"5e37a6571caf87149af83ac1cc0cdef99f117350","modified":1463218758000},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1463218758000},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1463218758000},{"_id":"source/_posts/convolution/2d-convolution.png","hash":"122f269ae61afec4935ba6c002f9b3f0dc933886","modified":1444230036000},{"_id":"source/_posts/gabor/gabor-filter-spatial.png","hash":"16ce8914de10f9e18c328a0842ee15588239684c","modified":1444553865000},{"_id":"source/_posts/linear-algebra-5/banner.jpeg","hash":"7cdb352f6daf0a76837cb65a25dc3b3fdc012b9b","modified":1466589180000},{"_id":"source/_posts/logistic-regression/convex-function.png","hash":"b81ac50f451e2a969f0652c2ddde699439331bba","modified":1463216885000},{"_id":"source/_posts/logistic-regression/cost-function.png","hash":"d7cb799bbd2e626e4544cf48a2163cb80a766843","modified":1463216885000},{"_id":"source/_drafts/captcha/lsd-line-remove.png","hash":"86e7491df22dd62ca20f9ad398a8e81a1ba4a8e9","modified":1463216885000},{"_id":"source/_drafts/captcha/mmedian-width-threshold.png","hash":"f66bc3551d471b8a02d610de2c44e842ee884c8d","modified":1463216885000},{"_id":"source/_posts/linear-algebra-1/banner.jpeg","hash":"bfdc469c70fb16c13d5f021211c31c58e1399ae7","modified":1463220909000},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1463218758000},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"36eefe6332b86b66023a9884b754d305235846b4","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1463218758000},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1463218758000},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1463218758000},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1463218758000},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1463218758000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1463218758000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1463218758000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1463218758000},{"_id":"source/_posts/convolution/banner.jpeg","hash":"b57aa630a2c4ed096c3fbffd65d9a7b37a3ec3ab","modified":1463239182000},{"_id":"source/_posts/linear-algebra-2/banner.jpeg","hash":"aa5f2295fce615840f1b881e1bded02a14bdd966","modified":1463239311000},{"_id":"source/_posts/linear-algebra-3/banner.jpeg","hash":"d3613c09999df345fa932c8dbca632f13f65d910","modified":1463767787000},{"_id":"source/_drafts/captcha/batch-contour.png","hash":"e79b2b223395995033c6404552779d858d6ec2ac","modified":1463216885000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1463218758000},{"_id":"source/_drafts/captcha/batch-median-contour.png","hash":"bff9ccf0b2ecc1ed206081b5999db0b53f866a4a","modified":1463216885000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1463218758000},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1463218758000},{"_id":"source/_posts/convolution/banner副本.jpeg","hash":"169a0c92e60167ee471c5e93b074682157fcc1c9","modified":1463239157000},{"_id":"source/_posts/gabor/banner.jpeg","hash":"9d68719ba74522370ba385c3af56ae9bb30dddcf","modified":1463239209000},{"_id":"source/_drafts/captcha/samples.png","hash":"3f7fd4d2405f769bd4a1376c4ee8ef8cc9033e6c","modified":1463216885000}],"Category":[{"name":"机器学习","_id":"cipr1ji7h00076jodzz5x0j46"},{"name":"programming","_id":"cipr1ji84000c6jodnroyzfv2"},{"name":"随笔","_id":"cipr1ji8c000l6jodlpknwytw"},{"name":"图像处理","_id":"cipr1ji8i000q6jodpsvkvt7k"},{"name":"线性代数","_id":"cipr1ji8u00176jodlrryuoeu"},{"name":"statistics","_id":"cipr1ji94001s6jod62i5s9vd"}],"Data":[],"Page":[{"_content":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n</head>\n<body>\n\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\"\n        charset=\"utf-8\" homePageUrl=\"your site url \"\n        homePageName=\"回到我的主页\">\n</script>\n\n</body>\n</html>\n","source":"404.html","raw":"<!DOCTYPE HTML>\n<html>\n<head>\n  <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/>\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n  <meta name=\"robots\" content=\"all\" />\n  <meta name=\"robots\" content=\"index,follow\"/>\n</head>\n<body>\n\n<script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\"\n        charset=\"utf-8\" homePageUrl=\"your site url \"\n        homePageName=\"回到我的主页\">\n</script>\n\n</body>\n</html>\n","date":"2016-05-14T09:07:41.000Z","updated":"2016-05-14T09:07:41.000Z","path":"404.html","title":"","comments":1,"layout":"page","_id":"cipr1ji5f00006jod5lq08omx"},{"title":"categories","date":"2016-05-07T04:52:22.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"title: categories\ndate: 2016-05-07 12:52:22\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-05-14T09:07:52.000Z","path":"categories/index.html","layout":"page","_id":"cipr1ji7700026joduxyzfsnb"},{"title":"tags","date":"2016-05-07T04:50:45.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"title: tags\ndate: 2016-05-07 12:50:45\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-05-14T09:07:59.000Z","path":"tags/index.html","layout":"page","_id":"cipr1ji7b00046jodio04uf9d"}],"Post":[{"title":"captcha","mathjax":true,"_content":"# 分析验证码样本图片\n从工商系统获取到22515张图片，经测量没有出现重复样本，推测验证码应该是由服务器即时生成的随机图片，没有用缓存策略。\n![验证码样本](/img/captcha/samples.png)\n## 概览所有样本\n分析发现，验证码样本可分为两类：成语类和运算类。二者分别有以下特征：\n- 成语类：\n    - 4 个字符\n    - 每个字符约为30像素 x 30像素大小\n    - 出现的字符不固定，总集合较大，几乎涵盖所有中文成语中出现的字符\n- 运算类\n    - 5 个字符，每个字符约为26像素x26像素大小。\n    - 每个字符\n    - 有一定模式，出现的字符集合确定\n    - 共有三种模式：“N加N等于”，“N减N等于”，“N乘N等于”，\n    其中 N $\\in$ {“零”，“壹”，“贰”，“叁”，“肆”，“伍”，“陆”，“柒”，“捌”，“玖”}\n\n经以上分析，发现“运算类”验证码识别目标较为确定，便于标注训练及验证分类效果，因此主要考虑识别此类验证码。\n\n## 分析单个样本\n![单个运算类验证码样本](/img/captcha/sample.jpg)\n上图为某“运算类”验证码样本，对其进一步分析发现验证码图片有以下几个特点：\n1. 杂点噪声\n    图片背景分布有随机椒盐噪声\n2. 线段噪声\n    图片前景分布有随机方向、随机长度、随机颜色的线段噪声。注意到线段是遮盖在文字上方的，且线段颜色和文字颜色相差并不大。\n3. 颜色对比度较低\n\t前景和背景的且颜色的对比度较低，前景和背景颜色都很暗淡，区分度不高。而且放大图片后发现，文字周围还有一圈“红晕”。\n\n# 第一次实验\n\n## 总体思路\n{% plantuml %}\n(*) -right--> \"下载验证码\"\n-right--> \"去除线段\"\n-right--> \"调用tesseract进行字符识别\"\n{% endplantuml %}\n\n## 实验总览\n{% plantuml %}\nstate \"灰度化\" as grayscale\nstate \"二值化\" as binary\n\n[*] -right--> grayscale\ngrayscale -right--> binary \nbinary -right--> edge\nbinary -down--> line\n\n[*] -down--> cluster\n\nstate \"边缘检测\" as edge {\nstate \"canny 检测边缘\" as canny\n[*] --> canny\n}\n\nstate \"检测线段\" as line {\n\nstate \"霍夫变换\" as hough\nstate \"轮廓模型\" as contour\nstate \"垂直直方图投影\" as vhist\nstate \"分割字符\" as segment\nstate \"中值滤波\" as median\nstate \"LSD 线段检测\" as lsd\n[*] --> hough\n\n[*] --> contour\ncontour --> vhist\nvhist --> segment\n\n[*] --> median\nmedian --> hough\n\n[*] --> median\nmedian --> contour\n\n[*] --> median \nmedian --> median : 两次中值滤波\nmedian --> vhist\n\n[*] --> lsd\n}\n\nstate \"颜色聚类\" as cluster {\n\nstate \"K-means 聚类\" as kmeans\nstate \"区域生长法\" as regiongrow\n[*] --> kmeans\n\n[*] --> regiongrow\n}\n\n{% endplantuml %}\n\n## 预处理及分割尝试\n首先，转换样本图片为灰度图片：\n![灰度处理](/img/captcha/grayscale.png)\n再通过大津法（Ostu）将图片二值化\n![二值化（大津法）](/img/captcha/binary.png)\n\n### 检测边缘\n对图片用canny算子检测边缘：\n![检测边缘](/img/captcha/edge.png)\n这个方法可以去除杂点，然而，被线段干扰较大\n\n### 去除线段\n#### 霍夫变换\n直接使用matlab vision包提供的HoughTransform检测线段，并在原图中，将检测的线段用白色覆盖，得到的结果如下：\n检测到的线段：\n![霍夫变换检测线段](/img/captcha/hough-line.png)\n![霍夫变换去除线段](/img/captcha/hough.png)\n\n#### 轮廓模型（蛇模型）+垂直直方图投影\n使用matlab中的activecontour函数检测字符轮廓，从而滤除线段，得到的结果如下：\n![蛇模型检测轮廓](/img/captcha/contour.png)\n注意，此方法要求先给定字符所在位置。\n\n批量应用轮廓模型分割字符位置结果：\n![批量应用轮廓模型分割字符位置结果](/img/captcha/batch-contour.png)\n\n#### 中值滤波\n考虑到杂点和杂线宽度较窄，使用中值滤波试图过滤掉杂点和杂线：\n![中值滤波结果](/img/captcha/median.png)\n\n##### 中值滤波+轮廓模型+垂直直方图投影\n先通过中值滤波去除杂点，同时得到较为“干净”的图片，之后用干净图片作为mask，应用activecontour函数检测字符轮廓，最后，用垂直直方图投影确定字符位置。得到结果如下：\n![中值滤波+轮廓模型](/img/captcha/median-contour.png)\n\n批量应用结果：\n![批量中值滤波+轮廓模型](/img/captcha/batch-median-contour.png)\n\n##### 两次中值滤波+垂直直方图投影+宽度阈值\n连续应用两次中值滤波，通过垂直直方图投影确定字符位置，最后用宽度阈值过滤，得到有效图像，结果如下：\n![两次中值滤波+垂直直方图投影+宽度阈值](/img/captcha/mmedian-width-threshold.png)\n\n#### LSD 检测线段\n##### 形态学算子+LSD检测线段+后处理\n1. 形态学算子预处理去除杂点\n![形态学算子预处理去除杂点](/img/captcha/lsd-pre.png)\n\n2. LSD 算法检测线段：\n![LSD算法检测线段](/img/captcha/lsd-line.png)\n3. LSD 算法滤除线段：\n![LSD算法滤除线段](/img/captcha/lsd-line-remove.png)\n\n4. 形态学算子后处理去除杂点、杂线\n![形态学算子后处理去除杂点](/img/captcha/lsd-post.png)\n\n\n### 颜色聚类\n#### K-means\n直接使用 K-means 对图片进行聚类：\n![K-means全局聚类](/img/captcha/kmeans.png)\n\n#### 区域生长法\n使用区域生长算法，从种子点开始，将附近像素相似的点聚为一类：\n![区域生长法（需给定种子点）](/img/captcha/region-grow.png)\n\n# 第二次实验\n## 总体思路\n不考虑滤除线段，直接转换空间得到特征表示，再训练分类器进行分类。\n\n{% plantuml %}\nstate \"预处理\" as preproc:   \"形态学算子：开运算\"\nstate \"分割字符\" as segment: \"二维核密度估计\"\nstate \"后处理\" as postproc:  \"连通区域分析\"\nstate \"提取特征\" as feature: \"Gabor 特征\"\nstate \"分类\" as classify:    \"Logistic 回归\"\n\n[*] -right--> preproc\npreproc -right--> segment\nsegment -right--> postproc\npostproc -down--> feature\nfeature -left--> classify\n{% endplantuml %}\n\n> TODO: 完成后续部分\n\n## 预处理\n\n## 分割\n\n## 后处理\n\n## 提取特征\n\n#### 分类\n","source":"_drafts/captcha.md","raw":"---\ntitle: captcha\ntags:\n\nmathjax: true\n---\n# 分析验证码样本图片\n从工商系统获取到22515张图片，经测量没有出现重复样本，推测验证码应该是由服务器即时生成的随机图片，没有用缓存策略。\n![验证码样本](/img/captcha/samples.png)\n## 概览所有样本\n分析发现，验证码样本可分为两类：成语类和运算类。二者分别有以下特征：\n- 成语类：\n    - 4 个字符\n    - 每个字符约为30像素 x 30像素大小\n    - 出现的字符不固定，总集合较大，几乎涵盖所有中文成语中出现的字符\n- 运算类\n    - 5 个字符，每个字符约为26像素x26像素大小。\n    - 每个字符\n    - 有一定模式，出现的字符集合确定\n    - 共有三种模式：“N加N等于”，“N减N等于”，“N乘N等于”，\n    其中 N $\\in$ {“零”，“壹”，“贰”，“叁”，“肆”，“伍”，“陆”，“柒”，“捌”，“玖”}\n\n经以上分析，发现“运算类”验证码识别目标较为确定，便于标注训练及验证分类效果，因此主要考虑识别此类验证码。\n\n## 分析单个样本\n![单个运算类验证码样本](/img/captcha/sample.jpg)\n上图为某“运算类”验证码样本，对其进一步分析发现验证码图片有以下几个特点：\n1. 杂点噪声\n    图片背景分布有随机椒盐噪声\n2. 线段噪声\n    图片前景分布有随机方向、随机长度、随机颜色的线段噪声。注意到线段是遮盖在文字上方的，且线段颜色和文字颜色相差并不大。\n3. 颜色对比度较低\n\t前景和背景的且颜色的对比度较低，前景和背景颜色都很暗淡，区分度不高。而且放大图片后发现，文字周围还有一圈“红晕”。\n\n# 第一次实验\n\n## 总体思路\n{% plantuml %}\n(*) -right--> \"下载验证码\"\n-right--> \"去除线段\"\n-right--> \"调用tesseract进行字符识别\"\n{% endplantuml %}\n\n## 实验总览\n{% plantuml %}\nstate \"灰度化\" as grayscale\nstate \"二值化\" as binary\n\n[*] -right--> grayscale\ngrayscale -right--> binary \nbinary -right--> edge\nbinary -down--> line\n\n[*] -down--> cluster\n\nstate \"边缘检测\" as edge {\nstate \"canny 检测边缘\" as canny\n[*] --> canny\n}\n\nstate \"检测线段\" as line {\n\nstate \"霍夫变换\" as hough\nstate \"轮廓模型\" as contour\nstate \"垂直直方图投影\" as vhist\nstate \"分割字符\" as segment\nstate \"中值滤波\" as median\nstate \"LSD 线段检测\" as lsd\n[*] --> hough\n\n[*] --> contour\ncontour --> vhist\nvhist --> segment\n\n[*] --> median\nmedian --> hough\n\n[*] --> median\nmedian --> contour\n\n[*] --> median \nmedian --> median : 两次中值滤波\nmedian --> vhist\n\n[*] --> lsd\n}\n\nstate \"颜色聚类\" as cluster {\n\nstate \"K-means 聚类\" as kmeans\nstate \"区域生长法\" as regiongrow\n[*] --> kmeans\n\n[*] --> regiongrow\n}\n\n{% endplantuml %}\n\n## 预处理及分割尝试\n首先，转换样本图片为灰度图片：\n![灰度处理](/img/captcha/grayscale.png)\n再通过大津法（Ostu）将图片二值化\n![二值化（大津法）](/img/captcha/binary.png)\n\n### 检测边缘\n对图片用canny算子检测边缘：\n![检测边缘](/img/captcha/edge.png)\n这个方法可以去除杂点，然而，被线段干扰较大\n\n### 去除线段\n#### 霍夫变换\n直接使用matlab vision包提供的HoughTransform检测线段，并在原图中，将检测的线段用白色覆盖，得到的结果如下：\n检测到的线段：\n![霍夫变换检测线段](/img/captcha/hough-line.png)\n![霍夫变换去除线段](/img/captcha/hough.png)\n\n#### 轮廓模型（蛇模型）+垂直直方图投影\n使用matlab中的activecontour函数检测字符轮廓，从而滤除线段，得到的结果如下：\n![蛇模型检测轮廓](/img/captcha/contour.png)\n注意，此方法要求先给定字符所在位置。\n\n批量应用轮廓模型分割字符位置结果：\n![批量应用轮廓模型分割字符位置结果](/img/captcha/batch-contour.png)\n\n#### 中值滤波\n考虑到杂点和杂线宽度较窄，使用中值滤波试图过滤掉杂点和杂线：\n![中值滤波结果](/img/captcha/median.png)\n\n##### 中值滤波+轮廓模型+垂直直方图投影\n先通过中值滤波去除杂点，同时得到较为“干净”的图片，之后用干净图片作为mask，应用activecontour函数检测字符轮廓，最后，用垂直直方图投影确定字符位置。得到结果如下：\n![中值滤波+轮廓模型](/img/captcha/median-contour.png)\n\n批量应用结果：\n![批量中值滤波+轮廓模型](/img/captcha/batch-median-contour.png)\n\n##### 两次中值滤波+垂直直方图投影+宽度阈值\n连续应用两次中值滤波，通过垂直直方图投影确定字符位置，最后用宽度阈值过滤，得到有效图像，结果如下：\n![两次中值滤波+垂直直方图投影+宽度阈值](/img/captcha/mmedian-width-threshold.png)\n\n#### LSD 检测线段\n##### 形态学算子+LSD检测线段+后处理\n1. 形态学算子预处理去除杂点\n![形态学算子预处理去除杂点](/img/captcha/lsd-pre.png)\n\n2. LSD 算法检测线段：\n![LSD算法检测线段](/img/captcha/lsd-line.png)\n3. LSD 算法滤除线段：\n![LSD算法滤除线段](/img/captcha/lsd-line-remove.png)\n\n4. 形态学算子后处理去除杂点、杂线\n![形态学算子后处理去除杂点](/img/captcha/lsd-post.png)\n\n\n### 颜色聚类\n#### K-means\n直接使用 K-means 对图片进行聚类：\n![K-means全局聚类](/img/captcha/kmeans.png)\n\n#### 区域生长法\n使用区域生长算法，从种子点开始，将附近像素相似的点聚为一类：\n![区域生长法（需给定种子点）](/img/captcha/region-grow.png)\n\n# 第二次实验\n## 总体思路\n不考虑滤除线段，直接转换空间得到特征表示，再训练分类器进行分类。\n\n{% plantuml %}\nstate \"预处理\" as preproc:   \"形态学算子：开运算\"\nstate \"分割字符\" as segment: \"二维核密度估计\"\nstate \"后处理\" as postproc:  \"连通区域分析\"\nstate \"提取特征\" as feature: \"Gabor 特征\"\nstate \"分类\" as classify:    \"Logistic 回归\"\n\n[*] -right--> preproc\npreproc -right--> segment\nsegment -right--> postproc\npostproc -down--> feature\nfeature -left--> classify\n{% endplantuml %}\n\n> TODO: 完成后续部分\n\n## 预处理\n\n## 分割\n\n## 后处理\n\n## 提取特征\n\n#### 分类\n","slug":"captcha","published":0,"date":"2016-06-21T14:37:22.000Z","updated":"2016-06-21T14:37:22.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji6x00016jodywpw1lcp"},{"title":"《统计学习基础》读书总结 - 第二章（上）","mathjax":true,"date":"2015-10-13T07:59:45.000Z","_content":"\n# 前言\n\n这本《统计学习基础（Elements of statistical Learning）》是我购买的第一本全英文技术书籍，书中的插图丰富而且美观，讲解有了图示之后也变得明白易懂……李航博士的《统计学习方法》一书中，引用到了这本《统计学习基础》然而由于各种原因，之前只是零碎地读过几章，并没有完整系统地阅读过。这次终于下定决心要详细阅读一遍，并将学习的心得总结在这里。\n\n<!-- more -->\n\n# 一、监督学习总览\n\n监督学习中，从输出的类别来看，我们可以将学习问题分为以下几类：\n- 输出为连续的数量（quantitative），此类问题被称为回归问题（regression）；\n- 输出为离散的类别（qualitative），此类问题被称为分类问题（classification）；\n- 输出为有序类别（ordered categorical）\n    - 二类问题，可将输出编码为 $\\{ 0, 1 \\}$；\n    - 多类问题，例如{小, 中, 大}三类，此时可将输出按“虚拟变量（dummy variables）”形式编码：\n    $$\\begin{cases}\n    100 &: \\text{小} \\\\\\\n    010 &: \\text{中} \\\\\\\n    001 &: \\text{大} \\\\\\\n    \\end{cases}$$\n\n# 二、最小二乘法和最近邻法\n\n这节从两个基本预测算法开始介绍：最小二乘法和最近邻法。\n\n## 2.1 最小二乘法\n线性模型与最小二乘法（Linear Models and Least Squares）\n给定输入 $X = (X\\_1, X\\_2, \\cdots, X\\_p)$，我们可以建立线性模型来预测输出 $Y$：\n$$ \\hat{Y} = \\hat{\\beta} + \\sum\\_{j=1}^p{X\\_j \\hat{\\beta\\_j} } \\tag{1}\\label{leastsquare}$$\n其中，$\\hat{\\beta\\_0}$就是截距，在机器学习中也被称为“偏差（bias）”。为了方便，我们常把 $\\hat{\\beta\\_0}$ 包含进 $\\hat{\\beta}$中，相应地，在 $X$ 中添加一个常量 1（$\\ref{leastsquare}$式子右边展开是$ 1 \\times \\hat{\\beta\\_0} + x\\_1 \\hat{\\beta\\_1} + \\cdots + x\\_p \\hat{\\beta\\_p} $），这样就能把整个线性模型写为一个内积的形式了：\n$$ \\hat{Y} = X^\\top \\hat{\\beta} $$\n\n注意，这里线性模型不只有一维情况，$\\hat{Y}$可以是一个K维向量，此时 $\\beta$ 是一个 p\\times K 的协方差矩阵，（X, \\hat{Y}）表示一个超平面。\n\n从 $p$ 维输入空间来看，$f(X)=X^\\top \\beta$ 是线性的，而$f'(X) = \\beta$将指向坡度最陡的方向。\n\n接下来用最小平方误差法来拟合模型：\n$$RSS(\\beta) = \\sum\\_{i=1}^N (y\\_i - x\\_i^\\top \\beta)^2$$\n用矩阵形式描述就是：\n$$RSS(\\beta) = (\\mathbf{y}-\\mathbf{X}\\beta)^\\top (\\mathbf{y}-\\mathbf{X}\\beta)$$\n其中，$\\mathbf{X}$为$N \\times p$的矩阵，每一行为一个输入向量，$\\mathbf{y}$为$N$维输出向量。\n\n我们拟合模型就需要最小化误差：$RSS(\\beta)$，令$\\frac{\\partial RSS(\\beta)}{\\beta}=0$得到：\n$$\\mathbf{X}^\\top (\\mathbf{y}-\\mathbf{X}\\beta)$$\n\n继而解出误差最小时的$\\beta$：\n$$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n\n有了$\\beta$，我们的模型也就确定了，从而就可以预测任意输入变量 $x\\_0$ 的类别：\n$$\\hat{y}(x\\_0) = x\\_0^\\top \\hat{\\beta}$$\n\n在二分类问题中，只有$y=0 或 y=1$两种情况，而用线性模型得到的输出向量范围是连续的值，此时可以定义当$\\hat{y}>0.5$时，分类为$y=1$类；而当$\\hat{y}<0.5$时，分类为$y=0$类。\n\n## 2.2 最近邻法\n最近邻法使用训练集中，$x$的最近邻观察点来估计分类输出$\\hat{Y}$：\n$$ \\hat{Y} = \\frac{1}{n} \\sum\\_{x\\_i \\in N\\_k(x)}{y\\_i} $$\n其中，$N\\_k(x)$是$x$的$k$近邻点。计算出$\\hat{Y}$后即可判定类别：当 $\\hat{Y} > 0.5$ 时判为一类；当 $\\hat{Y} < 0.5$ 时判为另一类。\n\n近邻的定义需要使用距离。距离一般有欧氏距离、马氏距离、曼哈顿距离、切比雪夫距离等等，这里不再展开。\n\n最近邻法分类中，随着 k 的增大，其分类面将更“泛化”，在训练集上的错误率也会提高；相反，当 k=1 时，其训练集错误率为 0（每类就取其自身的类别）。\n\n另外，最近邻法中只需要 1 个参数：$k$。*参数的有效数量(effective number of parameters)*为 $N/k$，并随着 $k$ 增大而减小（参数的有效数量**我暂时还没有很好地理解**。书上是这样介绍的：当 $k$ 个类别互不相交时，邻域数可为 $N/k$，从而每个邻域取一个均值即可代表这个类别）。\n\n> 参数的有效数量\n\n最近邻法中，不能用训练集上的均方误差来训练得到 $k$，因为这样的话我们将总会得到 $k=1$！\n\n## 2.3 最小二乘法和最近邻法的比较\n* 最小二乘法得到的线性决策面较为平滑，而且与数据的拟合更为稳定；然而它要求数据本身应该是适合线性决策面的，也就是说，它会有较小的variance，而可能有较大的 bia也就是说，它会有较小的方差（variance），而可能有较大的偏差（bias）。\n\n    这里需要指出，方差（variance）是指模型假设与训练样本之间的差别；而偏差（bias）是指模型假设与真实情况之间的差别。比如说我们现在有一组样本，用线性决策面能很好地拟合，这时，variance就比较小；而如果样本所代表的数据其实并非线性，那么这个决策面就不能很好地预测样本外的真实数据，bias就比较大了。\n\n* K近邻法对数据的分布要求不高，没有很强的假设，因而适用于各种情况。然而，这个方法得到的决策面受输入样本点的分布影响较大，因而较为不稳定。即variance较大而bias较小。\n\n## 2.4 最近邻法的几种改进\n\n* 通过核方法为样本点的邻居赋予不同的权重，离样本点越远，权重越小。\n* 在高维空间中，对距离核进行一些调整，以强调某些变量。\n* 局部回归：通过局部加权最小二乘法拟合，而非局部拟合常量（fitting constants locally？）\n* 对原始输入点进行扩展以使得线性模型能拟合任意复杂模型。\n* Projection pursuit 以及神经网络模型由非线性转换而来的线性模型组合而成。\n\n# 三、统计决策理论\n统计决策理论为统计机器学习提供了一个框架：\n\n## 3.1 连续形式的输出（quantitative output）\n首先，用 $X\\in \\mathbb{R}^p$ 表示随机输入向量，$Y\\in \\mathbb{R}$表示随机输出变量，$X$和$Y$联合概率：$Pr(X,Y)$。\n我们的目标就是，寻找一个函数$f(X)$，使得从输入$X$的值能预测出$Y$。\n这个理论需要一个损失函数（loss function）来惩罚预测的错误：$L(Y, f(X))$。\n\n由以上的铺垫，我们就有函数$f(x)$的解：\n$$ f(x) = E(Y \\mid X=x) $$\n也就是说，$f(x)$是样本$X=x$条件下，输出值的期望（这也叫回归函数）。\n\n下面我们结合之前学习的两个方法来看一下这个理论框架是如何应用的。\n\n### 3.1.1 K-近邻方法\n\n从这个理论来看K-近邻方法，就会发现，K-近邻实际上就是直接用训练数据来得$f(x)$的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行分类决策：\n$$ \\hat{f}(x) = Ave(y\\_i \\mid x\\_i \\in N\\_k(x)) $$\n从这个理论再来看K-近邻方法，就能发现，K-近邻实际上就是直接用训练数据来得到f(x)的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行类别决策：\n\n总结一下K-近邻的两个特点：\n- **K-近邻是通过样本来估计期望的，是基于样本的方法**；\n- K-近邻是把一个点上的条件概率放宽到了一个邻域上面。\n\n理论上，当样本数$N$和类别数$k$都趋向于无穷大时，$k/N \\rightarrow 0$，$\\hat{f}(x) \\rightarrow E(Y\\mid X=x)$。然而实际上，随着维数$p$增大，K-近邻中对“近邻”的测量复杂度也会随之增大，虽然$Ave(y\\_i \\mid x\\_i \\in N\\_k(x))$仍会继续收敛，但收敛的速度会随着维数增高而变慢。\n\n### 3.1.2 线性回归方法\n\n再来看线性回归，它实际上做了一个假设，假设$f(x)$是近似线性的：\n$$f(x) \\approx x^\\top \\beta$$\n也就是说**线性回归是基于模型的方法**。\n\n比如一个用平方误差作为损失函数的线性回归，它的期望预测误差（Expected Prediction Error）为：\n$$\\begin{align}\nEPE(f)  & = & E(Y - f(X))^2 \\\\\\\n        & = & \\int (y-f(x))^2 Pr(dx, dy)\n\\end{align}\n\\tag{2}\\label{2}$$\n我们可以将 $f(x)$ 代入，解得：\n$$ \\theta = [E(XX^\\top)]^{-1} E(XY) $$\n而最小二乘法实际上就是将$\\ref{2}$式中的期望用样本的均值代替。\n\n我们将$\\ref{2}$式的联合概率改写为在$X$上的条件概率，得到：\n$$EPE(f) = E\\_X E\\_Y ([Y - f(X)]^2 \\mid X)$$\n\n上式是采用平方误差（$L\\_2$）作为损失函数的结果，那如果我们改用$L\\_1$，结果就会是中值形式了：\n$$ \\hat{f}(x) = \\operatorname{median}(Y \\mid X=x) $$\n\n**中值形式相较于平方形式，对离群点更加稳健。然而 $L\\_1$ 导数有不连续情况，因而没有得到广泛应用。**\n\n## 3.2 不连续形式的输出（categorical output）\n预测不连续形式输出的基本模式和连续形式的差不多，只是需要用不同的损失函数来惩罚预测误差。\n\n用$G$表示输出的随机变量，$\\mathcal{G}$表示所有可能的输出，$\\hat{G}$表示估计的可能类别集合。我们的损失函数可以用一个 $K\\times K$ 矩阵 $L$ 描述，这里的 $K = \\lvert \\mathcal{G} \\rvert$，$L$上的元素$L(k,l)$表示将$\\mathcal{G}\\_k$分类为$\\mathcal{G}\\_l$的代价。\n\n根据这个损失函数，我们有：\n$$EPE = E[L(G, \\hat{G}(X))]$$\n改写为条件概率的形式就是：\n$$EPE = E\\_X \\sum\\_{k=1}^{K} L[\\mathcal{G}\\_k, \\hat{G}(X)] Pr(\\mathcal{G}\\_k \\mid X)$$\n逐点最小化EPE，就得到了估计输出：\n$$\\hat{G}(x) = \\operatorname{argmin}\\_{g\\in \\mathcal{G}} \\sum\\_{k=1}^K L(\\mathcal{G}\\_k, g) Pr(\\mathcal{G}\\_k \\mid X = x)$$\n\n### 3.2.1 贝叶斯分类器\n\n如果我们采用 0-1 损失函数（即分错代价为1，分对为0）的话，可以进一步将上式简化为：\n$$\\hat{G}(x) = \\operatorname{argmin}\\_{g\\in \\mathcal{G}} [1-Pr(g \\mid X=x)]$$\n\n上面的式子等价于 $\\operatorname{argmax}\\_{g\\in \\mathcal{G}} Pr(g \\mid X=x)$。也就是说在给定 $X=x$ 条件下，类别$G\\_k$概率最大，那就预测输出$G\\_k$。这个实际上就是贝叶斯分类器了。\n\n### 3.2.2 K-近邻与贝叶斯分类器的关系\n\nK-近邻分类类似于贝叶斯分类。他们的区别只在于：\n- 贝叶斯分类是考虑某一个点的条件概率，而K-近邻将其放宽到一个邻域了；\n- 贝叶斯分类器中的概率在K-近邻中用邻域中训练样本的比例来得到。\n","source":"_drafts/esl-2a.md","raw":"---\ntitle: 《统计学习基础》读书总结 - 第二章（上）\ntags:\n  - 机器学习\n  - 统计学习\n  - 读书笔记\n  - 统计学习基础\n  - ESL\ncategory: 机器学习\nmathjax: true\ndate: 2015-10-13 15:59:45\n---\n\n# 前言\n\n这本《统计学习基础（Elements of statistical Learning）》是我购买的第一本全英文技术书籍，书中的插图丰富而且美观，讲解有了图示之后也变得明白易懂……李航博士的《统计学习方法》一书中，引用到了这本《统计学习基础》然而由于各种原因，之前只是零碎地读过几章，并没有完整系统地阅读过。这次终于下定决心要详细阅读一遍，并将学习的心得总结在这里。\n\n<!-- more -->\n\n# 一、监督学习总览\n\n监督学习中，从输出的类别来看，我们可以将学习问题分为以下几类：\n- 输出为连续的数量（quantitative），此类问题被称为回归问题（regression）；\n- 输出为离散的类别（qualitative），此类问题被称为分类问题（classification）；\n- 输出为有序类别（ordered categorical）\n    - 二类问题，可将输出编码为 $\\{ 0, 1 \\}$；\n    - 多类问题，例如{小, 中, 大}三类，此时可将输出按“虚拟变量（dummy variables）”形式编码：\n    $$\\begin{cases}\n    100 &: \\text{小} \\\\\\\n    010 &: \\text{中} \\\\\\\n    001 &: \\text{大} \\\\\\\n    \\end{cases}$$\n\n# 二、最小二乘法和最近邻法\n\n这节从两个基本预测算法开始介绍：最小二乘法和最近邻法。\n\n## 2.1 最小二乘法\n线性模型与最小二乘法（Linear Models and Least Squares）\n给定输入 $X = (X\\_1, X\\_2, \\cdots, X\\_p)$，我们可以建立线性模型来预测输出 $Y$：\n$$ \\hat{Y} = \\hat{\\beta} + \\sum\\_{j=1}^p{X\\_j \\hat{\\beta\\_j} } \\tag{1}\\label{leastsquare}$$\n其中，$\\hat{\\beta\\_0}$就是截距，在机器学习中也被称为“偏差（bias）”。为了方便，我们常把 $\\hat{\\beta\\_0}$ 包含进 $\\hat{\\beta}$中，相应地，在 $X$ 中添加一个常量 1（$\\ref{leastsquare}$式子右边展开是$ 1 \\times \\hat{\\beta\\_0} + x\\_1 \\hat{\\beta\\_1} + \\cdots + x\\_p \\hat{\\beta\\_p} $），这样就能把整个线性模型写为一个内积的形式了：\n$$ \\hat{Y} = X^\\top \\hat{\\beta} $$\n\n注意，这里线性模型不只有一维情况，$\\hat{Y}$可以是一个K维向量，此时 $\\beta$ 是一个 p\\times K 的协方差矩阵，（X, \\hat{Y}）表示一个超平面。\n\n从 $p$ 维输入空间来看，$f(X)=X^\\top \\beta$ 是线性的，而$f'(X) = \\beta$将指向坡度最陡的方向。\n\n接下来用最小平方误差法来拟合模型：\n$$RSS(\\beta) = \\sum\\_{i=1}^N (y\\_i - x\\_i^\\top \\beta)^2$$\n用矩阵形式描述就是：\n$$RSS(\\beta) = (\\mathbf{y}-\\mathbf{X}\\beta)^\\top (\\mathbf{y}-\\mathbf{X}\\beta)$$\n其中，$\\mathbf{X}$为$N \\times p$的矩阵，每一行为一个输入向量，$\\mathbf{y}$为$N$维输出向量。\n\n我们拟合模型就需要最小化误差：$RSS(\\beta)$，令$\\frac{\\partial RSS(\\beta)}{\\beta}=0$得到：\n$$\\mathbf{X}^\\top (\\mathbf{y}-\\mathbf{X}\\beta)$$\n\n继而解出误差最小时的$\\beta$：\n$$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n\n有了$\\beta$，我们的模型也就确定了，从而就可以预测任意输入变量 $x\\_0$ 的类别：\n$$\\hat{y}(x\\_0) = x\\_0^\\top \\hat{\\beta}$$\n\n在二分类问题中，只有$y=0 或 y=1$两种情况，而用线性模型得到的输出向量范围是连续的值，此时可以定义当$\\hat{y}>0.5$时，分类为$y=1$类；而当$\\hat{y}<0.5$时，分类为$y=0$类。\n\n## 2.2 最近邻法\n最近邻法使用训练集中，$x$的最近邻观察点来估计分类输出$\\hat{Y}$：\n$$ \\hat{Y} = \\frac{1}{n} \\sum\\_{x\\_i \\in N\\_k(x)}{y\\_i} $$\n其中，$N\\_k(x)$是$x$的$k$近邻点。计算出$\\hat{Y}$后即可判定类别：当 $\\hat{Y} > 0.5$ 时判为一类；当 $\\hat{Y} < 0.5$ 时判为另一类。\n\n近邻的定义需要使用距离。距离一般有欧氏距离、马氏距离、曼哈顿距离、切比雪夫距离等等，这里不再展开。\n\n最近邻法分类中，随着 k 的增大，其分类面将更“泛化”，在训练集上的错误率也会提高；相反，当 k=1 时，其训练集错误率为 0（每类就取其自身的类别）。\n\n另外，最近邻法中只需要 1 个参数：$k$。*参数的有效数量(effective number of parameters)*为 $N/k$，并随着 $k$ 增大而减小（参数的有效数量**我暂时还没有很好地理解**。书上是这样介绍的：当 $k$ 个类别互不相交时，邻域数可为 $N/k$，从而每个邻域取一个均值即可代表这个类别）。\n\n> 参数的有效数量\n\n最近邻法中，不能用训练集上的均方误差来训练得到 $k$，因为这样的话我们将总会得到 $k=1$！\n\n## 2.3 最小二乘法和最近邻法的比较\n* 最小二乘法得到的线性决策面较为平滑，而且与数据的拟合更为稳定；然而它要求数据本身应该是适合线性决策面的，也就是说，它会有较小的variance，而可能有较大的 bia也就是说，它会有较小的方差（variance），而可能有较大的偏差（bias）。\n\n    这里需要指出，方差（variance）是指模型假设与训练样本之间的差别；而偏差（bias）是指模型假设与真实情况之间的差别。比如说我们现在有一组样本，用线性决策面能很好地拟合，这时，variance就比较小；而如果样本所代表的数据其实并非线性，那么这个决策面就不能很好地预测样本外的真实数据，bias就比较大了。\n\n* K近邻法对数据的分布要求不高，没有很强的假设，因而适用于各种情况。然而，这个方法得到的决策面受输入样本点的分布影响较大，因而较为不稳定。即variance较大而bias较小。\n\n## 2.4 最近邻法的几种改进\n\n* 通过核方法为样本点的邻居赋予不同的权重，离样本点越远，权重越小。\n* 在高维空间中，对距离核进行一些调整，以强调某些变量。\n* 局部回归：通过局部加权最小二乘法拟合，而非局部拟合常量（fitting constants locally？）\n* 对原始输入点进行扩展以使得线性模型能拟合任意复杂模型。\n* Projection pursuit 以及神经网络模型由非线性转换而来的线性模型组合而成。\n\n# 三、统计决策理论\n统计决策理论为统计机器学习提供了一个框架：\n\n## 3.1 连续形式的输出（quantitative output）\n首先，用 $X\\in \\mathbb{R}^p$ 表示随机输入向量，$Y\\in \\mathbb{R}$表示随机输出变量，$X$和$Y$联合概率：$Pr(X,Y)$。\n我们的目标就是，寻找一个函数$f(X)$，使得从输入$X$的值能预测出$Y$。\n这个理论需要一个损失函数（loss function）来惩罚预测的错误：$L(Y, f(X))$。\n\n由以上的铺垫，我们就有函数$f(x)$的解：\n$$ f(x) = E(Y \\mid X=x) $$\n也就是说，$f(x)$是样本$X=x$条件下，输出值的期望（这也叫回归函数）。\n\n下面我们结合之前学习的两个方法来看一下这个理论框架是如何应用的。\n\n### 3.1.1 K-近邻方法\n\n从这个理论来看K-近邻方法，就会发现，K-近邻实际上就是直接用训练数据来得$f(x)$的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行分类决策：\n$$ \\hat{f}(x) = Ave(y\\_i \\mid x\\_i \\in N\\_k(x)) $$\n从这个理论再来看K-近邻方法，就能发现，K-近邻实际上就是直接用训练数据来得到f(x)的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行类别决策：\n\n总结一下K-近邻的两个特点：\n- **K-近邻是通过样本来估计期望的，是基于样本的方法**；\n- K-近邻是把一个点上的条件概率放宽到了一个邻域上面。\n\n理论上，当样本数$N$和类别数$k$都趋向于无穷大时，$k/N \\rightarrow 0$，$\\hat{f}(x) \\rightarrow E(Y\\mid X=x)$。然而实际上，随着维数$p$增大，K-近邻中对“近邻”的测量复杂度也会随之增大，虽然$Ave(y\\_i \\mid x\\_i \\in N\\_k(x))$仍会继续收敛，但收敛的速度会随着维数增高而变慢。\n\n### 3.1.2 线性回归方法\n\n再来看线性回归，它实际上做了一个假设，假设$f(x)$是近似线性的：\n$$f(x) \\approx x^\\top \\beta$$\n也就是说**线性回归是基于模型的方法**。\n\n比如一个用平方误差作为损失函数的线性回归，它的期望预测误差（Expected Prediction Error）为：\n$$\\begin{align}\nEPE(f)  & = & E(Y - f(X))^2 \\\\\\\n        & = & \\int (y-f(x))^2 Pr(dx, dy)\n\\end{align}\n\\tag{2}\\label{2}$$\n我们可以将 $f(x)$ 代入，解得：\n$$ \\theta = [E(XX^\\top)]^{-1} E(XY) $$\n而最小二乘法实际上就是将$\\ref{2}$式中的期望用样本的均值代替。\n\n我们将$\\ref{2}$式的联合概率改写为在$X$上的条件概率，得到：\n$$EPE(f) = E\\_X E\\_Y ([Y - f(X)]^2 \\mid X)$$\n\n上式是采用平方误差（$L\\_2$）作为损失函数的结果，那如果我们改用$L\\_1$，结果就会是中值形式了：\n$$ \\hat{f}(x) = \\operatorname{median}(Y \\mid X=x) $$\n\n**中值形式相较于平方形式，对离群点更加稳健。然而 $L\\_1$ 导数有不连续情况，因而没有得到广泛应用。**\n\n## 3.2 不连续形式的输出（categorical output）\n预测不连续形式输出的基本模式和连续形式的差不多，只是需要用不同的损失函数来惩罚预测误差。\n\n用$G$表示输出的随机变量，$\\mathcal{G}$表示所有可能的输出，$\\hat{G}$表示估计的可能类别集合。我们的损失函数可以用一个 $K\\times K$ 矩阵 $L$ 描述，这里的 $K = \\lvert \\mathcal{G} \\rvert$，$L$上的元素$L(k,l)$表示将$\\mathcal{G}\\_k$分类为$\\mathcal{G}\\_l$的代价。\n\n根据这个损失函数，我们有：\n$$EPE = E[L(G, \\hat{G}(X))]$$\n改写为条件概率的形式就是：\n$$EPE = E\\_X \\sum\\_{k=1}^{K} L[\\mathcal{G}\\_k, \\hat{G}(X)] Pr(\\mathcal{G}\\_k \\mid X)$$\n逐点最小化EPE，就得到了估计输出：\n$$\\hat{G}(x) = \\operatorname{argmin}\\_{g\\in \\mathcal{G}} \\sum\\_{k=1}^K L(\\mathcal{G}\\_k, g) Pr(\\mathcal{G}\\_k \\mid X = x)$$\n\n### 3.2.1 贝叶斯分类器\n\n如果我们采用 0-1 损失函数（即分错代价为1，分对为0）的话，可以进一步将上式简化为：\n$$\\hat{G}(x) = \\operatorname{argmin}\\_{g\\in \\mathcal{G}} [1-Pr(g \\mid X=x)]$$\n\n上面的式子等价于 $\\operatorname{argmax}\\_{g\\in \\mathcal{G}} Pr(g \\mid X=x)$。也就是说在给定 $X=x$ 条件下，类别$G\\_k$概率最大，那就预测输出$G\\_k$。这个实际上就是贝叶斯分类器了。\n\n### 3.2.2 K-近邻与贝叶斯分类器的关系\n\nK-近邻分类类似于贝叶斯分类。他们的区别只在于：\n- 贝叶斯分类是考虑某一个点的条件概率，而K-近邻将其放宽到一个邻域了；\n- 贝叶斯分类器中的概率在K-近邻中用邻域中训练样本的比例来得到。\n","slug":"esl-2a","published":0,"updated":"2016-06-21T14:37:25.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji7900036jodti4rh0jh"},{"title":"愿望思维","_content":"\n这篇我们放下数学公式，来聊聊编程。“编程”这个概念如今在中国已经不再陌生，几乎各个高校都有自己的计算机学院，各种各样的培训班也是层出不穷。然而大家也都知道入门容易，精通很难。如果一个学习编程的人没能培养出这个领域所需要的良好思维习惯，而只是蜻蜓点水一般记住了一些基础的语法，概念，那么就很难对编程有更深入的理解。\n\n# 复杂度\n程序员的天敌是复杂度。从小的方面看，我们需要面对算法的复杂度，即如何用尽量少的开销实现一个计算过程；从大的方面看，我们需要面对系统的复杂度，即如何将一个现实中的复杂问题用各种计算模块相互搭配，组成一个系统来解决。如何控制软件开发中的复杂度，是计算机专业一直以来的热点话题。\n\n总的来说，解决系统复杂度，我们有几个武器：抽象、模块化、解耦合等等，我们今天介绍同样好用的武器：愿望四维。\n\n# 愿望思维\n> 要把大象装冰箱，总共分几步？\n某年春晚小品里，宋丹丹提出了这个问题，最一开始听得时候我像小品里的赵本山一样也是摸不着头脑，这就好像我们拿到一个任务需求，对方提出了一个非常宏大的愿景，然而作为具体实现者的我们，发现无从下手，不知道从哪里开始。这个时候我们不妨放轻松，先不去考虑具体实现细节，而是站在一定高度，把任务分解。\n\n我们首先定义一个把大象装冰箱的函数，名字就叫做 `packaging_elephant`：\n\n```python\n    def packaging_elephant():\n        pass\n```\n\n然后完全不考虑细节地把任务分解为三大步：\n1. 把冰箱门打开\n2. 把大象装进去\n3. 把冰箱门带上\n\n于是，我们的函数变成了：\n\n```python\n    def packaging_elephant():\n        open_fridgerator()\n        load(elephant)\n        close_fridgerator()\n```\n\n注意，里面的三个函数这时不需要定义，我们这里假设（“希望”）它们已经被定义并实现。至于这三个函数到底怎么实现、由谁实现，我们目前不需要关心。比如，`open_fridgerator()` 可能是未来的我或者是隔壁的小王来实现，我只要希望他能实现我想要的功能就行了。这样，解决整体问题的时候，我们只需要关心骨架，不必费心考虑怎么实现。而在考虑具体实现时（比如我开始实现 `open_fridgerator`这个函数），我们的问题域已经缩小，我在实现 `open_fridgerator` 的过程中不必考虑大象的存在，因此也就更能够专注，并有机会写出更为普适的代码（比如实现一个装载任意陆地动物的函数，而非只是装载大象）。\n\n这就是“愿望思维”的核心。借助“愿望思维”，不仅能让我们从实现细节中解脱出来，从大局角度思考，还可以帮助我们写出良好结构的代码。\n\n# 把握结构\n不止在编程中，我们在工作生活中也可以利用愿望思维解决问题。比如，写论文没思路，“七天憋出来六个字”，不妨先把大纲列出，先列一级标题，把握整个论文的结构，将一大块的文章分解为一章、一节。对于每一章、节，还可以进一步分解，为每一段定义一个中心话题。经过这样一番分解，我们得到了一个框架，接下来一个个地补全框架中的内容就是了。还有比如做一个 PPT，也是先写好一个“脚本”，定义好各个模块，然后再考虑各个模块的具体实现。这样下来，不仅能保证总体的思路清晰，而且原来的复杂任务得到分解，也方便分布到不同时间、空间完成（方便任务管理），同时还能在一定程度上保持上下文的一贯。\n\n反过来，我们在观察一个已经实现的复杂系统时，也可以提纲挈领，从抽象的结构层来分析理解。比如看一本厚厚的经典书，一上手可能觉得恐惧，觉得自己看不完这么多东西。然而，如果我们先把它的结构提取出来，在大的宏观角度看它是怎么实现出来的，我们就可以先对这本书有一个大的骨架形式的印象（这时不必考虑细节，就好像我们已经把每一章都读过了一样）。然后，就可以根据我们自己的需求，摘取需要的章节阅读（在阅读这些章节的时候同样可以递归地进行提纲挈领，提高阅读效率和质量）。再比如，很多电影可以比较明显地看出导演实现整个电影时的结构设计，这种电影的乐趣不仅在于具体某一个桥段、场景、台词，也在于整个结构的巧妙安排（这方面一个著名的例子就是怪才昆汀塔仑蒂诺的电影《低俗小说》）\n","source":"_drafts/wishful-thinking.md","raw":"---\ntitle: 愿望思维\ncategory: programming\ntags:\n  - 编程\n  - 重构\n\n---\n\n这篇我们放下数学公式，来聊聊编程。“编程”这个概念如今在中国已经不再陌生，几乎各个高校都有自己的计算机学院，各种各样的培训班也是层出不穷。然而大家也都知道入门容易，精通很难。如果一个学习编程的人没能培养出这个领域所需要的良好思维习惯，而只是蜻蜓点水一般记住了一些基础的语法，概念，那么就很难对编程有更深入的理解。\n\n# 复杂度\n程序员的天敌是复杂度。从小的方面看，我们需要面对算法的复杂度，即如何用尽量少的开销实现一个计算过程；从大的方面看，我们需要面对系统的复杂度，即如何将一个现实中的复杂问题用各种计算模块相互搭配，组成一个系统来解决。如何控制软件开发中的复杂度，是计算机专业一直以来的热点话题。\n\n总的来说，解决系统复杂度，我们有几个武器：抽象、模块化、解耦合等等，我们今天介绍同样好用的武器：愿望四维。\n\n# 愿望思维\n> 要把大象装冰箱，总共分几步？\n某年春晚小品里，宋丹丹提出了这个问题，最一开始听得时候我像小品里的赵本山一样也是摸不着头脑，这就好像我们拿到一个任务需求，对方提出了一个非常宏大的愿景，然而作为具体实现者的我们，发现无从下手，不知道从哪里开始。这个时候我们不妨放轻松，先不去考虑具体实现细节，而是站在一定高度，把任务分解。\n\n我们首先定义一个把大象装冰箱的函数，名字就叫做 `packaging_elephant`：\n\n```python\n    def packaging_elephant():\n        pass\n```\n\n然后完全不考虑细节地把任务分解为三大步：\n1. 把冰箱门打开\n2. 把大象装进去\n3. 把冰箱门带上\n\n于是，我们的函数变成了：\n\n```python\n    def packaging_elephant():\n        open_fridgerator()\n        load(elephant)\n        close_fridgerator()\n```\n\n注意，里面的三个函数这时不需要定义，我们这里假设（“希望”）它们已经被定义并实现。至于这三个函数到底怎么实现、由谁实现，我们目前不需要关心。比如，`open_fridgerator()` 可能是未来的我或者是隔壁的小王来实现，我只要希望他能实现我想要的功能就行了。这样，解决整体问题的时候，我们只需要关心骨架，不必费心考虑怎么实现。而在考虑具体实现时（比如我开始实现 `open_fridgerator`这个函数），我们的问题域已经缩小，我在实现 `open_fridgerator` 的过程中不必考虑大象的存在，因此也就更能够专注，并有机会写出更为普适的代码（比如实现一个装载任意陆地动物的函数，而非只是装载大象）。\n\n这就是“愿望思维”的核心。借助“愿望思维”，不仅能让我们从实现细节中解脱出来，从大局角度思考，还可以帮助我们写出良好结构的代码。\n\n# 把握结构\n不止在编程中，我们在工作生活中也可以利用愿望思维解决问题。比如，写论文没思路，“七天憋出来六个字”，不妨先把大纲列出，先列一级标题，把握整个论文的结构，将一大块的文章分解为一章、一节。对于每一章、节，还可以进一步分解，为每一段定义一个中心话题。经过这样一番分解，我们得到了一个框架，接下来一个个地补全框架中的内容就是了。还有比如做一个 PPT，也是先写好一个“脚本”，定义好各个模块，然后再考虑各个模块的具体实现。这样下来，不仅能保证总体的思路清晰，而且原来的复杂任务得到分解，也方便分布到不同时间、空间完成（方便任务管理），同时还能在一定程度上保持上下文的一贯。\n\n反过来，我们在观察一个已经实现的复杂系统时，也可以提纲挈领，从抽象的结构层来分析理解。比如看一本厚厚的经典书，一上手可能觉得恐惧，觉得自己看不完这么多东西。然而，如果我们先把它的结构提取出来，在大的宏观角度看它是怎么实现出来的，我们就可以先对这本书有一个大的骨架形式的印象（这时不必考虑细节，就好像我们已经把每一章都读过了一样）。然后，就可以根据我们自己的需求，摘取需要的章节阅读（在阅读这些章节的时候同样可以递归地进行提纲挈领，提高阅读效率和质量）。再比如，很多电影可以比较明显地看出导演实现整个电影时的结构设计，这种电影的乐趣不仅在于具体某一个桥段、场景、台词，也在于整个结构的巧妙安排（这方面一个著名的例子就是怪才昆汀塔仑蒂诺的电影《低俗小说》）\n","slug":"wishful-thinking","published":0,"date":"2016-06-22T13:39:14.000Z","updated":"2016-06-22T15:36:47.000Z","_id":"cipr1ji7c00056jodsxir1upb","comments":1,"layout":"post","photos":[],"link":""},{"title":"机器学习概念类比","date":"2015-06-13T13:47:27.000Z","_content":"* 机器学习的许多概念类似于我们人类的学习，比如说，\n<!--more-->\n* **训练集**\n    * 老师在课上教授大量的知识\n    * 这些知识某种程度上说，都是”有解且标注了解的”，我们在课上学习这些知识来构建我们脑中的模型\n* **测试集**\n    * 课后的练习题\n    * 我们并不知道各题的解，需要依靠之前课上通过学习知识构建出来的模型来解决这些问题，有点类似在”预测”练习题的解。\n* **实际测试**\n    * 考试\n    * 考试时我们不知道各题的解，纯粹是通过脑子里的模型来预测各题的解。\n* **欠拟合**\n    * 练习题做少了，或是上课经常不听\n    * 导致的结果就是，脑子里构建的模型很不完善，面对考试就瞎了。\n* **过拟合**\n    * 上课也认真听了，练习题也做了，但却是在“背答案”，把做的练习题的答案都背下来\n        * 导致的结果是，在做测试集的时候没问题，但一面临考试就又瞎了，因为考试的题目比那些练习题要灵活，“背答案”的方法只能保证做练习测试时没问题，而无法应对考试。\n","source":"_drafts/机器学习概念类比.md","raw":"---\ntitle: 机器学习概念类比\ndate: 2015-06-13 21:47:27\ncategories: 机器学习\ntags:\n- 机器学习\n- Machine Learning\n- 个人思考\n---\n* 机器学习的许多概念类似于我们人类的学习，比如说，\n<!--more-->\n* **训练集**\n    * 老师在课上教授大量的知识\n    * 这些知识某种程度上说，都是”有解且标注了解的”，我们在课上学习这些知识来构建我们脑中的模型\n* **测试集**\n    * 课后的练习题\n    * 我们并不知道各题的解，需要依靠之前课上通过学习知识构建出来的模型来解决这些问题，有点类似在”预测”练习题的解。\n* **实际测试**\n    * 考试\n    * 考试时我们不知道各题的解，纯粹是通过脑子里的模型来预测各题的解。\n* **欠拟合**\n    * 练习题做少了，或是上课经常不听\n    * 导致的结果就是，脑子里构建的模型很不完善，面对考试就瞎了。\n* **过拟合**\n    * 上课也认真听了，练习题也做了，但却是在“背答案”，把做的练习题的答案都背下来\n        * 导致的结果是，在做测试集的时候没问题，但一面临考试就又瞎了，因为考试的题目比那些练习题要灵活，“背答案”的方法只能保证做练习测试时没问题，而无法应对考试。\n","slug":"机器学习概念类比","published":0,"updated":"2016-06-21T14:37:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji7k00086jodmeqkmfn9"},{"title":"Ein 和 Eout，假设模型以及开放思维","mathjax":true,"date":"2015-10-12T17:32:18.000Z","_content":"\n\n加州理工的[机器学习公开课](http://open.163.com/special/opencourse/learningfromdata.html)上，[穆斯塔法教授](https://work.caltech.edu)对机器学习的讲解非常细致，每个公式、每个符号都会进行详细的剖析，并直观地解释出来，他对机器学习的热情和态度很值得我们学习。\n\n而这门课程在讲解学习的可行性时，多次提到了$E\\_{in}$和$E\\_{out}$的概念。 $E\\_{in}$和$E\\_{out}$分别表示模型假设对样本（已知）的错误率和对真实情况（未知）的错误率。统计机器学习之所以可行，就是因为有这样一个理论支撑：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 e^{-2\\epsilon^2 N} \\tag{\\*}\\label{\\*}$$\n上面的式子叫 [Hoeffding 不等式](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)。从公式中可以看出，当N足够大时，$E\\_{in}$和$E\\_{out}$相差太大的概率就比较小。换句话说，当N足够大时，$E\\_{in}$就会更接近于$E\\_{out}$。如此，我们机器学习提出的模型假设如果对大量已知样本能够较好地拟合的话，那它对真实的未知样本应该也能够较好地拟合。\n\n上面的$\\ref{\\*}$式只适用于一次随机试验，一次试验只建立一个模型假设，而我们实际训练时是要提出若干个假设，再从这些假设中选择最符合目标函数的那个假设。也就是：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 M e^{-2\\epsilon^2 N} \\tag{\\*\\*}\\label{\\*\\*}$$\n式子中的 M 就是我们提出的假设数目。\n\n<!-- more -->\n---\n\n理论先讲到这里，下面谈谈我个人对$E\\_{in}$和$E\\_{out}$的一点理解。\n\n**我们人在观察世界、观察他人的时候，其实就是在学习。从小到大，我们通过眼耳鼻等五官获知大量样本，再从这些样本中不断丰富和调整自己头脑中的假设模型，我们对事物、对社会、对世界的认识也就是指自己头脑中的这个模型，即所谓的“世界观”、“价值观”等等。每个人对社会、对他人的评判也都是来源于他头脑中对所认识事物建立的模型。**\n\n比如“士别三日，当刮目相看”的例子，孙权、鲁肃与吕蒙长期共事，逐渐对吕蒙建立了假设模型，而当吕蒙“乃始就学”之后，鲁肃再与吕蒙论议，发现他头脑中的这个模型已不能很好地拟合现在的吕蒙，乃大惊曰“士别三日，当刮目想看”。我们平时看人时，也要经常提醒自己，自己对他人的看法也只是自己头脑中的“假设模型”而已，你自己觉得拟合的再好，觉得$E\\_{in}$已经很小了，但也不能说明这个“假设”就能很好地拟合真实情况。$E\\_{in}$小，不代表$E\\_{out}$就小，还有可能是过拟合，或是假设模型本身就错了啊。\n\n所以有时面对他人无端指责或是误解时，我们可以这样想：他对你的看法，其实是他先对你观察，并在他头脑中建立一个模型，这个模型就是你在他头脑中的投影。而他表达出来对你的看法，也都是基于他对你在他头脑中的投影所做的观察。既然是投影，必然不全面，这个投影不一定能代表你的真实情况。这样看来，当你被人无缘由嘲讽的时候，反而暴露出了他认识的局限。你也不妨就当他在对一个“幽灵”说话。\n\n再从这个角度来看，我们也就不难理解新闻上经常出现的一些类似“某‘好男人’被曝出轨”、“某温文尔雅的编剧吸毒”的新闻所引起的舆论哗然。从机器学习的角度看，这就是反映了大众脑中的模型和真实情况反差太大，而这个反差的揭露时间又过短使得大家一时接受不了（Justin Beiber的变化时间就够长，所以它的那些新闻，我也都见怪不怪了 -\\_-）。大众脑中所建立模型的$E\\_{out}$过大，或者说 \"bias\" 过大，这个模型不能很好地预测真实世界。\n\n那么，我们如何能够改善我们的假设对于真实情况的预测呢？我们再来回顾一下$\\ref{\\*\\*}式$：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 M e^{-2\\epsilon^2 N}$$ \n要想提高$P[E\\_{in} - E\\_{out} > \\epsilon] $：一个是提高$N$，即增加我们的阅历，而且所“阅”的人（样本）分布也要尽可能的广，这样建立的模型泛化能力才更强。另一个就是不断迭代，不断评价旧有假设，从中发现不足（“吾日三省吾身”），并在其基础上进行改良，提出新的假设。也就是说，我们要保持一个勤于思考、开放的头脑，时刻提醒自己认识具有局限，保持谦卑的心态，不断学习、吸收并更新我们对世界、对他人的看法！\n\n版权声明：\n---\n本文中所有文字版权均属本人所有，未经允许请勿转载。\n","source":"_posts/Ein-Eout.md","raw":"---\ntitle: Ein 和 Eout，假设模型以及开放思维\ntags:\n  - 随笔\n  - 机器学习感悟\ncategory: 随笔\nmathjax: true\ndate: 2015-10-13 01:32:18\n---\n\n\n加州理工的[机器学习公开课](http://open.163.com/special/opencourse/learningfromdata.html)上，[穆斯塔法教授](https://work.caltech.edu)对机器学习的讲解非常细致，每个公式、每个符号都会进行详细的剖析，并直观地解释出来，他对机器学习的热情和态度很值得我们学习。\n\n而这门课程在讲解学习的可行性时，多次提到了$E\\_{in}$和$E\\_{out}$的概念。 $E\\_{in}$和$E\\_{out}$分别表示模型假设对样本（已知）的错误率和对真实情况（未知）的错误率。统计机器学习之所以可行，就是因为有这样一个理论支撑：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 e^{-2\\epsilon^2 N} \\tag{\\*}\\label{\\*}$$\n上面的式子叫 [Hoeffding 不等式](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)。从公式中可以看出，当N足够大时，$E\\_{in}$和$E\\_{out}$相差太大的概率就比较小。换句话说，当N足够大时，$E\\_{in}$就会更接近于$E\\_{out}$。如此，我们机器学习提出的模型假设如果对大量已知样本能够较好地拟合的话，那它对真实的未知样本应该也能够较好地拟合。\n\n上面的$\\ref{\\*}$式只适用于一次随机试验，一次试验只建立一个模型假设，而我们实际训练时是要提出若干个假设，再从这些假设中选择最符合目标函数的那个假设。也就是：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 M e^{-2\\epsilon^2 N} \\tag{\\*\\*}\\label{\\*\\*}$$\n式子中的 M 就是我们提出的假设数目。\n\n<!-- more -->\n---\n\n理论先讲到这里，下面谈谈我个人对$E\\_{in}$和$E\\_{out}$的一点理解。\n\n**我们人在观察世界、观察他人的时候，其实就是在学习。从小到大，我们通过眼耳鼻等五官获知大量样本，再从这些样本中不断丰富和调整自己头脑中的假设模型，我们对事物、对社会、对世界的认识也就是指自己头脑中的这个模型，即所谓的“世界观”、“价值观”等等。每个人对社会、对他人的评判也都是来源于他头脑中对所认识事物建立的模型。**\n\n比如“士别三日，当刮目相看”的例子，孙权、鲁肃与吕蒙长期共事，逐渐对吕蒙建立了假设模型，而当吕蒙“乃始就学”之后，鲁肃再与吕蒙论议，发现他头脑中的这个模型已不能很好地拟合现在的吕蒙，乃大惊曰“士别三日，当刮目想看”。我们平时看人时，也要经常提醒自己，自己对他人的看法也只是自己头脑中的“假设模型”而已，你自己觉得拟合的再好，觉得$E\\_{in}$已经很小了，但也不能说明这个“假设”就能很好地拟合真实情况。$E\\_{in}$小，不代表$E\\_{out}$就小，还有可能是过拟合，或是假设模型本身就错了啊。\n\n所以有时面对他人无端指责或是误解时，我们可以这样想：他对你的看法，其实是他先对你观察，并在他头脑中建立一个模型，这个模型就是你在他头脑中的投影。而他表达出来对你的看法，也都是基于他对你在他头脑中的投影所做的观察。既然是投影，必然不全面，这个投影不一定能代表你的真实情况。这样看来，当你被人无缘由嘲讽的时候，反而暴露出了他认识的局限。你也不妨就当他在对一个“幽灵”说话。\n\n再从这个角度来看，我们也就不难理解新闻上经常出现的一些类似“某‘好男人’被曝出轨”、“某温文尔雅的编剧吸毒”的新闻所引起的舆论哗然。从机器学习的角度看，这就是反映了大众脑中的模型和真实情况反差太大，而这个反差的揭露时间又过短使得大家一时接受不了（Justin Beiber的变化时间就够长，所以它的那些新闻，我也都见怪不怪了 -\\_-）。大众脑中所建立模型的$E\\_{out}$过大，或者说 \"bias\" 过大，这个模型不能很好地预测真实世界。\n\n那么，我们如何能够改善我们的假设对于真实情况的预测呢？我们再来回顾一下$\\ref{\\*\\*}式$：\n$$P[E\\_{in} - E\\_{out} > \\epsilon] = 2 M e^{-2\\epsilon^2 N}$$ \n要想提高$P[E\\_{in} - E\\_{out} > \\epsilon] $：一个是提高$N$，即增加我们的阅历，而且所“阅”的人（样本）分布也要尽可能的广，这样建立的模型泛化能力才更强。另一个就是不断迭代，不断评价旧有假设，从中发现不足（“吾日三省吾身”），并在其基础上进行改良，提出新的假设。也就是说，我们要保持一个勤于思考、开放的头脑，时刻提醒自己认识具有局限，保持谦卑的心态，不断学习、吸收并更新我们对世界、对他人的看法！\n\n版权声明：\n---\n本文中所有文字版权均属本人所有，未经允许请勿转载。\n","slug":"Ein-Eout","published":1,"updated":"2016-05-20T18:14:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji7s00096jodc895cm2g"},{"title":"复数的几种表示形式","date":"2015-10-06T06:47:16.000Z","updated":"2015-10-06T06:47:16.000Z","mathjax":true,"_content":"\n研究傅里叶变换的过程中经常要和复数打交道，经常会遇到 $e^{ix}$ 这种形式。\n\n这里就总结一下复数的直角坐标、极坐标，以及复指数表示形式，也有对欧拉公式的直观解释，以便更好地理解傅里叶变换。\n\n本文中图片均来自于 [国立台湾师范大学资讯工程学系的演算法笔记](http://www.csie.ntnu.edu.tw/~u91029/Wave.html)。\n\n<!--more-->\n\n## 一、复数的直角坐标表示\n\n首先，复数基本单位是 $i=\\sqrt{-1}$，有了这个单位，复数空间中的每个数都可以表示为 $a+bi$ 的形式。其中，a 被称为“实部（real part）”，b 被称为“虚部（imaginary part）”。\n\n复数可以在复平面（complex plane）上表示，复平面横纵坐标分别为实部和虚部，下图就是复数 $2+3i$ 在复平面上的表示。\n\n{% asset_img complex-plane.png  复平面 %}\n\n我们可以发现，这个复平面和实数空间的直角坐标系类似。那可不可以用极坐标的方法表示复数呢？\n\n## 二、复数的极坐标表示\n\n事实上，复数是可以用极坐标表示的，那一个复数用极坐标表示时的长度和角度分别是多少呢？我们可以在复平面中计算出来。\n\n例如，复数 $4+3i$ 的复平面直角坐标表示是$(4, 3)$，原点指向该点的向量长度 $r=\\sqrt{3^2+4^2}=5$，向量的角度 $\\theta = arctan(\\frac{3}{4})$。\n\n{% asset_img complex-polar-plane.png 复数的极坐标表示 %}\n\n这里，复数极坐标表示的长度 $r$ 也被称为“强度（magnitude）”，角度 $\\theta$ 也被称为“相位（phase）”。\n\n### 2.1 由复数极坐标得到直角坐标\n\n上面我们用复数的直角坐标计算出了极坐标，那么是不是也可以由极坐标推出直角坐标呢？我们还是从复平面中来看：\n\n{% asset_img complex-polar-transform.png  复数两种表示形式之间的转换 %}\n\n从上图可以看出，当我们有复数极坐标 $(r, \\theta)$ 时，我们可以得到其直角坐标 $(r \\cos(\\theta), r \\sin(\\theta))$，即该复数为 $r\\cos\\theta + r\\*i\\sin\\theta$。\n\n## 三、复数的复指数表示与欧拉公式\n欧拉有一天发现，神奇数字 $e$ 的纯虚数次方竟然在复数平面上绕圈！\n\n用极坐标形式表示，就是 $e^{i\\theta}=\\cos\\theta+i\\sin\\theta$。\n\n如此，一个复数就又多了一种指数的表示形式，即复指数形式：$r e^{i\\theta} = r \\cos\\theta + r\\*i \\sin\\theta$。\n\n而当 $r=1$，$\\theta=\\pi$ 时，对应的直角坐标刚好就是 $(-1, 0)$ ，也就是实数 -1。由此就有了那个著名的“欧拉公式”：$$e^{i\\pi}+1=0$$\n\n### 3.1 复数波和实数波\n\n实数波我们比较熟悉，就是 $\\sin\\theta$ 或 $\\cos\\theta$ 形式。而复数波则是由 $e^{i\\theta}$ 来定义，实数波和复数波的示意图如下：\n\n{% asset_img real-wave-complex-wave.png  实数波和复数波示意 %}\n\n从示意图中，可以看出，当俯视复数波时，观察到的投影即是一个实数波，即是 $e^{i\\theta}=\\cos\\theta + i\\* \\sin\\theta$ 的实部：$\\cos\\theta$；当从左侧侧视复数波时，得到的投影即是其虚部：$\\sin\\theta$。\n\n事实上，复数波的完整定义为：\n$$ Ae^{i(\\omega t+\\phi)} = Ae^{i(2\\pi f t+\\phi)} = A\\cos(2\\pi f t+\\phi) + iA\\sin(2\\pi f t+\\phi) $$\n其中，$A$为振幅，$\\omega$为角速度，$f$为频率，$\\phi$为初试相位，这个波的强度（magnitude）为 $A = \\sqrt{A^2 \\cdot \\cos^2(2\\pi f t+\\phi) + A^2 \\cdot \\sin^2(2\\pi f t+\\phi)}$，瞬时相位（phase）为 $2\\pi f t + \\phi$。\n\n由于复指数形式的复数波$e^{i\\theta}$相较于$\\cos(\\theta)+i\\sin(\\theta)$更简单且更易于控制，因而在信号处理中得到广泛的使用。除此之外，$e^{i\\theta}$形式可以看作是实数波的基础，因为我们可以组合两个复数波来得到$\\cos(\\theta)$和$\\sin(\\theta)$：\n\n$$\\cos(\\theta)=\\frac{e^{i\\theta}+e^{-i\\theta}}{2}$$\n$$\\sin(\\theta)=\\frac{e^{i\\theta}-e^{-i\\theta}}{2i}$$\n    \n另外，在信号处理中，我们只需要考虑实部的线性运算，因此，在我们对一个复数波进行滤波后，得到的复数波可以分解为 $\\cos$ 和 $\\sin$ 的形式，进而只需要选取实部所对应的 $\\cos$ 部分就行了。\n\n在傅里叶变换中，便是将任意非周期函数分解为了各种复数波叠加的形式，因而傅里叶变换的公式中才会有类似 $e^{ix}$ 的形式。\n\n### 参考资料：\n1. [国立台湾师范大学资讯工程学系的演算法笔记](http://www.csie.ntnu.edu.tw/~u91029/Wave.html)\n2. [斯坦福大学 JULIUS O. SMITH III 所著 Introduction to Digital Filters with Audio Applications 在线版](https://ccrma.stanford.edu/~jos/fp/Complex_Sinusoids.html)\n","source":"_posts/complex.md","raw":"---\ntitle: 复数的几种表示形式\ndate: 2015-10-06 14:47:16\nupdated: 2015-10-06 14:47:16\ncategory: 图像处理\ntags:\n- 傅里叶变换\n- 欧拉公式\n- 复数\n- 数字信号处理\n- 图像处理\n- 基础概念\n- Gabor 特征\nmathjax: true\n---\n\n研究傅里叶变换的过程中经常要和复数打交道，经常会遇到 $e^{ix}$ 这种形式。\n\n这里就总结一下复数的直角坐标、极坐标，以及复指数表示形式，也有对欧拉公式的直观解释，以便更好地理解傅里叶变换。\n\n本文中图片均来自于 [国立台湾师范大学资讯工程学系的演算法笔记](http://www.csie.ntnu.edu.tw/~u91029/Wave.html)。\n\n<!--more-->\n\n## 一、复数的直角坐标表示\n\n首先，复数基本单位是 $i=\\sqrt{-1}$，有了这个单位，复数空间中的每个数都可以表示为 $a+bi$ 的形式。其中，a 被称为“实部（real part）”，b 被称为“虚部（imaginary part）”。\n\n复数可以在复平面（complex plane）上表示，复平面横纵坐标分别为实部和虚部，下图就是复数 $2+3i$ 在复平面上的表示。\n\n{% asset_img complex-plane.png  复平面 %}\n\n我们可以发现，这个复平面和实数空间的直角坐标系类似。那可不可以用极坐标的方法表示复数呢？\n\n## 二、复数的极坐标表示\n\n事实上，复数是可以用极坐标表示的，那一个复数用极坐标表示时的长度和角度分别是多少呢？我们可以在复平面中计算出来。\n\n例如，复数 $4+3i$ 的复平面直角坐标表示是$(4, 3)$，原点指向该点的向量长度 $r=\\sqrt{3^2+4^2}=5$，向量的角度 $\\theta = arctan(\\frac{3}{4})$。\n\n{% asset_img complex-polar-plane.png 复数的极坐标表示 %}\n\n这里，复数极坐标表示的长度 $r$ 也被称为“强度（magnitude）”，角度 $\\theta$ 也被称为“相位（phase）”。\n\n### 2.1 由复数极坐标得到直角坐标\n\n上面我们用复数的直角坐标计算出了极坐标，那么是不是也可以由极坐标推出直角坐标呢？我们还是从复平面中来看：\n\n{% asset_img complex-polar-transform.png  复数两种表示形式之间的转换 %}\n\n从上图可以看出，当我们有复数极坐标 $(r, \\theta)$ 时，我们可以得到其直角坐标 $(r \\cos(\\theta), r \\sin(\\theta))$，即该复数为 $r\\cos\\theta + r\\*i\\sin\\theta$。\n\n## 三、复数的复指数表示与欧拉公式\n欧拉有一天发现，神奇数字 $e$ 的纯虚数次方竟然在复数平面上绕圈！\n\n用极坐标形式表示，就是 $e^{i\\theta}=\\cos\\theta+i\\sin\\theta$。\n\n如此，一个复数就又多了一种指数的表示形式，即复指数形式：$r e^{i\\theta} = r \\cos\\theta + r\\*i \\sin\\theta$。\n\n而当 $r=1$，$\\theta=\\pi$ 时，对应的直角坐标刚好就是 $(-1, 0)$ ，也就是实数 -1。由此就有了那个著名的“欧拉公式”：$$e^{i\\pi}+1=0$$\n\n### 3.1 复数波和实数波\n\n实数波我们比较熟悉，就是 $\\sin\\theta$ 或 $\\cos\\theta$ 形式。而复数波则是由 $e^{i\\theta}$ 来定义，实数波和复数波的示意图如下：\n\n{% asset_img real-wave-complex-wave.png  实数波和复数波示意 %}\n\n从示意图中，可以看出，当俯视复数波时，观察到的投影即是一个实数波，即是 $e^{i\\theta}=\\cos\\theta + i\\* \\sin\\theta$ 的实部：$\\cos\\theta$；当从左侧侧视复数波时，得到的投影即是其虚部：$\\sin\\theta$。\n\n事实上，复数波的完整定义为：\n$$ Ae^{i(\\omega t+\\phi)} = Ae^{i(2\\pi f t+\\phi)} = A\\cos(2\\pi f t+\\phi) + iA\\sin(2\\pi f t+\\phi) $$\n其中，$A$为振幅，$\\omega$为角速度，$f$为频率，$\\phi$为初试相位，这个波的强度（magnitude）为 $A = \\sqrt{A^2 \\cdot \\cos^2(2\\pi f t+\\phi) + A^2 \\cdot \\sin^2(2\\pi f t+\\phi)}$，瞬时相位（phase）为 $2\\pi f t + \\phi$。\n\n由于复指数形式的复数波$e^{i\\theta}$相较于$\\cos(\\theta)+i\\sin(\\theta)$更简单且更易于控制，因而在信号处理中得到广泛的使用。除此之外，$e^{i\\theta}$形式可以看作是实数波的基础，因为我们可以组合两个复数波来得到$\\cos(\\theta)$和$\\sin(\\theta)$：\n\n$$\\cos(\\theta)=\\frac{e^{i\\theta}+e^{-i\\theta}}{2}$$\n$$\\sin(\\theta)=\\frac{e^{i\\theta}-e^{-i\\theta}}{2i}$$\n    \n另外，在信号处理中，我们只需要考虑实部的线性运算，因此，在我们对一个复数波进行滤波后，得到的复数波可以分解为 $\\cos$ 和 $\\sin$ 的形式，进而只需要选取实部所对应的 $\\cos$ 部分就行了。\n\n在傅里叶变换中，便是将任意非周期函数分解为了各种复数波叠加的形式，因而傅里叶变换的公式中才会有类似 $e^{ix}$ 的形式。\n\n### 参考资料：\n1. [国立台湾师范大学资讯工程学系的演算法笔记](http://www.csie.ntnu.edu.tw/~u91029/Wave.html)\n2. [斯坦福大学 JULIUS O. SMITH III 所著 Introduction to Digital Filters with Audio Applications 在线版](https://ccrma.stanford.edu/~jos/fp/Complex_Sinusoids.html)\n","slug":"complex","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji81000b6jodctffzok4"},{"title":"我对卷积的理解","date":"2015-10-06T12:05:00.000Z","mathjax":true,"_content":"{% asset_img banner.jpeg %}\n\n在学习机器学习和图像处理的过程中，经常会遇到卷积这个概念。我每次遇到这个概念都有点似懂非懂的样子。有时候清楚它的直观解释，但又搞不清公式中是如何体现的。究其原因，还是我没有完全搞懂这个概念。 维基百科上有一个动态图来演示这个概念，但对于我来说还是有些复杂。于是自己在网上找了很多文章来研究，终于有了比较直观的印象，这里就趁热把我理解的解释一下，作为总结。\n\n## 一、一维卷积\n\n### 1.1 数学定义\n\n维基百科上，卷积的形式化定义如下：\n$$ f(x)\\*g(x) = \\int\\_{-\\infty}^{\\infty} f(\\tau)g(x-\\tau) d\\tau \\tag{1}\\label{1} $$\n\n### 1.2 直观解释\n\n先来分析一下这个公式：\n\n1. $f(x)\\*g(x)$ 表示 $f(x)$ 和 $g(x)$ 的卷积，注意此处自变量为 $x$；\n2. 它是对 $(-\\infty, \\infty)$ 区间上对 $\\tau$ 求积分；\n3. 积分对象为两个函数的乘积：$f(\\tau)$ 和 $g(x-\\tau)$。\n4. 等式右边只有 $g(x-\\tau)$ 提到了 $x$，其他部分都在关注 $\\tau$\n\n这样一个公式恐怕还是难以理解，接下来将通过一个例子来进行解释。\n\n<!-- more -->\n### 1.3 例子\n\n试想小明有一段时间每天都要去输液，输的药会在身体里残留直至失效，药效随着时间是不断衰落的。 这里为简便起见，假设药效 4 天就失效，而且药效持续函数是离散的。如下图所示：\n\n{% asset_img conv-effect-function.png 药效持续函数 %}\n\n图中，横坐标为天数，纵坐标为药效。输液当天（day=0）药效为 100%，第二天减弱为 80%，第三天减弱为 40%，第四天减弱为 0。\n\n现在先定义一些符号：\n记天数为 $t$，每天输液的药量为 $\\operatorname{m}(t)$, 药效函数为 $\\operatorname{eff}(t)$，小明身上残留的药效为 $\\operatorname{rest}(t)$\n其中药效函数：\n$$\\operatorname{eff}(t) = \n\\begin{cases} 100 \\% & \\text{t=0}  \\\\\\\n80 \\% & \\text{t=1}  \\\\\\\n40 \\% & \\text{t=2}  \\\\\\\n0  \\% & \\text{t>=3}  \\\\\\\n\\end{cases}$$\n\n下面观察一下小明从第一天起，连续三天输液后身上所留下的药效（假设每天药量固定为10）。\n- 第一天，小明去医院输完液后，药效为 10（$ \\operatorname{rest}(t) = \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）。\n\n{% asset_img conv-effect-day1.png 第一天累积的药效示意 %}\n\n- 第二天，小明去医院准备输液\n    - 输液前，他身上带着前一天的药效，此时已经衰减为 10$\\cdot$ 80%=8，即 $ \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) $。\n    - 输液后，他身上携带的药效为：8 + 10 = 18（$ \\operatorname{rest}(t) = \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) + \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）\n\n{% asset_img conv-effect-day2.png 第二天累积的药效示意 %}\n\n- 第三天，小明去医院准备输液\n    - 输液前，他身上带着前两天的药效，第一天的此时已衰减为 10$\\cdot$ 40%=4（$ \\operatorname{m}(t-2)\\cdot \\operatorname{eff}(2) $），第二天的此时衰减为 10$\\cdot$ 80%=8（$ \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) $）。\n    - 输液后，他身上携带的药效为：4 + 8 + 10 = 22（$ \\operatorname{rest}(t) = \\operatorname{m}(t-2)\\cdot \\operatorname{eff}(2) + \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) + \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）。\n\n{% asset_img conv-effect-day3.png 第三天累积的药效示意 %}\n\n### 1.4 分析\n\n从上面的分析我们可以得到，小明第 t 天身上残留的药效 $\\operatorname{rest}(t) = \\sum\\_{i=1}^n \\operatorname{m}(t-i) \\operatorname{eff}(i)$，其中 $n$ 为药效有效的最大天数。 我们不难想象，但药效函数 $\\operatorname{eff}(t)$ 为连续时，上式中的求和就应改为积分；而当药效能无限期有效时，上式中 $n$ 就为 $\\infty$。 无限期有效的药效函数，所对应的 $\\operatorname{rest}(t) = \\int\\_{-\\infty}^\\infty \\operatorname{m}(t-\\tau) \\operatorname{eff}(\\tau) \\,d\\tau$（本例中严格来说应该是 $\\int\\_0^\\infty$ ，这里推广到了 $(-\\infty, \\infty)$）。推导到这里，基本就是维基百科上卷积的定义了。\n\n### 1.5 总结\n\n我之前对卷积概念的困惑主要是因为对公式 $\\ref{1}$ 的那个 $\\tau$ 的意义理解错了，总以为 $\\tau$ 是随着坐标轴变化的量。 事实上，在上面举的例子中，**$\\tau$ 是作为沿着纵坐标遍历的量：它的作用是对“纵向”上，历次函数 $\\operatorname{eff}(t)$ 在当前点($t$)残余量($\\operatorname{rest}(t)$)的求和。积分也是对纵向上的积分，而非横向上沿自变量的积分**。\n\n横坐标变化的量始终为 $t$，而且在卷积中并没有明显体现出 $t$ 的变化。\n\n最后重新回顾一下上面的整个过程：比较三天以来的示意图可以发现，如果我们以“当天”而不是第 $t$ 天为参考的话，就会看到 $\\operatorname{eff}(t)$ 随着时间是在向左平移（深蓝的线表示当天，前几天的线都在其左边），然后各天衰落后的药量残余等于 $\\operatorname{eff}(t)$ 值乘上初始的药量值，最后将各天的药量残余求个和。整个过程的核心就是**“（反转），移动，乘积，求和”**，这里面“反转”的概念也好理解，就是本来 $\\operatorname{eff}(t)$ 是**“朝着右边”**走的函数，$t=0,t=1,\\cdots$，$\\operatorname{eff}(t)$ 是形容**t 天后的药量的**，然而实际例子中我们是以当天为参考系，我们是在**“朝着左边”**看的，因而要“反转”。我认为这个“反转”是一个很自然的过程，不算是整个卷积的核心。 此外，在计算机领域，至少我接触到的图像处理、机器学习方面用到的卷积，其卷积核（就是例子中不断平移的函数 $\\operatorname{eff}(t)$）一般是对称的，所以这个反转的概念也不是那么必要。\n\n## 二、二维卷积\n\n### 2.1 数学定义\n\n$$ f(x, y)\\* g(x, y) = \\int\\_{\\tau\\_1=-\\infty}^\\infty \\int\\_{\\tau\\_2=-\\infty}^{\\infty} f(\\tau\\_1, \\tau\\_2) \\cdot g(x-\\tau\\_1, y-\\tau\\_2)\\,d\\tau\\_1 d\\tau\\_2 \\tag{2} $$\n\n二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式：\n\n$$ f[x,y] * g[x,y] = \\sum\\_{n\\_1=-\\infty}^\\infty \\sum\\_{n\\_2=-\\infty}^\\infty f[n\\_1, n\\_2] \\cdot g[x-n\\_1, y-n\\_2] \\tag{3} $$\n\n### 2.2 图像处理中的二维卷积\n\n二维卷积就是一维卷积的扩展，原理差不多。核心还是**（反转），移动，乘积，求和**。这里二维的反转就是将卷积核沿反对角线翻转，比如：\n$$\\begin{bmatrix} \n    a & b & c \\\\\\\n    d & e & f \\\\\\\n    g & h & i \\\\\\\n    \\end{bmatrix}\n\\text{翻转为} \\begin{bmatrix}\n    i & h & g \\\\\\\n    f & e & d \\\\\\\n    c & b & a \\\\\\\n    \\end{bmatrix}$$\n\n之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像，这个图像完全由卷积核在各个位置时的乘积求和的结果组成。\n\n举一个最简单的均值滤波的例子：\n$$ \\text{这是一个 3x3 的均值滤波核，也就是卷积核：}\n\\begin{bmatrix}\n    1/9 & 1/9 & 1/9 \\\\\\\n    1/9 & 1/9 & 1/9 \\\\\\\n    1/9 & 1/9 & 1/9 \\\\\\\n\\end{bmatrix} \\\\\\\n\\text{这是被卷积图像，这里简化为一个二维 5x5 矩阵：}\n\\begin{bmatrix}\n    3 & 3 & 3 & 3 & 3 \\\\\\\n    4 & 4 & 4 & 4 & 4 \\\\\\\n    5 & 5 & 5 & 5 & 5 \\\\\\\n    6 & 6 & 6 & 6 & 6 \\\\\\\n    7 & 7 & 7 & 7 & 7 \\\\\\\n\\end{bmatrix} \\\\\\\n$$\n\n当卷积核运动到图像右下角处（卷积中心和图像对应图像第 4 行第 4 列）时，它和图像卷积的结果如下图所示：\n\n{% asset_img 2d-convolution.png 二维卷积示例 %}\n\n可以看出，二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是核的大小）加权求和得到该像素点的输出值。滤波器核在这里是作为一个“权重表”来使用的。\n\n参考资料：\n---\n1. [中文维基百科/卷积](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF)\n2. [斯坦福大学CS178课程资料（有一个卷积的在线Applet演示）](https://graphics.stanford.edu/courses/cs178/applets/convolution.html)\n3. [Understanding Convolution（用图和例子从一维卷积一直讲到了CNN）](http://colah.github.io/posts/2014-07-Understanding-Convolutions)\n\n版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/convolution.md","raw":"---\ntitle: 我对卷积的理解\ndate: 2015-10-06 20:05:00\ntags:\n  - 数学\n  - 卷积\n  - 图像处理\n  - 机器学习\n  - 基础概念\n  - Gabor 特征\ncategory: 图像处理\nmathjax: true\n---\n{% asset_img banner.jpeg %}\n\n在学习机器学习和图像处理的过程中，经常会遇到卷积这个概念。我每次遇到这个概念都有点似懂非懂的样子。有时候清楚它的直观解释，但又搞不清公式中是如何体现的。究其原因，还是我没有完全搞懂这个概念。 维基百科上有一个动态图来演示这个概念，但对于我来说还是有些复杂。于是自己在网上找了很多文章来研究，终于有了比较直观的印象，这里就趁热把我理解的解释一下，作为总结。\n\n## 一、一维卷积\n\n### 1.1 数学定义\n\n维基百科上，卷积的形式化定义如下：\n$$ f(x)\\*g(x) = \\int\\_{-\\infty}^{\\infty} f(\\tau)g(x-\\tau) d\\tau \\tag{1}\\label{1} $$\n\n### 1.2 直观解释\n\n先来分析一下这个公式：\n\n1. $f(x)\\*g(x)$ 表示 $f(x)$ 和 $g(x)$ 的卷积，注意此处自变量为 $x$；\n2. 它是对 $(-\\infty, \\infty)$ 区间上对 $\\tau$ 求积分；\n3. 积分对象为两个函数的乘积：$f(\\tau)$ 和 $g(x-\\tau)$。\n4. 等式右边只有 $g(x-\\tau)$ 提到了 $x$，其他部分都在关注 $\\tau$\n\n这样一个公式恐怕还是难以理解，接下来将通过一个例子来进行解释。\n\n<!-- more -->\n### 1.3 例子\n\n试想小明有一段时间每天都要去输液，输的药会在身体里残留直至失效，药效随着时间是不断衰落的。 这里为简便起见，假设药效 4 天就失效，而且药效持续函数是离散的。如下图所示：\n\n{% asset_img conv-effect-function.png 药效持续函数 %}\n\n图中，横坐标为天数，纵坐标为药效。输液当天（day=0）药效为 100%，第二天减弱为 80%，第三天减弱为 40%，第四天减弱为 0。\n\n现在先定义一些符号：\n记天数为 $t$，每天输液的药量为 $\\operatorname{m}(t)$, 药效函数为 $\\operatorname{eff}(t)$，小明身上残留的药效为 $\\operatorname{rest}(t)$\n其中药效函数：\n$$\\operatorname{eff}(t) = \n\\begin{cases} 100 \\% & \\text{t=0}  \\\\\\\n80 \\% & \\text{t=1}  \\\\\\\n40 \\% & \\text{t=2}  \\\\\\\n0  \\% & \\text{t>=3}  \\\\\\\n\\end{cases}$$\n\n下面观察一下小明从第一天起，连续三天输液后身上所留下的药效（假设每天药量固定为10）。\n- 第一天，小明去医院输完液后，药效为 10（$ \\operatorname{rest}(t) = \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）。\n\n{% asset_img conv-effect-day1.png 第一天累积的药效示意 %}\n\n- 第二天，小明去医院准备输液\n    - 输液前，他身上带着前一天的药效，此时已经衰减为 10$\\cdot$ 80%=8，即 $ \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) $。\n    - 输液后，他身上携带的药效为：8 + 10 = 18（$ \\operatorname{rest}(t) = \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) + \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）\n\n{% asset_img conv-effect-day2.png 第二天累积的药效示意 %}\n\n- 第三天，小明去医院准备输液\n    - 输液前，他身上带着前两天的药效，第一天的此时已衰减为 10$\\cdot$ 40%=4（$ \\operatorname{m}(t-2)\\cdot \\operatorname{eff}(2) $），第二天的此时衰减为 10$\\cdot$ 80%=8（$ \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) $）。\n    - 输液后，他身上携带的药效为：4 + 8 + 10 = 22（$ \\operatorname{rest}(t) = \\operatorname{m}(t-2)\\cdot \\operatorname{eff}(2) + \\operatorname{m}(t-1)\\cdot \\operatorname{eff}(1) + \\operatorname{m}(t)\\cdot \\operatorname{eff}(0) $）。\n\n{% asset_img conv-effect-day3.png 第三天累积的药效示意 %}\n\n### 1.4 分析\n\n从上面的分析我们可以得到，小明第 t 天身上残留的药效 $\\operatorname{rest}(t) = \\sum\\_{i=1}^n \\operatorname{m}(t-i) \\operatorname{eff}(i)$，其中 $n$ 为药效有效的最大天数。 我们不难想象，但药效函数 $\\operatorname{eff}(t)$ 为连续时，上式中的求和就应改为积分；而当药效能无限期有效时，上式中 $n$ 就为 $\\infty$。 无限期有效的药效函数，所对应的 $\\operatorname{rest}(t) = \\int\\_{-\\infty}^\\infty \\operatorname{m}(t-\\tau) \\operatorname{eff}(\\tau) \\,d\\tau$（本例中严格来说应该是 $\\int\\_0^\\infty$ ，这里推广到了 $(-\\infty, \\infty)$）。推导到这里，基本就是维基百科上卷积的定义了。\n\n### 1.5 总结\n\n我之前对卷积概念的困惑主要是因为对公式 $\\ref{1}$ 的那个 $\\tau$ 的意义理解错了，总以为 $\\tau$ 是随着坐标轴变化的量。 事实上，在上面举的例子中，**$\\tau$ 是作为沿着纵坐标遍历的量：它的作用是对“纵向”上，历次函数 $\\operatorname{eff}(t)$ 在当前点($t$)残余量($\\operatorname{rest}(t)$)的求和。积分也是对纵向上的积分，而非横向上沿自变量的积分**。\n\n横坐标变化的量始终为 $t$，而且在卷积中并没有明显体现出 $t$ 的变化。\n\n最后重新回顾一下上面的整个过程：比较三天以来的示意图可以发现，如果我们以“当天”而不是第 $t$ 天为参考的话，就会看到 $\\operatorname{eff}(t)$ 随着时间是在向左平移（深蓝的线表示当天，前几天的线都在其左边），然后各天衰落后的药量残余等于 $\\operatorname{eff}(t)$ 值乘上初始的药量值，最后将各天的药量残余求个和。整个过程的核心就是**“（反转），移动，乘积，求和”**，这里面“反转”的概念也好理解，就是本来 $\\operatorname{eff}(t)$ 是**“朝着右边”**走的函数，$t=0,t=1,\\cdots$，$\\operatorname{eff}(t)$ 是形容**t 天后的药量的**，然而实际例子中我们是以当天为参考系，我们是在**“朝着左边”**看的，因而要“反转”。我认为这个“反转”是一个很自然的过程，不算是整个卷积的核心。 此外，在计算机领域，至少我接触到的图像处理、机器学习方面用到的卷积，其卷积核（就是例子中不断平移的函数 $\\operatorname{eff}(t)$）一般是对称的，所以这个反转的概念也不是那么必要。\n\n## 二、二维卷积\n\n### 2.1 数学定义\n\n$$ f(x, y)\\* g(x, y) = \\int\\_{\\tau\\_1=-\\infty}^\\infty \\int\\_{\\tau\\_2=-\\infty}^{\\infty} f(\\tau\\_1, \\tau\\_2) \\cdot g(x-\\tau\\_1, y-\\tau\\_2)\\,d\\tau\\_1 d\\tau\\_2 \\tag{2} $$\n\n二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式：\n\n$$ f[x,y] * g[x,y] = \\sum\\_{n\\_1=-\\infty}^\\infty \\sum\\_{n\\_2=-\\infty}^\\infty f[n\\_1, n\\_2] \\cdot g[x-n\\_1, y-n\\_2] \\tag{3} $$\n\n### 2.2 图像处理中的二维卷积\n\n二维卷积就是一维卷积的扩展，原理差不多。核心还是**（反转），移动，乘积，求和**。这里二维的反转就是将卷积核沿反对角线翻转，比如：\n$$\\begin{bmatrix} \n    a & b & c \\\\\\\n    d & e & f \\\\\\\n    g & h & i \\\\\\\n    \\end{bmatrix}\n\\text{翻转为} \\begin{bmatrix}\n    i & h & g \\\\\\\n    f & e & d \\\\\\\n    c & b & a \\\\\\\n    \\end{bmatrix}$$\n\n之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像，这个图像完全由卷积核在各个位置时的乘积求和的结果组成。\n\n举一个最简单的均值滤波的例子：\n$$ \\text{这是一个 3x3 的均值滤波核，也就是卷积核：}\n\\begin{bmatrix}\n    1/9 & 1/9 & 1/9 \\\\\\\n    1/9 & 1/9 & 1/9 \\\\\\\n    1/9 & 1/9 & 1/9 \\\\\\\n\\end{bmatrix} \\\\\\\n\\text{这是被卷积图像，这里简化为一个二维 5x5 矩阵：}\n\\begin{bmatrix}\n    3 & 3 & 3 & 3 & 3 \\\\\\\n    4 & 4 & 4 & 4 & 4 \\\\\\\n    5 & 5 & 5 & 5 & 5 \\\\\\\n    6 & 6 & 6 & 6 & 6 \\\\\\\n    7 & 7 & 7 & 7 & 7 \\\\\\\n\\end{bmatrix} \\\\\\\n$$\n\n当卷积核运动到图像右下角处（卷积中心和图像对应图像第 4 行第 4 列）时，它和图像卷积的结果如下图所示：\n\n{% asset_img 2d-convolution.png 二维卷积示例 %}\n\n可以看出，二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是核的大小）加权求和得到该像素点的输出值。滤波器核在这里是作为一个“权重表”来使用的。\n\n参考资料：\n---\n1. [中文维基百科/卷积](https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF)\n2. [斯坦福大学CS178课程资料（有一个卷积的在线Applet演示）](https://graphics.stanford.edu/courses/cs178/applets/convolution.html)\n3. [Understanding Convolution（用图和例子从一维卷积一直讲到了CNN）](http://colah.github.io/posts/2014-07-Understanding-Convolutions)\n\n版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"convolution","published":1,"updated":"2016-05-20T18:14:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji85000d6jod293rqbis"},{"title":"Gabor 特征总结","mathjax":true,"date":"2015-10-11T09:02:00.000Z","_content":"{% asset_img banner.jpeg %}\n\nGabor 特征是一种可以用来描述图像纹理信息的特征，Gabor 滤波器的频率和方向与人类的视觉系统类似，特别适合于纹理表示与判别。\n\nGabor 特征主要依靠 Gabor 核在频率域上对信号进行加窗，从而能描述信号的局部频率信息。\n\n说到 Gabor 核，不能不提到傅里叶变换。正是靠傅里叶变换，我们才能将信号转换到频率域，才能让Gabor核在频率域去加窗。而在原本的空间域中，一个 Gabor 核实际上就是一个高斯核与正弦波调制的结果，可以看做是高斯核应用在了正弦波的频域部分。\n\n上面说的还是比较笼统，下面我们一步一步介绍Gabor核是怎么对信号“加窗”的。\n\n<!-- more -->\n# 一、傅里叶变换\n\n关于傅里叶变换，韩昊同学总结过一个[很直观的解释](http://zhuanlan.zhihu.com/wille/19763358)。我这里就不赘述了。\n\n总之，傅里叶变换是图像处理里面一个很重要的工具，本质是将任意一个函数转化为若干不同频率正弦波的组合，（组合方式在离散函数中就是相加，在连续函数中就是积分）。由此，将空域（或时域）信号转换到了频域（即频率域）。\n\n空间域中多个波叠加，在频率域中就对应着若干个散落的点。韩昊同学将其比喻为不同音阶组成的音谱。\n\n频率域中的基本元素就是正弦波：空间域中的一个正弦波波形，在频率域中只要一个点就能表示。\n\n[维基百科](https://zh.wikipedia.org/wiki/傅里叶变换)上有一个动态图，展示了一个叠加波如何分解到频率域上的若干点：\n\n![叠加波的分解过程](https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif?1444117179010)\n\n事实上，任何波都可以看做是若干（乃至无穷）个不同频率正弦波的叠加。\n\n就像可见光可以看做不同频率的光的叠加一样，通过傅里叶变换，我们能将任何波分解为不同频率波的叠加。这样转换的好处是：有些情况下空域中很复杂的问题，在频率域会变得十分简单。\n\n<!--more-->\n# 二、Gabor 核\n\n## 2.1 一维 Gabor 核\n\n### 2.1.1 一维傅里叶变换\n一维傅里叶变化定义如下：\n$$\\hat{f}(\\xi)=\\int\\_{-\\infty}^\\infty f(t) e^{-i2\\pi t \\xi}\\, dt,\\quad \\xi \\text{为任意实数} \\tag{1}\\label{1}$$\n\n其中，f 为输入信号，$\\xi$ 表示分解得到的各个波的频率，$\\hat{f}(f, \\xi)$ 为变换后的信号。公式中的 $e^{-i2\\pi x \\xi}$ 表示一个复数波，关于复数波的解释可以看我{% post_link  complex 之前的一篇文章 %}\n\n从上面的公式可以看出，原信号 $f(t)$ 以 t 为自变量，描述了信号值随时间的变化，说明原信号空间在时间域中。经过傅里叶变换后，函数自变量变为了 $\\xi$ ，$\\hat{f}(\\xi)$ 描述了信号值随频率的变化，即信号转换到了频率域空间中。如果说原来信号的图示需要以时间（空间）为坐标轴的话，信号在傅里叶变换后的图示就需要以频率为坐标轴了。\n    \n### 2.1.2 一维 Gabor 核\n一维Gabor核由一个高斯核与一个复数波的乘积定义：\n$$ Gabor(t) = ke^{i\\theta} \\omega(at) s(t) \\tag{2}\\label{2}$$\n\n其中，\n$$\\begin{cases}\n\\omega(t)=e^{-\\pi t^2} \\\\\\\ns(t) = e^{i(2\\pi f\\_0 t)} \\\\\\\n\\end{cases}$$\n这里，$f\\_0$ 是复数波$s(t)$的频率。\n\n将复数波$s(t) = e^{i(2\\pi f\\_0 t)}$代入$\\ref{2}$式中，得到：\n$$\\begin{align}\nGabor(t) & = k \\omega(at) e^{i(2\\pi f\\_0 t + \\theta)} \\\\\\\n     & = k \\omega(at) \\left[ \\cos(2\\pi f\\_0 t+\\theta) + i\\sin(2\\pi f\\_0 t+\\theta) \\right]\n\\end{align}$$\n\n上面最后一步得到了 Gabor 核的复数表示，我们就可以按实部和虚部将其拆分为实核和虚核，在很多应用中，我们只需要应用 Gabor核的实数部分即可：\n$$\\begin{cases}\nGabor\\_{real}(t) = \\omega(at)\\cos(2\\pi f\\_0 t + \\theta) \\\\\\\nGabor\\_{imag}(t) = \\omega(at)\\sin(2\\pi f\\_0 t + \\theta)\n\\end{cases}$$\n\n### 2.1.3 Gabor 核的傅里叶变换\n将 Gabor 核（式$\\ref{2}$）套入一维傅里叶变换（式$\\ref{1}$）中，得到 Gabor 核的傅里叶变换：\n$$\\begin{align}\n\\hat{Gabor}(f)\n& = ke^{i\\theta} \\int\\_{-\\infty}^{\\infty} e^{-i 2\\pi f t} \\omega(at) s(t) \\,dt \\\\\\\n& = ke^{i\\theta} \\int\\_{-\\infty}^{\\infty} e^{-i2\\pi (f-f\\_0)t} \\omega(at) \\,dt \\\\\\\n& = (k/a) \\cdot e^{i\\theta} \\cdot \\hat{\\omega}\\left( (f-f\\_0)/a \\right) \\\\\\\n\\end{align} \\tag{3}\\label{3}$$\n\n上式中出现了 $\\hat{\\omega}(\\frac{f-f\\_0}{a})$ 的形式，这里需要补充高斯核一个很有趣的性质：$\\hat{\\omega}(f) = \\omega(f) = e^{-\\pi f^2}$，这个性质这里就不证明了，有兴趣的同学可以自己推导一下。根据这个性质，上式中的 $\\hat{\\omega}(\\frac{f-f\\_0}{a})$ 也可以写作 $\\omega(\\frac{f-f\\_0}{a})$，二者可以自由转换。\n\n此外，$\\ref{3}$式中的末尾，我们知道了Gabor核傅里叶变换后是这样一个形式：$\\frac{k}{a} e^{i\\theta} \\hat{\\omega}(\\frac{f-f\\_0}{a})$，这个形式可以看做是一个复数波，它的幅度\n$$A = \\left\\lVert \\hat{Gabor}(f) \\right\\rVert = \\frac{k}{a} \\hat{\\omega}(\\frac{f-f\\_0}{a}) = \\frac{k}{a} \\omega(\\frac{f-f\\_0}{a})  $$\n\n也就是说，Gabor核相当于在频率域应用了一个高斯核窗口。假设我们这时有了一个信号的频率域：$f\\_{in}(f)$，那么我们直接用频率域的Gabor核 $\\hat{Gabor}$ 与其相乘，就实现了对 $f\\_0$ 频率邻域范围内的滤波效果：输入信号频率离这个 Gabor 核的 $f\\_0$ 越远，则乘上Gabor核之后的结果就越小，尤其是当 $f\\_{in}$ 在 $f\\_0$ 的 $3\\sigma$ 区间外时，这个频率几乎可以忽略不计。于是，最终能保留下来的信号就都是 $f\\_0$ 频率附近的信号了。\n\n这个想法，用公式表示出来就是：\n$$ \\hat{Gabor} \\cdot \\hat{f\\_{in}} $$\n\n从这个角度出发，给我们任意一个输入信号，我们先用傅里叶变换将其变换到频率域得到$\\hat{f\\_{in}}$，再用 Gabor 核的傅里叶变换结果与之相乘，就是频域滤波的结果了。\n\n不过我们大可不必这么麻烦，因为有卷积定理：\n$$Gabor \\* f\\_{in} = \\hat{Gabor} \\cdot \\hat{f\\_{in}} $$\n这样看来，我们只需要用 Gabor 核和输入信号卷积就可以得到输入信号在某频率邻域附近的响应结果！！\n\n我们既可以用这个响应结果来实现频域滤波，又可以用它来描述信号的频率信息。下面要提到的Gabor特征，就是用Gabor核来描述信号的频率信息，从而作为信号的特征的。\n\n## 2.2 二维 Gabor 变换\n\n将上面的一维情况推广至二维：\n\n### 2.2.1 二维傅里叶变换：\n二维傅里叶变换定义如下：\n$$ \\hat{f}(\\xi\\_x, \\xi\\_y) = \\iint f(x,y) e^{-i2\\pi (\\xi\\_x x + \\xi\\_y y)}\\, dx dy$$\n\n为了简洁，改用 $(u\\_0, v\\_0)$ 来代替 $(\\xi\\_x, \\xi\\_y)$，则上式可写为：\n$$ \\hat{f}(u\\_0, v\\_0) = \\iint f(x, y) \\exp {\\left( -i2\\pi {\\left( u\\_0 x + v\\_0 y\\right) }\\right) } \\, dxdy \\tag{4}\\label{4}$$\n提醒一下，这里 $(x, y)$ 表示空域坐标，$(u\\_0, v\\_0)$ 表示频域坐标。\n\n### 2.2.2 二维复数波\n二维复数波完整定义如下（用复指数形式表示）：\n$$ s(x,y) = \\exp\\left( i \\left(2\\pi (u\\_0 x + v\\_0 y) + P \\right) \\right) $$\n\n由于初始相位对Gabor核影响不大，因此可以将其省略，得到更简洁的形式（论文中关于 Gabor 函数的定义各不一样，主要是这些细节的考虑不同）：\n$$ s(x,y) = \\exp \\left(i \\left(2\\pi (u\\_0 x + v\\_0 y) \\right) \\right) $$\n\n### 2.2.3 二维高斯函数\n二维高斯函数定义如下：\n$$ \\omega(x, y, \\sigma_x, \\sigma_y) = K \\exp(-\\pi \\left (x-x\\_0)^2 / \\sigma\\_x^2 + (y-y\\_0)^2 / \\sigma\\_y^2\\right ) \\tag{5}\\label{5}$$\n其中，$\\sigma_x, \\sigma_y$ 分别为两个方向上的尺度参数（scaling parameters），用来控制高斯函数在两个方向上的“展布”形状。$(x\\_0, y\\_0)$ 为高斯函数的中心点。$K$ 为常数。\n\n考虑全面的话，高斯函数还要有（顺时针）旋转，即：\n$$\\begin{cases}\n(x-x\\_0)\\_r = (x-x\\_0)\\cos \\theta + (y-y\\_0)\\sin \\theta \\\\\\\n(y-y\\_0)\\_r = -(y-y\\_0)\\sin \\theta + (y-y\\_0)\\cos \\theta\n\\end{cases}$$\n\n加入旋转参数后的二维高斯函数为：\n$$ \\omega\\_r(x, y, \\theta, \\sigma\\_x, \\sigma\\_y) = K \\exp(-\\pi \\left (x-x\\_0)\\_r^2 / \\sigma\\_x^2 + (y-y\\_0)\\_r^2 / \\sigma\\_y^2\\right )$$\n\n{% asset_img 2d-gaussian.png 二维高斯 %}\n\n上图即是一个二维高斯核的图像，该高斯核中，$(x\\_0, y\\_0) = (0, 0)$，$(\\sigma\\_x^2, \\sigma\\_y^2) = (50, 40)$，$\\theta = -45°$\n\n从图像可以看出，$\\sigma\\_x 和 \\sigma\\_y$分别控制了高斯两个方向的“展布”情况。\n\n### 2.2.4 Gabor 滤波器核\n类似一维 Gabor 核，我们将二维高斯函数与二维复数波相乘，就得到了二维的Gabor核：\n$$\\begin{align}\nGabor(x\\_0, y\\_0, \\theta, \\sigma\\_x, \\sigma\\_y, u\\_0, v\\_0)  \n& = s(x,y) \\omega\\_r(x,y) \\\\\\\n& = K \\exp\\left(-\\pi \\left( (x-x\\_0)\\_r^2/\\sigma\\_x^2 + (y-y\\_0)\\_r^2/\\sigma\\_y^2 \\right) \\right) \\exp\\left(i 2\\pi (u\\_0 x + v\\_0 y) \\right) \\\\\\\n\\end{align}$$\n\n它的各个参数含义如下：\n- $(x\\_0, y\\_0)$: 高斯核的中心点\n- $\\theta$: 高斯核的旋转方向（顺时针）\n- $(\\sigma_x, \\sigma_y)$: 高斯核两个方向上的尺度\n- $(u\\_0, v\\_0)$: 频域坐标\n- $K$: 高斯核的幅度（magnitude）的比例\n\n{% asset_img gabor-filter-frequency.png Gabor 核频率域图示 %}\n\n上图为Gabor核在频率域中的图示，这个Gabor核就是从之前那个高斯核得到的，其参数分别为：$u\\_0 = v\\_0 = 1/80$，$x\\_0 = y\\_0 = 0$，$\\sigma\\_x^2 = 50$，$\\sigma\\_y^2 = 40$，$\\theta = -45°$，$F\\_0 = \\sqrt{2}/80$，$\\omega\\_0=45°$。\n\n{% asset_img gabor-filter-spatial.png Gabor 核空间域图示 %}\n\n上图为Gabor核在空间域中的图示，参数和上面那个Gabor核一样。图像左边是实部，右边是虚部。这样的Gabor核与图像进行卷积，我们便能得到图像在$(u\\_0, v\\_0)$频率附近的响应情况。在图像处理中，通常使用Gabor的实部进行卷积就可以。\n\n# 三、Gabor 核作为图像特征\n\n通过上面的分析，我们知道了，一个Gabor核能获取到图像某个频率邻域的响应情况，这个响应结果可以看做是图像的一个特征。那么，我们如果用多个不同频率的Gabor核去获取图像在不同频率邻域的响应情况，最后就能形成图像在各个频率段的特征，这个特征就可以描述图像的频率信息了\n\n {% asset_img gabor-filter-banks.png  一系列 Gabor 核 %}\n\n上图展示了一系列具有不同频率的 Gabor 核，用这些核与图像卷积，我们就能得到图像上每个点和其附近区域的频率分布情况。\n\n由于纹理特征通常和频率相关，因此Gabor核经常用来作为纹理特征。又因为字符识别问题通常都是识别纹理的过程，所以Gabor核在光学字符识别（OCR）系统中也有广泛应用。\n\n# 写在最后\n由于本人对信号处理不是太了解，因此对傅里叶变换、频率域的理解都是个人粗浅的理解。为了完成这篇文章，我学习了很多信号处理的知识，重新理解了一些基本概念，看别人的帖子建立过一些认识，随后这层理解不牢又被推翻，再重新建立……前前后后用了一周的时间才最终完成。如有不严谨或错误的地方，还请大家谅解。要严肃学习的话最好还是看权威教材、看论文，我这篇文章可以作为另一个角度的补充。\n\n# 参考资料\n1. [中文维基百科 / 傅里叶变换](https://zh.wikipedia.org/wiki/傅里叶变换)\n2. [韩昊同学对傅里叶变换的直观解释](http://zhuanlan.zhihu.com/wille/19763358)\n3. [中文维基百科 / 卷积定理](https://zh.wikipedia.org/wiki/卷积定理)\n4. [英文维基百科 / Gabor_filter](http://en.wikipedia.org/Gabor_filter)\n5. [英文维基百科 / Gabor_transform](https://en.wikipedia.org/wiki/Gabor_transform)\n6. Movellan J R. Tutorial on Gabor filters[J]. Open Source Document, 2002.\n7. Idrissa M, Acheroy M. Texture classification using Gabor filters[J]. Pattern Recognition Letters, 2002, 23(9): 1095-1102.\n\n# 版权声明\n本文所有文字版权均属本人所有，如需转载请注明来源。\n","source":"_posts/gabor.md","raw":"---\ntitle: Gabor 特征总结\ntags:\n  - 图像处理\n  - 机器学习\n  - 特征提取\n  - 傅里叶变换\n  - Gabor\n  - Gabor 特征\ncategory: 图像处理\nmathjax: true\ndate: 2015-10-11 17:02:00\n---\n{% asset_img banner.jpeg %}\n\nGabor 特征是一种可以用来描述图像纹理信息的特征，Gabor 滤波器的频率和方向与人类的视觉系统类似，特别适合于纹理表示与判别。\n\nGabor 特征主要依靠 Gabor 核在频率域上对信号进行加窗，从而能描述信号的局部频率信息。\n\n说到 Gabor 核，不能不提到傅里叶变换。正是靠傅里叶变换，我们才能将信号转换到频率域，才能让Gabor核在频率域去加窗。而在原本的空间域中，一个 Gabor 核实际上就是一个高斯核与正弦波调制的结果，可以看做是高斯核应用在了正弦波的频域部分。\n\n上面说的还是比较笼统，下面我们一步一步介绍Gabor核是怎么对信号“加窗”的。\n\n<!-- more -->\n# 一、傅里叶变换\n\n关于傅里叶变换，韩昊同学总结过一个[很直观的解释](http://zhuanlan.zhihu.com/wille/19763358)。我这里就不赘述了。\n\n总之，傅里叶变换是图像处理里面一个很重要的工具，本质是将任意一个函数转化为若干不同频率正弦波的组合，（组合方式在离散函数中就是相加，在连续函数中就是积分）。由此，将空域（或时域）信号转换到了频域（即频率域）。\n\n空间域中多个波叠加，在频率域中就对应着若干个散落的点。韩昊同学将其比喻为不同音阶组成的音谱。\n\n频率域中的基本元素就是正弦波：空间域中的一个正弦波波形，在频率域中只要一个点就能表示。\n\n[维基百科](https://zh.wikipedia.org/wiki/傅里叶变换)上有一个动态图，展示了一个叠加波如何分解到频率域上的若干点：\n\n![叠加波的分解过程](https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif?1444117179010)\n\n事实上，任何波都可以看做是若干（乃至无穷）个不同频率正弦波的叠加。\n\n就像可见光可以看做不同频率的光的叠加一样，通过傅里叶变换，我们能将任何波分解为不同频率波的叠加。这样转换的好处是：有些情况下空域中很复杂的问题，在频率域会变得十分简单。\n\n<!--more-->\n# 二、Gabor 核\n\n## 2.1 一维 Gabor 核\n\n### 2.1.1 一维傅里叶变换\n一维傅里叶变化定义如下：\n$$\\hat{f}(\\xi)=\\int\\_{-\\infty}^\\infty f(t) e^{-i2\\pi t \\xi}\\, dt,\\quad \\xi \\text{为任意实数} \\tag{1}\\label{1}$$\n\n其中，f 为输入信号，$\\xi$ 表示分解得到的各个波的频率，$\\hat{f}(f, \\xi)$ 为变换后的信号。公式中的 $e^{-i2\\pi x \\xi}$ 表示一个复数波，关于复数波的解释可以看我{% post_link  complex 之前的一篇文章 %}\n\n从上面的公式可以看出，原信号 $f(t)$ 以 t 为自变量，描述了信号值随时间的变化，说明原信号空间在时间域中。经过傅里叶变换后，函数自变量变为了 $\\xi$ ，$\\hat{f}(\\xi)$ 描述了信号值随频率的变化，即信号转换到了频率域空间中。如果说原来信号的图示需要以时间（空间）为坐标轴的话，信号在傅里叶变换后的图示就需要以频率为坐标轴了。\n    \n### 2.1.2 一维 Gabor 核\n一维Gabor核由一个高斯核与一个复数波的乘积定义：\n$$ Gabor(t) = ke^{i\\theta} \\omega(at) s(t) \\tag{2}\\label{2}$$\n\n其中，\n$$\\begin{cases}\n\\omega(t)=e^{-\\pi t^2} \\\\\\\ns(t) = e^{i(2\\pi f\\_0 t)} \\\\\\\n\\end{cases}$$\n这里，$f\\_0$ 是复数波$s(t)$的频率。\n\n将复数波$s(t) = e^{i(2\\pi f\\_0 t)}$代入$\\ref{2}$式中，得到：\n$$\\begin{align}\nGabor(t) & = k \\omega(at) e^{i(2\\pi f\\_0 t + \\theta)} \\\\\\\n     & = k \\omega(at) \\left[ \\cos(2\\pi f\\_0 t+\\theta) + i\\sin(2\\pi f\\_0 t+\\theta) \\right]\n\\end{align}$$\n\n上面最后一步得到了 Gabor 核的复数表示，我们就可以按实部和虚部将其拆分为实核和虚核，在很多应用中，我们只需要应用 Gabor核的实数部分即可：\n$$\\begin{cases}\nGabor\\_{real}(t) = \\omega(at)\\cos(2\\pi f\\_0 t + \\theta) \\\\\\\nGabor\\_{imag}(t) = \\omega(at)\\sin(2\\pi f\\_0 t + \\theta)\n\\end{cases}$$\n\n### 2.1.3 Gabor 核的傅里叶变换\n将 Gabor 核（式$\\ref{2}$）套入一维傅里叶变换（式$\\ref{1}$）中，得到 Gabor 核的傅里叶变换：\n$$\\begin{align}\n\\hat{Gabor}(f)\n& = ke^{i\\theta} \\int\\_{-\\infty}^{\\infty} e^{-i 2\\pi f t} \\omega(at) s(t) \\,dt \\\\\\\n& = ke^{i\\theta} \\int\\_{-\\infty}^{\\infty} e^{-i2\\pi (f-f\\_0)t} \\omega(at) \\,dt \\\\\\\n& = (k/a) \\cdot e^{i\\theta} \\cdot \\hat{\\omega}\\left( (f-f\\_0)/a \\right) \\\\\\\n\\end{align} \\tag{3}\\label{3}$$\n\n上式中出现了 $\\hat{\\omega}(\\frac{f-f\\_0}{a})$ 的形式，这里需要补充高斯核一个很有趣的性质：$\\hat{\\omega}(f) = \\omega(f) = e^{-\\pi f^2}$，这个性质这里就不证明了，有兴趣的同学可以自己推导一下。根据这个性质，上式中的 $\\hat{\\omega}(\\frac{f-f\\_0}{a})$ 也可以写作 $\\omega(\\frac{f-f\\_0}{a})$，二者可以自由转换。\n\n此外，$\\ref{3}$式中的末尾，我们知道了Gabor核傅里叶变换后是这样一个形式：$\\frac{k}{a} e^{i\\theta} \\hat{\\omega}(\\frac{f-f\\_0}{a})$，这个形式可以看做是一个复数波，它的幅度\n$$A = \\left\\lVert \\hat{Gabor}(f) \\right\\rVert = \\frac{k}{a} \\hat{\\omega}(\\frac{f-f\\_0}{a}) = \\frac{k}{a} \\omega(\\frac{f-f\\_0}{a})  $$\n\n也就是说，Gabor核相当于在频率域应用了一个高斯核窗口。假设我们这时有了一个信号的频率域：$f\\_{in}(f)$，那么我们直接用频率域的Gabor核 $\\hat{Gabor}$ 与其相乘，就实现了对 $f\\_0$ 频率邻域范围内的滤波效果：输入信号频率离这个 Gabor 核的 $f\\_0$ 越远，则乘上Gabor核之后的结果就越小，尤其是当 $f\\_{in}$ 在 $f\\_0$ 的 $3\\sigma$ 区间外时，这个频率几乎可以忽略不计。于是，最终能保留下来的信号就都是 $f\\_0$ 频率附近的信号了。\n\n这个想法，用公式表示出来就是：\n$$ \\hat{Gabor} \\cdot \\hat{f\\_{in}} $$\n\n从这个角度出发，给我们任意一个输入信号，我们先用傅里叶变换将其变换到频率域得到$\\hat{f\\_{in}}$，再用 Gabor 核的傅里叶变换结果与之相乘，就是频域滤波的结果了。\n\n不过我们大可不必这么麻烦，因为有卷积定理：\n$$Gabor \\* f\\_{in} = \\hat{Gabor} \\cdot \\hat{f\\_{in}} $$\n这样看来，我们只需要用 Gabor 核和输入信号卷积就可以得到输入信号在某频率邻域附近的响应结果！！\n\n我们既可以用这个响应结果来实现频域滤波，又可以用它来描述信号的频率信息。下面要提到的Gabor特征，就是用Gabor核来描述信号的频率信息，从而作为信号的特征的。\n\n## 2.2 二维 Gabor 变换\n\n将上面的一维情况推广至二维：\n\n### 2.2.1 二维傅里叶变换：\n二维傅里叶变换定义如下：\n$$ \\hat{f}(\\xi\\_x, \\xi\\_y) = \\iint f(x,y) e^{-i2\\pi (\\xi\\_x x + \\xi\\_y y)}\\, dx dy$$\n\n为了简洁，改用 $(u\\_0, v\\_0)$ 来代替 $(\\xi\\_x, \\xi\\_y)$，则上式可写为：\n$$ \\hat{f}(u\\_0, v\\_0) = \\iint f(x, y) \\exp {\\left( -i2\\pi {\\left( u\\_0 x + v\\_0 y\\right) }\\right) } \\, dxdy \\tag{4}\\label{4}$$\n提醒一下，这里 $(x, y)$ 表示空域坐标，$(u\\_0, v\\_0)$ 表示频域坐标。\n\n### 2.2.2 二维复数波\n二维复数波完整定义如下（用复指数形式表示）：\n$$ s(x,y) = \\exp\\left( i \\left(2\\pi (u\\_0 x + v\\_0 y) + P \\right) \\right) $$\n\n由于初始相位对Gabor核影响不大，因此可以将其省略，得到更简洁的形式（论文中关于 Gabor 函数的定义各不一样，主要是这些细节的考虑不同）：\n$$ s(x,y) = \\exp \\left(i \\left(2\\pi (u\\_0 x + v\\_0 y) \\right) \\right) $$\n\n### 2.2.3 二维高斯函数\n二维高斯函数定义如下：\n$$ \\omega(x, y, \\sigma_x, \\sigma_y) = K \\exp(-\\pi \\left (x-x\\_0)^2 / \\sigma\\_x^2 + (y-y\\_0)^2 / \\sigma\\_y^2\\right ) \\tag{5}\\label{5}$$\n其中，$\\sigma_x, \\sigma_y$ 分别为两个方向上的尺度参数（scaling parameters），用来控制高斯函数在两个方向上的“展布”形状。$(x\\_0, y\\_0)$ 为高斯函数的中心点。$K$ 为常数。\n\n考虑全面的话，高斯函数还要有（顺时针）旋转，即：\n$$\\begin{cases}\n(x-x\\_0)\\_r = (x-x\\_0)\\cos \\theta + (y-y\\_0)\\sin \\theta \\\\\\\n(y-y\\_0)\\_r = -(y-y\\_0)\\sin \\theta + (y-y\\_0)\\cos \\theta\n\\end{cases}$$\n\n加入旋转参数后的二维高斯函数为：\n$$ \\omega\\_r(x, y, \\theta, \\sigma\\_x, \\sigma\\_y) = K \\exp(-\\pi \\left (x-x\\_0)\\_r^2 / \\sigma\\_x^2 + (y-y\\_0)\\_r^2 / \\sigma\\_y^2\\right )$$\n\n{% asset_img 2d-gaussian.png 二维高斯 %}\n\n上图即是一个二维高斯核的图像，该高斯核中，$(x\\_0, y\\_0) = (0, 0)$，$(\\sigma\\_x^2, \\sigma\\_y^2) = (50, 40)$，$\\theta = -45°$\n\n从图像可以看出，$\\sigma\\_x 和 \\sigma\\_y$分别控制了高斯两个方向的“展布”情况。\n\n### 2.2.4 Gabor 滤波器核\n类似一维 Gabor 核，我们将二维高斯函数与二维复数波相乘，就得到了二维的Gabor核：\n$$\\begin{align}\nGabor(x\\_0, y\\_0, \\theta, \\sigma\\_x, \\sigma\\_y, u\\_0, v\\_0)  \n& = s(x,y) \\omega\\_r(x,y) \\\\\\\n& = K \\exp\\left(-\\pi \\left( (x-x\\_0)\\_r^2/\\sigma\\_x^2 + (y-y\\_0)\\_r^2/\\sigma\\_y^2 \\right) \\right) \\exp\\left(i 2\\pi (u\\_0 x + v\\_0 y) \\right) \\\\\\\n\\end{align}$$\n\n它的各个参数含义如下：\n- $(x\\_0, y\\_0)$: 高斯核的中心点\n- $\\theta$: 高斯核的旋转方向（顺时针）\n- $(\\sigma_x, \\sigma_y)$: 高斯核两个方向上的尺度\n- $(u\\_0, v\\_0)$: 频域坐标\n- $K$: 高斯核的幅度（magnitude）的比例\n\n{% asset_img gabor-filter-frequency.png Gabor 核频率域图示 %}\n\n上图为Gabor核在频率域中的图示，这个Gabor核就是从之前那个高斯核得到的，其参数分别为：$u\\_0 = v\\_0 = 1/80$，$x\\_0 = y\\_0 = 0$，$\\sigma\\_x^2 = 50$，$\\sigma\\_y^2 = 40$，$\\theta = -45°$，$F\\_0 = \\sqrt{2}/80$，$\\omega\\_0=45°$。\n\n{% asset_img gabor-filter-spatial.png Gabor 核空间域图示 %}\n\n上图为Gabor核在空间域中的图示，参数和上面那个Gabor核一样。图像左边是实部，右边是虚部。这样的Gabor核与图像进行卷积，我们便能得到图像在$(u\\_0, v\\_0)$频率附近的响应情况。在图像处理中，通常使用Gabor的实部进行卷积就可以。\n\n# 三、Gabor 核作为图像特征\n\n通过上面的分析，我们知道了，一个Gabor核能获取到图像某个频率邻域的响应情况，这个响应结果可以看做是图像的一个特征。那么，我们如果用多个不同频率的Gabor核去获取图像在不同频率邻域的响应情况，最后就能形成图像在各个频率段的特征，这个特征就可以描述图像的频率信息了\n\n {% asset_img gabor-filter-banks.png  一系列 Gabor 核 %}\n\n上图展示了一系列具有不同频率的 Gabor 核，用这些核与图像卷积，我们就能得到图像上每个点和其附近区域的频率分布情况。\n\n由于纹理特征通常和频率相关，因此Gabor核经常用来作为纹理特征。又因为字符识别问题通常都是识别纹理的过程，所以Gabor核在光学字符识别（OCR）系统中也有广泛应用。\n\n# 写在最后\n由于本人对信号处理不是太了解，因此对傅里叶变换、频率域的理解都是个人粗浅的理解。为了完成这篇文章，我学习了很多信号处理的知识，重新理解了一些基本概念，看别人的帖子建立过一些认识，随后这层理解不牢又被推翻，再重新建立……前前后后用了一周的时间才最终完成。如有不严谨或错误的地方，还请大家谅解。要严肃学习的话最好还是看权威教材、看论文，我这篇文章可以作为另一个角度的补充。\n\n# 参考资料\n1. [中文维基百科 / 傅里叶变换](https://zh.wikipedia.org/wiki/傅里叶变换)\n2. [韩昊同学对傅里叶变换的直观解释](http://zhuanlan.zhihu.com/wille/19763358)\n3. [中文维基百科 / 卷积定理](https://zh.wikipedia.org/wiki/卷积定理)\n4. [英文维基百科 / Gabor_filter](http://en.wikipedia.org/Gabor_filter)\n5. [英文维基百科 / Gabor_transform](https://en.wikipedia.org/wiki/Gabor_transform)\n6. Movellan J R. Tutorial on Gabor filters[J]. Open Source Document, 2002.\n7. Idrissa M, Acheroy M. Texture classification using Gabor filters[J]. Pattern Recognition Letters, 2002, 23(9): 1095-1102.\n\n# 版权声明\n本文所有文字版权均属本人所有，如需转载请注明来源。\n","slug":"gabor","published":1,"updated":"2016-05-20T18:14:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji87000f6joda7gzav9x"},{"title":"线性代数拾遗（一 ）：线性方程组、向量方程和矩阵方程","mathjax":true,"date":"2016-05-03T14:55:27.000Z","_content":"\n{% asset_img banner.jpeg %}\n\n# 前言\n线性代数在各大理工科，乃至经济金融领域的使用之广泛，毋庸置疑。 一直以来，我虽也知道线性代数的重要，但从内心上其实一直是犯怵的（尤其是学习论文、算法中，基本只要看到对方把算法向量化之后就蒙圈了），当年在学校学习过程中很多也是靠着死记硬背过来的，对它的直观意义一直都没能有很好的理解。\n\n最近，这么一本书进入了我的视线：《线性代数及其应用》，听书名感觉平平，但只翻了几页就感觉十分过瘾，仿佛打通了任督二脉。以往很多死记硬背的知识点在这本书的解释下，变成了可以直观推导出来的结果。这本书不仅对线性代数的基本概念阐述地很直观形象，而且还有许多现实生活中的应用，特别是经济、物理、计算机领域，真正让人领略到线性代数作为现代数学的魅力。\n\n我特将自己的读书总结和体会记录于此，也是希望借此加深自己的理解。\n\n注意，这个系列假设你已经有了线性代数基础，像是行变换、将矩阵转换为行阶梯形式这种基本技巧已经掌握。本文不再赘述具体操作步骤，主要关注于概念的直观理解。\n\n<!-- more -->\n# 线性方程组、向量方程和矩阵方程\n## 一、线性方程组\n线性代数，最基本的问题，就是解线性方程组了。线性方程组就是一组形如 $a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b$ 的方程。一个线性方程组中的变量是相同的，如果第一个方程是关于 $x_1 \\cdots x_n$ 的，那么其他的也都应该如此（这些变量不一定都出现，因为系数可以有 0）。\n### 1.1 线性方程组的矩阵形式\n方程组\n\n{% math %}\n\\left\\{\n\\begin{equation}\\label{initial}\n\\begin{array}{ccl}\nx_1 &-& 2x_2 &+& x_3 &= 0 \\\\\n    && 2x_2 &-& 8x_3 &= 8 \\\\\n{-4x_1} &+& 5x_2 &+& 9x_3 &= {-9}\n\\end{array}\n\\end{equation}\n\\right.\n{% endmath %}\n\n可以通过増广矩阵形式描述：\n{% math %}\n\\begin{bmatrix}\n1 & -2 & 1 & 0\\\\\n0 & 2 & -8 & 8\\\\\n-4 & 5 & 9 & -9\\\\\n\\end{bmatrix}\n{% endmath %}\n增广矩阵去掉最后一列，就是该方程组的系数矩阵。\n\n矩阵形式只是线性方程组的一种表示形式。今后的很多关于线性方程组的计算，都将在矩阵形式上进行操作，然而你也需要知道，在这些操作进行的同时，线性方程组也在进行类似的变换。比如，将增广矩阵的第一、二行对换，那么同时，它所代表的线性方程组中，第一、二个方程也进行了对调。\n\n### 1.2 线性方程组的解\n解一个线性方程组，就是通过对其矩阵形式行变换（三种方式：交换方程的先后顺序，一个方程左右同乘以某数，和两个方程相加） 转换为行阶梯形式。比如\n\n{% math %}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\\\\n0 & 2 & -8 & 8 & \\\\\n-4 & 5 & 9 & -9 &\n\\end{bmatrix}\n\\text{转化为行阶梯形式：}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\\\\n0 & 1 & -4 & 4 & \\\\\n0 & 0 & 1 & 3\n\\end{bmatrix}\n\\text{和最简形式：}\n\\begin{bmatrix}\n1 & 0 & 0 & 29 & \\\\\n0 & 1 & 0 & 16 & \\\\\n0 & 0 & 1 & 3\n\\end{bmatrix}\n{% endmath %}\n\n上面最简形式的矩阵对应的线性方程组是\n\n{% math %}\n\\left\\{\n\\begin{array}{ccl}\nx_1 && && &= 29 \\\\\n    && x_2 && &= 16 \\\\\n&& && x_3 &= 3\n\\end{array}\n\\right.\n{% endmath %}\n\n这个线性方程组和一开始的方程组是等价的，只是处于不同的状态，它们的解也是相同的，而显然行最简形式的方程组最容易解，所以我们一般都**将线性方程组的増广矩阵转化为行最简形式继而求解**。\n\n### 1.3 解的存在性和唯一性\n还记得线性代数时经常讨论的“无解““唯一解”“无穷多解”吧？\n\n首先来看刚才的方程组，经过行变换后，方程组的解已经很显然了：$x_1 = 29, x_2 =16, x_3 = 3$。这个方程组的解就只有一个，是唯一解。\n\n#### 1.3.1 无解\n我们再来看一个方程组：\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\n&& x_2 &-& 4x_3 &= 8 \\\\\n2x_1 &-& 3x_2 &+& 2x_3 &= 1\\\\\n5x_1 &-& 8x_2 &+& 7x_3 &= 1\n\\end{array}\n\\right.\n{% endmath %}\n\n它的增广矩阵\n{% math %}\n\\begin{bmatrix}\n0 & 1 & -4 & 8 & \\\\\n2 & -3 & 2 & 1 & \\\\\n5 & -8 & 7 & 1\n\\end{bmatrix}\n\\text{ 行变换得到：}\n\\begin{bmatrix}\n2 & -3 & 2 & 1 & \\\\\n0 & 1 & -4 & 8 & \\\\\n0 & 0 & 0 & 5/2\n\\end{bmatrix}\n{% endmath %}\n\n变换后的矩阵所对应的方程组为\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\n2x_1 &-& 3x_2 &+& 2x_3 &= 1 \\\\\n&& x_2 &-& 4x_3 &= 8\\\\\n&& && 0 &= 5/2\n\\end{array}\n\\right.\n{% endmath %}\n\n显然，第三个方程 $0=5/2$ 是无解的。对比这个方程组和它对应的增广矩阵，我们可以发现，**当增广矩阵的行阶梯形式存在 $[0\\ \\cdots\\ 0\\ b]$ 形式的行时，方程组无解。**\n\n#### 1.3.2 有解\n当增广矩阵变换为行阶梯形式后，不存在 $[0\\ \\cdots\\ 0\\ b]$ 形式的行，则说明方程有解。我们接下来讨论下它的解具体会是怎么样的。\n\n假设现在有这样一个已经化为行最简形式的增广矩阵：\n\n{% math %}\n\\begin{bmatrix}\n1 & 0 & -5 & 1 & \\\\\n0 & 1 & 1 & 4 & \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n这个矩阵有 4 列，故而有 3 个变量。相对应的方程组为：\n\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\nx_1 && &-& 5x_3 &= 1 \\\\\n&& x_2 &+& x_3 &= 4\\\\\n&& && 0 &= 0\n\\end{array}\n\\right.\n{% endmath %}\n\n观察这个方程组，$x_1$ 和 $x_2$ 只存在于一个方程中（对应行最简形式中的主元位置），$x_3$ 存在于两个方程中。那么我们可以通过 $x_3$ 来表示 $x_1$ 和 $x_2$：\n\n{% math %}\n\\left\\{\n\\begin{aligned}\nx_1 &=\\: 5x_3 \\:+\\: 1 \\\\\nx_2 &=\\: x_3 \\:+\\: 4 \\\\\nx_3 &\\: \\text{是自由变量}\n\\end{aligned}\n\\right.\n{% endmath %}\n\n上面列出来的实际上就是这个方程组的解集了。**$x_1$ 和 $x_2$ 被称为“基本变量”；$x_3$被称为“自由变量”，因为它在解集里不受任何约束，而基本变量需要自由变量来表示； 也就是说，自由变量确定了一个值，基本变量也就随之确定了一个值。上面这个解集形式也被称为方程组的“通解”，因为它给出了方程组所有解的显示表示。**\n\n需要注意的是，我们需要先将增广矩阵变换为行最简形式，才能知道谁是自由变量，谁是基本变量。\n\n**因为自由变量能取任意值，所以，存在自由变量的线性方程组有无穷多解，而没有自由变量的线性方程组则只有一个唯一解（就像本文第一个方程组那样）。**\n\n总结一下：\n\n- 当增广矩阵的行阶梯形式（当然行最简形式也可以）**存在$[0\\ \\cdots\\ 0\\ b]$ 形式**时，方程组无解；\n- 当增广矩阵的行最简形式**不存在自由变量**时，方程组有唯一解；\n- 当增广矩阵的行最简形式**存在自由变量**时，方程组有无穷多解；\n\n## 二、向量方程\nn 维空间中的点可用 n 维向量表示。\n\n向量之间可以线性组合：\n{% math %}\n\\begin{equation}\n\\mathbf{y} = c_1 \\mathbf{v_1} + \\cdots + c_p \\mathbf{v_p}\n\\end{equation}\n{% endmath %}\n\n那么，假如有三个向量：$\\mathbf{a_1} = [1, -2, -5]^T, \\mathbf{a_2} = [2, 5, 6]^T, \\mathbf{b} = [7, 4, -3]^T$，想要知道 $\\mathbf{b}$ 是否能通过 $\\mathbf{a_1}$ 和 $\\mathbf{a_2}$ 线性表示，实际上就是求线性方程 $x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} = \\mathbf{b}$ 是否有解的问题。\n\n把这个方程展开来看，就是：\n{% math %}\n\\begin{equation}\nx_1 \\begin{bmatrix}1\\\\-2\\\\-5\\end{bmatrix} + x_2 \\begin{bmatrix}2\\\\5\\\\6\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n等同于\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}x_1\\\\-2x_1\\\\-5x_1\\end{bmatrix} +  \\begin{bmatrix}2x_2\\\\5x_2\\\\6x_2\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n和\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}x_1 + 2x_2\\\\-2x_1+5x_2\\\\-5x_1+6x_2\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n所以这个问题其实和一个线性方程组是等价的，这个线性方程组对应的増广矩阵就是（{% math %}[\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{b}]{% endmath %}）：\n\n{% math %}\n\\begin{bmatrix}\n1 & 2 & 7 \\\\\n-2 & 5 & 4 \\\\\n-5 & -6 & -3\n\\end{bmatrix}\n{% endmath %}\n\n化简为行最简形式就是：\n\n{% math %}\n\\begin{bmatrix}\n1 & 0 & 3 \\\\\n0 & 1 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n可以看出，这个线性方程组的解为 $x_1 = 3$ 和 $x_2 = 2$。继而我们就知道了 $\\bf{b}$ 和 $\\bf{a_1}$, $\\bf{a_2}$ 的关系：\n\n{% math %}\n\\begin{equation}\n3 \\begin{bmatrix}1\\\\-2\\\\-5\\end{bmatrix} + 2 \\begin{bmatrix}2\\\\5\\\\6\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n我们反过来回顾这一过程，可以发现，之前我们线性方程组的的增广矩阵表示形式，其实也可以看做是列向量组成的形式，在这个例子中，增广矩阵可以表示为 {% math %}[\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{b}]{% endmath %}。**把增广矩阵按列拆开看，我们就可以得到线性方程组的向量方程表示形式。**\n\n向量方程是线性方程组另一种重要的表现形式，它能帮助我们将矩阵、线性方程组的抽象概念同几何的直观联系起来。\n\n在几何中，n 个向量 $\\mathbf{v_1}, \\mathbf{v_2}, \\cdots, \\mathbf{v_p}$ 的所有线性组合 $c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + \\cdots + c_p \\mathbf{v_p}$ 成为一个空间，称作由 $\\mathbf{v_1}, \\mathbf{v_2}, \\cdots, \\mathbf{v_p}$ 张成的 $R^n$ 的子空间，记作 {% math %}\\operatorname{Span}\\left\\{\\mathbf{v_1}, \\cdots, \\mathbf{v_p}\\right\\}{% endmath %}。\n\n一个向量张成的空间是一根直线，两个向量张成的空间是一个平面。\n\n## 三、矩阵方程\n向量的线性组合可以看作向量与矩阵的乘积，比如一个 $m\\times n$ 的矩阵 $\\mathbf{A}$，各列为 $\\mathbf{a_1}, \\cdots, \\mathbf{a_n}$，而 $x$ 为 $n$ 维向量，则有：\n\n{% math %}\n\\begin{equation}\n\\mathbf{A}\\mathbf{x} = [\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\cdots \\mathbf{a_n}]\n\\begin{bmatrix}x_1\\\\ \\vdots\\\\ x_n\\end{bmatrix}\n= x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n}\n\\end{equation}\n{% endmath %}\n\n这种形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 的形式，就称为矩阵方程。\n\n由矩阵方程的定义，我们可以得出：**方程$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$有解当且仅当$\\mathbf{b}$为$\\mathbf{A}$中列的线性组合。**又因为我们之前提到，这些列向量的所有线性组合构成了{% math %}\\operatorname{Span}\\left\\{\\mathbf{a_1}, \\cdots, \\mathbf{a_n}\\right\\}{% endmath %}，向量 $\\mathbf{b}$  是否存在于这个空间，就等价于 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有解。\n\n下面我们来讨论下任意 $\\mathbf{b} \\in R^m$ 的情况。\n\n设\n{% math %}\n\\begin{equation}\n\\mathbf{A} = \\begin{bmatrix}1&3&4\\\\ -4&2&-6\\\\ -3&-2&-7\\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix}b1\\\\b2\\\\b3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n求方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 是否对 $b_1, b_2, b_3$ 的所有取值都有解？\n\n我们首先对增广矩阵化简：\n{% math %}\n\\begin{bmatrix}\n1 & 3 & 4 & b_1 \\\\\n-4 & 2 & -6 & b_2 \\\\\n-3 & -2 & -7 & b_3\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 3 & 4 & b_1 \\\\\n0 & 14 & 10 & b_2+4b_1 \\\\\n0 & 0 & 0 & b_1-\\frac{1}{2}b_2+b_3\n\\end{bmatrix}\n{% endmath %}\n\n可以看出，当$\\mathbf{b}$ 取某些值时，$b_1-\\frac{1}{2}b_2+b_3$ 不等于0，于是就会有无解的情况。只有当\n$$b_1-\\frac{1}{2}b_2+b_3=0$$\n时方程才有解。注意，这个式子在几何中表示三维中的一个平面， 结合$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，这个平面就是$\\mathbf{A}$ 中列向量线性组合构成的集合。\n\n本来 $\\mathbf{b}$ 是三维的向量，如果没有限制的话它可以表示整个三维空间，然而，在这个空间中，一大部分都不满足使 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有解。这仅剩的一个平面就是 $\\mathbf{A}$ 的列向量所能张成的全部空间。  这些三维列向量最终张成了一个二维平面。\n\n观察行最简形式矩阵，可以知道，之所以 $\\mathbf{b}$ 的一些取值造成矩阵方程无解，是因为系数矩阵 $\\mathbf{A}$ 中最后一行没有主元，在行最简形式中变成了形如 [0 0 0 b] 的行。**如果系数矩阵 $\\mathbf{A}$ 中每一行都有主元的话，那么就不会出现无解的情况。**\n\n反过来看，当 n 个 m 维列向量能张成 $R^m$ 时，就说明对任意 $\\mathbf{b} \\in R^m$，方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 都有解，也就是说，$R^m$ 空间中的任意向量，都可以由 $\\mathbf{A}$ 的列线性表示。\n\n总结一下，就是以下四点相互等价。\n\n1. 对任意 $\\mathbf{b}\\in R^m$，方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 都有解。\n2. 任意 $\\mathbf{b}\\in R^m$ 都是$\\mathbf{A}$ 中列的一个线性组合。\n3. $\\mathbf{A}$ 的列张成 $R^m$。\n4. $\\mathbf{A}$ 中每一行都有主元位置。\n\n## 四、三种等价形式\n矩阵方程\n{% math %} \\begin{equation}\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\end{equation} {% endmath %}\n\n和向量方程\n{% math %} \\begin{equation}\nx_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{b}\n\\end{equation} {% endmath %}\n\n以及下列增广矩阵对应的线性方程组具有相同的解集\n{% math %} \\begin{equation}\n[\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\cdots\\ \\mathbf{a_n}\\ \\mathbf{b}]\n\\end{equation} {% endmath %}\n\n\n> 矩阵方程、向量方程和线性方程组是三种不同但却相互等价的形式。在现实生活中构造一个数学模型时，我们可以在任何情况下自由选择其中任何一种最自然、最便利的陈述形式。\n\n以上三种形式就是我们在解线性方程组时的三个工具，结合具体问题，我们可以通过不同角度观察问题，进而求解。 另外，这三种形式的求解，都是对增广矩阵进行行化简，因此，増广矩阵的行变换是一切的基础。\n\n# 参考资料：\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/linear-algebra-1.md","raw":"---\ntitle: 线性代数拾遗（一 ）：线性方程组、向量方程和矩阵方程\ntags:\n  - 数学\n  - 线性代数\n  - 线性代数拾遗\n  - 基础概念\ncategory: 线性代数\nmathjax: true\ndate: 2016-05-03 22:55:27\n---\n\n{% asset_img banner.jpeg %}\n\n# 前言\n线性代数在各大理工科，乃至经济金融领域的使用之广泛，毋庸置疑。 一直以来，我虽也知道线性代数的重要，但从内心上其实一直是犯怵的（尤其是学习论文、算法中，基本只要看到对方把算法向量化之后就蒙圈了），当年在学校学习过程中很多也是靠着死记硬背过来的，对它的直观意义一直都没能有很好的理解。\n\n最近，这么一本书进入了我的视线：《线性代数及其应用》，听书名感觉平平，但只翻了几页就感觉十分过瘾，仿佛打通了任督二脉。以往很多死记硬背的知识点在这本书的解释下，变成了可以直观推导出来的结果。这本书不仅对线性代数的基本概念阐述地很直观形象，而且还有许多现实生活中的应用，特别是经济、物理、计算机领域，真正让人领略到线性代数作为现代数学的魅力。\n\n我特将自己的读书总结和体会记录于此，也是希望借此加深自己的理解。\n\n注意，这个系列假设你已经有了线性代数基础，像是行变换、将矩阵转换为行阶梯形式这种基本技巧已经掌握。本文不再赘述具体操作步骤，主要关注于概念的直观理解。\n\n<!-- more -->\n# 线性方程组、向量方程和矩阵方程\n## 一、线性方程组\n线性代数，最基本的问题，就是解线性方程组了。线性方程组就是一组形如 $a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b$ 的方程。一个线性方程组中的变量是相同的，如果第一个方程是关于 $x_1 \\cdots x_n$ 的，那么其他的也都应该如此（这些变量不一定都出现，因为系数可以有 0）。\n### 1.1 线性方程组的矩阵形式\n方程组\n\n{% math %}\n\\left\\{\n\\begin{equation}\\label{initial}\n\\begin{array}{ccl}\nx_1 &-& 2x_2 &+& x_3 &= 0 \\\\\n    && 2x_2 &-& 8x_3 &= 8 \\\\\n{-4x_1} &+& 5x_2 &+& 9x_3 &= {-9}\n\\end{array}\n\\end{equation}\n\\right.\n{% endmath %}\n\n可以通过増广矩阵形式描述：\n{% math %}\n\\begin{bmatrix}\n1 & -2 & 1 & 0\\\\\n0 & 2 & -8 & 8\\\\\n-4 & 5 & 9 & -9\\\\\n\\end{bmatrix}\n{% endmath %}\n增广矩阵去掉最后一列，就是该方程组的系数矩阵。\n\n矩阵形式只是线性方程组的一种表示形式。今后的很多关于线性方程组的计算，都将在矩阵形式上进行操作，然而你也需要知道，在这些操作进行的同时，线性方程组也在进行类似的变换。比如，将增广矩阵的第一、二行对换，那么同时，它所代表的线性方程组中，第一、二个方程也进行了对调。\n\n### 1.2 线性方程组的解\n解一个线性方程组，就是通过对其矩阵形式行变换（三种方式：交换方程的先后顺序，一个方程左右同乘以某数，和两个方程相加） 转换为行阶梯形式。比如\n\n{% math %}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\\\\n0 & 2 & -8 & 8 & \\\\\n-4 & 5 & 9 & -9 &\n\\end{bmatrix}\n\\text{转化为行阶梯形式：}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\\\\n0 & 1 & -4 & 4 & \\\\\n0 & 0 & 1 & 3\n\\end{bmatrix}\n\\text{和最简形式：}\n\\begin{bmatrix}\n1 & 0 & 0 & 29 & \\\\\n0 & 1 & 0 & 16 & \\\\\n0 & 0 & 1 & 3\n\\end{bmatrix}\n{% endmath %}\n\n上面最简形式的矩阵对应的线性方程组是\n\n{% math %}\n\\left\\{\n\\begin{array}{ccl}\nx_1 && && &= 29 \\\\\n    && x_2 && &= 16 \\\\\n&& && x_3 &= 3\n\\end{array}\n\\right.\n{% endmath %}\n\n这个线性方程组和一开始的方程组是等价的，只是处于不同的状态，它们的解也是相同的，而显然行最简形式的方程组最容易解，所以我们一般都**将线性方程组的増广矩阵转化为行最简形式继而求解**。\n\n### 1.3 解的存在性和唯一性\n还记得线性代数时经常讨论的“无解““唯一解”“无穷多解”吧？\n\n首先来看刚才的方程组，经过行变换后，方程组的解已经很显然了：$x_1 = 29, x_2 =16, x_3 = 3$。这个方程组的解就只有一个，是唯一解。\n\n#### 1.3.1 无解\n我们再来看一个方程组：\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\n&& x_2 &-& 4x_3 &= 8 \\\\\n2x_1 &-& 3x_2 &+& 2x_3 &= 1\\\\\n5x_1 &-& 8x_2 &+& 7x_3 &= 1\n\\end{array}\n\\right.\n{% endmath %}\n\n它的增广矩阵\n{% math %}\n\\begin{bmatrix}\n0 & 1 & -4 & 8 & \\\\\n2 & -3 & 2 & 1 & \\\\\n5 & -8 & 7 & 1\n\\end{bmatrix}\n\\text{ 行变换得到：}\n\\begin{bmatrix}\n2 & -3 & 2 & 1 & \\\\\n0 & 1 & -4 & 8 & \\\\\n0 & 0 & 0 & 5/2\n\\end{bmatrix}\n{% endmath %}\n\n变换后的矩阵所对应的方程组为\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\n2x_1 &-& 3x_2 &+& 2x_3 &= 1 \\\\\n&& x_2 &-& 4x_3 &= 8\\\\\n&& && 0 &= 5/2\n\\end{array}\n\\right.\n{% endmath %}\n\n显然，第三个方程 $0=5/2$ 是无解的。对比这个方程组和它对应的增广矩阵，我们可以发现，**当增广矩阵的行阶梯形式存在 $[0\\ \\cdots\\ 0\\ b]$ 形式的行时，方程组无解。**\n\n#### 1.3.2 有解\n当增广矩阵变换为行阶梯形式后，不存在 $[0\\ \\cdots\\ 0\\ b]$ 形式的行，则说明方程有解。我们接下来讨论下它的解具体会是怎么样的。\n\n假设现在有这样一个已经化为行最简形式的增广矩阵：\n\n{% math %}\n\\begin{bmatrix}\n1 & 0 & -5 & 1 & \\\\\n0 & 1 & 1 & 4 & \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n这个矩阵有 4 列，故而有 3 个变量。相对应的方程组为：\n\n{% math %}\n\\left\\{\n\\begin{array}{cccl}\nx_1 && &-& 5x_3 &= 1 \\\\\n&& x_2 &+& x_3 &= 4\\\\\n&& && 0 &= 0\n\\end{array}\n\\right.\n{% endmath %}\n\n观察这个方程组，$x_1$ 和 $x_2$ 只存在于一个方程中（对应行最简形式中的主元位置），$x_3$ 存在于两个方程中。那么我们可以通过 $x_3$ 来表示 $x_1$ 和 $x_2$：\n\n{% math %}\n\\left\\{\n\\begin{aligned}\nx_1 &=\\: 5x_3 \\:+\\: 1 \\\\\nx_2 &=\\: x_3 \\:+\\: 4 \\\\\nx_3 &\\: \\text{是自由变量}\n\\end{aligned}\n\\right.\n{% endmath %}\n\n上面列出来的实际上就是这个方程组的解集了。**$x_1$ 和 $x_2$ 被称为“基本变量”；$x_3$被称为“自由变量”，因为它在解集里不受任何约束，而基本变量需要自由变量来表示； 也就是说，自由变量确定了一个值，基本变量也就随之确定了一个值。上面这个解集形式也被称为方程组的“通解”，因为它给出了方程组所有解的显示表示。**\n\n需要注意的是，我们需要先将增广矩阵变换为行最简形式，才能知道谁是自由变量，谁是基本变量。\n\n**因为自由变量能取任意值，所以，存在自由变量的线性方程组有无穷多解，而没有自由变量的线性方程组则只有一个唯一解（就像本文第一个方程组那样）。**\n\n总结一下：\n\n- 当增广矩阵的行阶梯形式（当然行最简形式也可以）**存在$[0\\ \\cdots\\ 0\\ b]$ 形式**时，方程组无解；\n- 当增广矩阵的行最简形式**不存在自由变量**时，方程组有唯一解；\n- 当增广矩阵的行最简形式**存在自由变量**时，方程组有无穷多解；\n\n## 二、向量方程\nn 维空间中的点可用 n 维向量表示。\n\n向量之间可以线性组合：\n{% math %}\n\\begin{equation}\n\\mathbf{y} = c_1 \\mathbf{v_1} + \\cdots + c_p \\mathbf{v_p}\n\\end{equation}\n{% endmath %}\n\n那么，假如有三个向量：$\\mathbf{a_1} = [1, -2, -5]^T, \\mathbf{a_2} = [2, 5, 6]^T, \\mathbf{b} = [7, 4, -3]^T$，想要知道 $\\mathbf{b}$ 是否能通过 $\\mathbf{a_1}$ 和 $\\mathbf{a_2}$ 线性表示，实际上就是求线性方程 $x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} = \\mathbf{b}$ 是否有解的问题。\n\n把这个方程展开来看，就是：\n{% math %}\n\\begin{equation}\nx_1 \\begin{bmatrix}1\\\\-2\\\\-5\\end{bmatrix} + x_2 \\begin{bmatrix}2\\\\5\\\\6\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n等同于\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}x_1\\\\-2x_1\\\\-5x_1\\end{bmatrix} +  \\begin{bmatrix}2x_2\\\\5x_2\\\\6x_2\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n和\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}x_1 + 2x_2\\\\-2x_1+5x_2\\\\-5x_1+6x_2\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n所以这个问题其实和一个线性方程组是等价的，这个线性方程组对应的増广矩阵就是（{% math %}[\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{b}]{% endmath %}）：\n\n{% math %}\n\\begin{bmatrix}\n1 & 2 & 7 \\\\\n-2 & 5 & 4 \\\\\n-5 & -6 & -3\n\\end{bmatrix}\n{% endmath %}\n\n化简为行最简形式就是：\n\n{% math %}\n\\begin{bmatrix}\n1 & 0 & 3 \\\\\n0 & 1 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n可以看出，这个线性方程组的解为 $x_1 = 3$ 和 $x_2 = 2$。继而我们就知道了 $\\bf{b}$ 和 $\\bf{a_1}$, $\\bf{a_2}$ 的关系：\n\n{% math %}\n\\begin{equation}\n3 \\begin{bmatrix}1\\\\-2\\\\-5\\end{bmatrix} + 2 \\begin{bmatrix}2\\\\5\\\\6\\end{bmatrix}\n= \\begin{bmatrix}7\\\\4\\\\-3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n我们反过来回顾这一过程，可以发现，之前我们线性方程组的的增广矩阵表示形式，其实也可以看做是列向量组成的形式，在这个例子中，增广矩阵可以表示为 {% math %}[\\mathbf{a_1}, \\mathbf{a_2}, \\mathbf{b}]{% endmath %}。**把增广矩阵按列拆开看，我们就可以得到线性方程组的向量方程表示形式。**\n\n向量方程是线性方程组另一种重要的表现形式，它能帮助我们将矩阵、线性方程组的抽象概念同几何的直观联系起来。\n\n在几何中，n 个向量 $\\mathbf{v_1}, \\mathbf{v_2}, \\cdots, \\mathbf{v_p}$ 的所有线性组合 $c_1 \\mathbf{v_1} + c_2 \\mathbf{v_2} + \\cdots + c_p \\mathbf{v_p}$ 成为一个空间，称作由 $\\mathbf{v_1}, \\mathbf{v_2}, \\cdots, \\mathbf{v_p}$ 张成的 $R^n$ 的子空间，记作 {% math %}\\operatorname{Span}\\left\\{\\mathbf{v_1}, \\cdots, \\mathbf{v_p}\\right\\}{% endmath %}。\n\n一个向量张成的空间是一根直线，两个向量张成的空间是一个平面。\n\n## 三、矩阵方程\n向量的线性组合可以看作向量与矩阵的乘积，比如一个 $m\\times n$ 的矩阵 $\\mathbf{A}$，各列为 $\\mathbf{a_1}, \\cdots, \\mathbf{a_n}$，而 $x$ 为 $n$ 维向量，则有：\n\n{% math %}\n\\begin{equation}\n\\mathbf{A}\\mathbf{x} = [\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\cdots \\mathbf{a_n}]\n\\begin{bmatrix}x_1\\\\ \\vdots\\\\ x_n\\end{bmatrix}\n= x_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n}\n\\end{equation}\n{% endmath %}\n\n这种形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 的形式，就称为矩阵方程。\n\n由矩阵方程的定义，我们可以得出：**方程$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$有解当且仅当$\\mathbf{b}$为$\\mathbf{A}$中列的线性组合。**又因为我们之前提到，这些列向量的所有线性组合构成了{% math %}\\operatorname{Span}\\left\\{\\mathbf{a_1}, \\cdots, \\mathbf{a_n}\\right\\}{% endmath %}，向量 $\\mathbf{b}$  是否存在于这个空间，就等价于 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有解。\n\n下面我们来讨论下任意 $\\mathbf{b} \\in R^m$ 的情况。\n\n设\n{% math %}\n\\begin{equation}\n\\mathbf{A} = \\begin{bmatrix}1&3&4\\\\ -4&2&-6\\\\ -3&-2&-7\\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix}b1\\\\b2\\\\b3\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n求方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 是否对 $b_1, b_2, b_3$ 的所有取值都有解？\n\n我们首先对增广矩阵化简：\n{% math %}\n\\begin{bmatrix}\n1 & 3 & 4 & b_1 \\\\\n-4 & 2 & -6 & b_2 \\\\\n-3 & -2 & -7 & b_3\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 3 & 4 & b_1 \\\\\n0 & 14 & 10 & b_2+4b_1 \\\\\n0 & 0 & 0 & b_1-\\frac{1}{2}b_2+b_3\n\\end{bmatrix}\n{% endmath %}\n\n可以看出，当$\\mathbf{b}$ 取某些值时，$b_1-\\frac{1}{2}b_2+b_3$ 不等于0，于是就会有无解的情况。只有当\n$$b_1-\\frac{1}{2}b_2+b_3=0$$\n时方程才有解。注意，这个式子在几何中表示三维中的一个平面， 结合$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，这个平面就是$\\mathbf{A}$ 中列向量线性组合构成的集合。\n\n本来 $\\mathbf{b}$ 是三维的向量，如果没有限制的话它可以表示整个三维空间，然而，在这个空间中，一大部分都不满足使 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有解。这仅剩的一个平面就是 $\\mathbf{A}$ 的列向量所能张成的全部空间。  这些三维列向量最终张成了一个二维平面。\n\n观察行最简形式矩阵，可以知道，之所以 $\\mathbf{b}$ 的一些取值造成矩阵方程无解，是因为系数矩阵 $\\mathbf{A}$ 中最后一行没有主元，在行最简形式中变成了形如 [0 0 0 b] 的行。**如果系数矩阵 $\\mathbf{A}$ 中每一行都有主元的话，那么就不会出现无解的情况。**\n\n反过来看，当 n 个 m 维列向量能张成 $R^m$ 时，就说明对任意 $\\mathbf{b} \\in R^m$，方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 都有解，也就是说，$R^m$ 空间中的任意向量，都可以由 $\\mathbf{A}$ 的列线性表示。\n\n总结一下，就是以下四点相互等价。\n\n1. 对任意 $\\mathbf{b}\\in R^m$，方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 都有解。\n2. 任意 $\\mathbf{b}\\in R^m$ 都是$\\mathbf{A}$ 中列的一个线性组合。\n3. $\\mathbf{A}$ 的列张成 $R^m$。\n4. $\\mathbf{A}$ 中每一行都有主元位置。\n\n## 四、三种等价形式\n矩阵方程\n{% math %} \\begin{equation}\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\end{equation} {% endmath %}\n\n和向量方程\n{% math %} \\begin{equation}\nx_1 \\mathbf{a_1} + x_2 \\mathbf{a_2} + \\cdots + x_n \\mathbf{a_n} = \\mathbf{b}\n\\end{equation} {% endmath %}\n\n以及下列增广矩阵对应的线性方程组具有相同的解集\n{% math %} \\begin{equation}\n[\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\cdots\\ \\mathbf{a_n}\\ \\mathbf{b}]\n\\end{equation} {% endmath %}\n\n\n> 矩阵方程、向量方程和线性方程组是三种不同但却相互等价的形式。在现实生活中构造一个数学模型时，我们可以在任何情况下自由选择其中任何一种最自然、最便利的陈述形式。\n\n以上三种形式就是我们在解线性方程组时的三个工具，结合具体问题，我们可以通过不同角度观察问题，进而求解。 另外，这三种形式的求解，都是对增广矩阵进行行化简，因此，増广矩阵的行变换是一切的基础。\n\n# 参考资料：\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"linear-algebra-1","published":1,"updated":"2016-05-21T03:36:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji89000i6jod3x7nv2e6"},{"title":"线性代数拾遗（二）：线性方程组的解集及其几何意义","mathjax":true,"date":"2016-05-13T16:00:00.000Z","_content":"\n{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-1 上一章 %}我们讲到三种等价形式：线性方程组、向量方程和矩阵方程。由于这三者之间的等价关系，我们解决现实问题时可以自由选取其中任意一个作为模型。我个人认为，线性方程组是最“质朴”的形式；向量方程则是与几何建立了关系，这将方便我们进行更直观的推理；矩阵方程则是向量方程的一种“封装”，是向量方程的一种抽象，它将具体的向量形式隐藏，提供给我们一个简洁的 API 形式——矩阵。未来将要介绍的很多概念就是基于对这一层封装的研究，如果到时候我们发现某个概念理解有困难，不妨转换思路到向量方程或线性方程组的形式进行分析。\n\n此外，我们之前还进行了关于线性方程组解集的讨论，在这章我们对其进一步探讨。\n\n<!-- more -->\n\n# 一、齐次线性方程组\n形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的线性方程组称为`齐次方程组`。显然，$\\mathbf{x}=\\mathbf{0}$ 是方程的解，这个解太平凡了，以致于就叫`平凡解`。我们平常更关心的是它还有没有别的解，即`非平凡解`。下面以一个例子分析一下：\n\n例：判断下列齐次方程组是否有非平凡解，表示其解集。\n{% math %}\n\\begin{array}\n3x_1 &+& 5x_2 &-& 4x_3 &= 0 \\\\\n-3x_1 &-& 2x_2 &+& 4x_3 &= 0 \\\\\n6x_1 &+& x_2 &-& 8x_3 &= 0\n\\end{array}\n{% endmath %}\n\n对于这类求解集的问题，我们可以直接对增广矩阵化简，得到\n{% math %}\n\\begin{equation*}\n[\\mathbf{A}\\ \\mathbf{0}] \\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 0 \\\\\n-3 & -2 & 4 & 0 \\\\\n6 & 1 & -8 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -\\frac{4}{3} & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{equation*}\n{% endmath %}\n\n从最后的行最简形式，我们可以得到解：{% math %}x_1 = \\frac{4}{3} x_3, x_2 =0{% endmath %}，其中 $x_3$ 是自由变量。所以 $\\mathbf{x}$ 的通解就是 {% math %}\\mathbf{x} = \\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = x_3\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix} = x_3\\mathbf{v}{% endmath %}。也就是说，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的解是三维空间（因为向量 $\\mathbf{v}$ 是三维的）中的一条直线（因为只有一个自由变量）。进一步推广，我们不难想象，如果解集中有 $p$ 个自由变量，则解集就是 $m$ 维空间（$m$ 为 $\\mathbf{A}$ 的行数）中，$p$ 个向量张成的空间。**如果没有自由变量（也就是 $\\mathbf{A}$ 各列线性无关），那么就有 0 个向量张成的空间，即 $\\operatorname{Span}\\{\\mathbf{0}\\}$，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 也就只有平凡解。**\n\n# 二、非齐次线性方程组\n`非齐次线性方程组`形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，\n为了方便对比，我们把上面那个例子改为一个非齐次方程组进行分析：\n{% math %}\n\\begin{array}\n3x_1 &+& 5x_2 &-& 4x_3 &=& 7 \\\\\n-3x_1 &-& 2x_2 &+& 4x_3 &=& -1 \\\\ \n6x_1 &+& x_2 &-& 8x_3 &=& -4 \n\\end{array}\n{% endmath %}\n\n老套路，我们对这个方程组的增广矩阵行化简：\n{% math %}\n\\begin{bmatrix}\n3 & 5 & -4 & 7 \\\\\n-3 & -2 & 4 & -1 \\\\\n6 & 1 & -8 & -4\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 7 \\\\\n0 & 1 & 0 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -\\frac{4}{3} & -1 \\\\0 & 1 & 0 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n化简后可以得到方程组的解为：{% math %}x_1 = -1 + \\frac{4}{3}x_3，x_2 = 2{% endmath %}，其中 $x_3$ 是自由变量。\n我们把这个解集用向量的形式表示出来就是：\n{% math %}\n\\begin{equation*}\n\\mathbf{x} = \\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n= \\begin{bmatrix}-1+\\frac{4}{3}x_3\\\\ 2\\\\ x_3\\end{bmatrix}\n= \\begin{bmatrix}-1\\\\ 2\\\\ 0\\end{bmatrix} + x_3 \\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}\n\\end{equation*}\n{% endmath %}\n注意到这个向量可分解为一个常数向量{% math %}\\begin{bmatrix}-1\\\\ 2\\\\ 0\\end{bmatrix}{% endmath %}和一个可任意伸缩的向量{% math %}x_3\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}{% endmath %}，而且，常数向量就是行化简后矩阵的最后一列，而 {% math %}\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}{% endmath %} 同样是齐次方程组的解。这是因为非齐次方程组只是最后一列由$\\mathbf{0}$换成了$\\mathbf{b}$，而且最后一列不会影响前面三列，所以齐次和非齐次方程组行化简后，变量的对应系数是相同的（系数矩阵就是前三列），**非齐次方程组的解仅仅只比齐次方程组的解多了一个常数向量**。例如齐次方程组的解集为$\\mathbf{x}=t\\mathbf{v}$，则非齐次方程组的解集就是 $\\mathbf{x}=\\mathbf{p}+t\\mathbf{v}$，其中 $t$ 为任意实数。从几何的角度来看，就是**齐次方程组的解集经向量 $\\mathbf{p}$ 平移得到非齐次方程组的解集**。这个 $\\mathbf{p}$ 的学名就叫做`特解`。\n\n注意，这里讲齐次方程组和非齐次方程组的解有一个前提，就是非齐次方程组首先要是有解的，如果$\\mathbf{0}$变成$\\mathbf{b}$ 导致方程组没有解，那么也就不能用齐次方程组的解集平移了。\n\n结合之前总结的齐次线性方程组解的性质，当方程组含有 $p$ 个自由变量时，齐次方程组的解集是 $p$ 个向量的张成空间，而非齐次方程组解集只是这个空间进行了平移（前提是非齐次方程组有解），并没有改变这个空间的基本性质（比如空间的维度）。\n\n# 三、列空间\n矩阵{% math %} \\mathbf{A} = [\\mathbf{a_1} \\mathbf{a_2} \\cdots \\mathbf{a_n}] {% endmath %}的各个列向量线性组合组成的集合，就是$\\mathbf{A}$的列空间。记作 $\\operatorname{Col}\\mathbf{A}$，即\n{% math %}\n\\begin{equation*}\n\\operatorname{Col} \\mathbf{A} = \\operatorname{Span}\\{\\mathbf{a_1}, \\mathbf{a_2}, \\cdots, \\mathbf{a_n}\\}\n\\end{equation*}\n{% endmath %}\n\n这个列空间，我们应该不陌生了，上一章中很多时候都是把矩阵看成列向量的排列，考虑 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 的解的情况时其实就是在列向量中进行分析的。列空间在分析矩阵中各列向量的线性相关性时很有帮助：只有各列线性无关时，这 $n$ 个列才能张成 $n$ 维空间，这时就说这个矩阵的秩为 $n$；而假如这里面有 1 列和其他某列线性相关，那么这 $n$ 个列就只能张成 $n-1$ 维空间，这个矩阵的秩就是 $n-1$；也就是说，**矩阵的秩说明了这个矩阵的列向量最多能张成多少维**。\n\n如下图中，$\\mathbf{A} = [\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\mathbf{a_3}]$，由于有两个向量线性相关，导致 3 个列向量只能张成 2 维，因此 $\\mathbf{A}$ 的秩为 2。所以 $\\mathbf{A}\\mathbf{x}$ 得不到任意三维向量 $\\mathbf{b}$，也就是 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 并不对所有 $\\mathbf{b}$ 成立（只有$\\mathbf{b}$ 是 $\\mathbf{A}$ 列空间中的向量时才成立）。\n\n{% asset_img r_less_than_n.png 秩小于n的情况 %}\n\n更进一步，非齐次线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 中，如果 $\\mathbf{A}$已知，$\\mathbf{x}$和$\\mathbf{b}$ 未知，此时我们关注的问题是 $\\mathbf{A}$ 的列向量能张成多少维；如果 $\\mathbf{A}$ 和 $\\mathbf{b}$ 已知，我们关注的问题就是 $\\mathbf{A}$ 中 $n$ 个列向量如何线性表示能表示成 $\\mathbf{b}$，这时候我们如果提前知道 $\\mathbf{A}$ 的列空间达不到 $\\mathbf{b}$ 的维数，那么这些列向量就一定无法线性组合出 $\\mathbf{b}$。\n\n# 四、零空间\n齐次方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的全部解组成的集合，称为矩阵 $\\mathbf{A}$ 的零空间，记作 $\\operatorname{Nul} \\mathbf{A}$。\n\n当 $\\mathbf{A}$ 中的列向量线性无关时，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 只有零解，这时 $\\mathbf{A}$ 的零空间就是 $\\mathbf{0}$；而只要 $\\mathbf{A}$ 中的列向量线性相关，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 就存在非零解，这时 $\\mathbf{A}$ 的零空间就是一个维度大于 0 的空间。\n\n关于列空间和零空间的讨论先在这里打住，之后会进一步讨论它们之间的关系和各自的意义。目前只要知道列空间是由 $\\mathbf{A}$ 的列向量张成的，而零空间的意义更隐晦一些，是 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的所有解组成的空间。从列空间能看出 $\\mathbf{A}$ 各列的线性相关关系，列向量越相关，列空间维度越低。从零空间也能看出 $\\mathbf{A}$ 各列的线性相关性，列向量越相关，零空间维度越高。而负责量化描述 $\\mathbf{A}$ 列向量有多么线性相关的，是一个叫做`秩`的东西。\n\n# 参考资料：\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n- 麻省理工学院的[线性代数公开课](http://open.163.com/special/opencourse/daishu.html)\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/linear-algebra-2.md","raw":"---\ntitle: 线性代数拾遗（二）：线性方程组的解集及其几何意义\ntags:\n - 数学\n - 线性代数\n - 线性代数拾遗\n - 基础概念\ncategory: 线性代数\nmathjax: true\ndate: 2016-05-14\n---\n\n{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-1 上一章 %}我们讲到三种等价形式：线性方程组、向量方程和矩阵方程。由于这三者之间的等价关系，我们解决现实问题时可以自由选取其中任意一个作为模型。我个人认为，线性方程组是最“质朴”的形式；向量方程则是与几何建立了关系，这将方便我们进行更直观的推理；矩阵方程则是向量方程的一种“封装”，是向量方程的一种抽象，它将具体的向量形式隐藏，提供给我们一个简洁的 API 形式——矩阵。未来将要介绍的很多概念就是基于对这一层封装的研究，如果到时候我们发现某个概念理解有困难，不妨转换思路到向量方程或线性方程组的形式进行分析。\n\n此外，我们之前还进行了关于线性方程组解集的讨论，在这章我们对其进一步探讨。\n\n<!-- more -->\n\n# 一、齐次线性方程组\n形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的线性方程组称为`齐次方程组`。显然，$\\mathbf{x}=\\mathbf{0}$ 是方程的解，这个解太平凡了，以致于就叫`平凡解`。我们平常更关心的是它还有没有别的解，即`非平凡解`。下面以一个例子分析一下：\n\n例：判断下列齐次方程组是否有非平凡解，表示其解集。\n{% math %}\n\\begin{array}\n3x_1 &+& 5x_2 &-& 4x_3 &= 0 \\\\\n-3x_1 &-& 2x_2 &+& 4x_3 &= 0 \\\\\n6x_1 &+& x_2 &-& 8x_3 &= 0\n\\end{array}\n{% endmath %}\n\n对于这类求解集的问题，我们可以直接对增广矩阵化简，得到\n{% math %}\n\\begin{equation*}\n[\\mathbf{A}\\ \\mathbf{0}] \\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 0 \\\\\n-3 & -2 & 4 & 0 \\\\\n6 & 1 & -8 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -\\frac{4}{3} & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{equation*}\n{% endmath %}\n\n从最后的行最简形式，我们可以得到解：{% math %}x_1 = \\frac{4}{3} x_3, x_2 =0{% endmath %}，其中 $x_3$ 是自由变量。所以 $\\mathbf{x}$ 的通解就是 {% math %}\\mathbf{x} = \\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = x_3\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix} = x_3\\mathbf{v}{% endmath %}。也就是说，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的解是三维空间（因为向量 $\\mathbf{v}$ 是三维的）中的一条直线（因为只有一个自由变量）。进一步推广，我们不难想象，如果解集中有 $p$ 个自由变量，则解集就是 $m$ 维空间（$m$ 为 $\\mathbf{A}$ 的行数）中，$p$ 个向量张成的空间。**如果没有自由变量（也就是 $\\mathbf{A}$ 各列线性无关），那么就有 0 个向量张成的空间，即 $\\operatorname{Span}\\{\\mathbf{0}\\}$，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 也就只有平凡解。**\n\n# 二、非齐次线性方程组\n`非齐次线性方程组`形如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$，\n为了方便对比，我们把上面那个例子改为一个非齐次方程组进行分析：\n{% math %}\n\\begin{array}\n3x_1 &+& 5x_2 &-& 4x_3 &=& 7 \\\\\n-3x_1 &-& 2x_2 &+& 4x_3 &=& -1 \\\\ \n6x_1 &+& x_2 &-& 8x_3 &=& -4 \n\\end{array}\n{% endmath %}\n\n老套路，我们对这个方程组的增广矩阵行化简：\n{% math %}\n\\begin{bmatrix}\n3 & 5 & -4 & 7 \\\\\n-3 & -2 & 4 & -1 \\\\\n6 & 1 & -8 & -4\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n3 & 5 & -4 & 7 \\\\\n0 & 1 & 0 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -\\frac{4}{3} & -1 \\\\0 & 1 & 0 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n{% endmath %}\n\n化简后可以得到方程组的解为：{% math %}x_1 = -1 + \\frac{4}{3}x_3，x_2 = 2{% endmath %}，其中 $x_3$ 是自由变量。\n我们把这个解集用向量的形式表示出来就是：\n{% math %}\n\\begin{equation*}\n\\mathbf{x} = \\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n= \\begin{bmatrix}-1+\\frac{4}{3}x_3\\\\ 2\\\\ x_3\\end{bmatrix}\n= \\begin{bmatrix}-1\\\\ 2\\\\ 0\\end{bmatrix} + x_3 \\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}\n\\end{equation*}\n{% endmath %}\n注意到这个向量可分解为一个常数向量{% math %}\\begin{bmatrix}-1\\\\ 2\\\\ 0\\end{bmatrix}{% endmath %}和一个可任意伸缩的向量{% math %}x_3\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}{% endmath %}，而且，常数向量就是行化简后矩阵的最后一列，而 {% math %}\\begin{bmatrix}\\frac{4}{3}\\\\ 0\\\\ 1\\end{bmatrix}{% endmath %} 同样是齐次方程组的解。这是因为非齐次方程组只是最后一列由$\\mathbf{0}$换成了$\\mathbf{b}$，而且最后一列不会影响前面三列，所以齐次和非齐次方程组行化简后，变量的对应系数是相同的（系数矩阵就是前三列），**非齐次方程组的解仅仅只比齐次方程组的解多了一个常数向量**。例如齐次方程组的解集为$\\mathbf{x}=t\\mathbf{v}$，则非齐次方程组的解集就是 $\\mathbf{x}=\\mathbf{p}+t\\mathbf{v}$，其中 $t$ 为任意实数。从几何的角度来看，就是**齐次方程组的解集经向量 $\\mathbf{p}$ 平移得到非齐次方程组的解集**。这个 $\\mathbf{p}$ 的学名就叫做`特解`。\n\n注意，这里讲齐次方程组和非齐次方程组的解有一个前提，就是非齐次方程组首先要是有解的，如果$\\mathbf{0}$变成$\\mathbf{b}$ 导致方程组没有解，那么也就不能用齐次方程组的解集平移了。\n\n结合之前总结的齐次线性方程组解的性质，当方程组含有 $p$ 个自由变量时，齐次方程组的解集是 $p$ 个向量的张成空间，而非齐次方程组解集只是这个空间进行了平移（前提是非齐次方程组有解），并没有改变这个空间的基本性质（比如空间的维度）。\n\n# 三、列空间\n矩阵{% math %} \\mathbf{A} = [\\mathbf{a_1} \\mathbf{a_2} \\cdots \\mathbf{a_n}] {% endmath %}的各个列向量线性组合组成的集合，就是$\\mathbf{A}$的列空间。记作 $\\operatorname{Col}\\mathbf{A}$，即\n{% math %}\n\\begin{equation*}\n\\operatorname{Col} \\mathbf{A} = \\operatorname{Span}\\{\\mathbf{a_1}, \\mathbf{a_2}, \\cdots, \\mathbf{a_n}\\}\n\\end{equation*}\n{% endmath %}\n\n这个列空间，我们应该不陌生了，上一章中很多时候都是把矩阵看成列向量的排列，考虑 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 的解的情况时其实就是在列向量中进行分析的。列空间在分析矩阵中各列向量的线性相关性时很有帮助：只有各列线性无关时，这 $n$ 个列才能张成 $n$ 维空间，这时就说这个矩阵的秩为 $n$；而假如这里面有 1 列和其他某列线性相关，那么这 $n$ 个列就只能张成 $n-1$ 维空间，这个矩阵的秩就是 $n-1$；也就是说，**矩阵的秩说明了这个矩阵的列向量最多能张成多少维**。\n\n如下图中，$\\mathbf{A} = [\\mathbf{a_1}\\ \\mathbf{a_2}\\ \\mathbf{a_3}]$，由于有两个向量线性相关，导致 3 个列向量只能张成 2 维，因此 $\\mathbf{A}$ 的秩为 2。所以 $\\mathbf{A}\\mathbf{x}$ 得不到任意三维向量 $\\mathbf{b}$，也就是 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 并不对所有 $\\mathbf{b}$ 成立（只有$\\mathbf{b}$ 是 $\\mathbf{A}$ 列空间中的向量时才成立）。\n\n{% asset_img r_less_than_n.png 秩小于n的情况 %}\n\n更进一步，非齐次线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 中，如果 $\\mathbf{A}$已知，$\\mathbf{x}$和$\\mathbf{b}$ 未知，此时我们关注的问题是 $\\mathbf{A}$ 的列向量能张成多少维；如果 $\\mathbf{A}$ 和 $\\mathbf{b}$ 已知，我们关注的问题就是 $\\mathbf{A}$ 中 $n$ 个列向量如何线性表示能表示成 $\\mathbf{b}$，这时候我们如果提前知道 $\\mathbf{A}$ 的列空间达不到 $\\mathbf{b}$ 的维数，那么这些列向量就一定无法线性组合出 $\\mathbf{b}$。\n\n# 四、零空间\n齐次方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的全部解组成的集合，称为矩阵 $\\mathbf{A}$ 的零空间，记作 $\\operatorname{Nul} \\mathbf{A}$。\n\n当 $\\mathbf{A}$ 中的列向量线性无关时，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 只有零解，这时 $\\mathbf{A}$ 的零空间就是 $\\mathbf{0}$；而只要 $\\mathbf{A}$ 中的列向量线性相关，$\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 就存在非零解，这时 $\\mathbf{A}$ 的零空间就是一个维度大于 0 的空间。\n\n关于列空间和零空间的讨论先在这里打住，之后会进一步讨论它们之间的关系和各自的意义。目前只要知道列空间是由 $\\mathbf{A}$ 的列向量张成的，而零空间的意义更隐晦一些，是 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 的所有解组成的空间。从列空间能看出 $\\mathbf{A}$ 各列的线性相关关系，列向量越相关，列空间维度越低。从零空间也能看出 $\\mathbf{A}$ 各列的线性相关性，列向量越相关，零空间维度越高。而负责量化描述 $\\mathbf{A}$ 列向量有多么线性相关的，是一个叫做`秩`的东西。\n\n# 参考资料：\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n- 麻省理工学院的[线性代数公开课](http://open.163.com/special/opencourse/daishu.html)\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"linear-algebra-2","published":1,"updated":"2016-05-21T14:19:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8b000j6jodmuceunsg"},{"title":"线性代数拾遗（三）：线性变换以及矩阵的意义","mathjax":true,"date":"2016-05-20T13:11:00.000Z","_content":"\n{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-2 上一章 %}我们讨论了齐次和非齐次两种线性方程组的解集，以及它们的几何意义。由齐次线性方程组，我们引入了零空间的概念；而由非齐次线性方程组，我们引入了列空间的概念。这两个空间目前是我们理解线性方程组的桥梁，未来还会对这些空间进行更进一步的讨论。在这之前，让我们先来研究一下矩阵的意义。 \n\n之前的两章中，矩阵是在矩阵方程中出现的，当时我们理解它的意义为“对向量的一种封装”，也就是一种“数据”的形式理解矩阵的。这一章，我们引入矩阵的另一层意义：`线性变换`。\n\n<!-- more -->\n\n# 一、变换 \n假如有如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 形式的方程：\n{% math %}\n\\begin{bmatrix}\n4 & -3 & 1 & 3 \\\\\n2 & 0 & 5 & 1\n\\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n=\n\\begin{bmatrix} 5 \\\\ 8 \\end{bmatrix}\n{% endmath %} \n以往我们都是将其看成是几个列向量的线性组合，即{% math %}1\\begin{bmatrix}4 \\\\ 2\\end{bmatrix} + 1\\begin{bmatrix}-3 \\\\ 0\\end{bmatrix} + 1\\begin{bmatrix}1 \\\\ 5\\end{bmatrix} + 1\\begin{bmatrix}3 \\\\ 1\\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 8 \\end{bmatrix}{% endmath %}，这次我们换个角度，把 $\\mathbf{A}$ 看作一个整体，整个方程就是一个 4 维向量 $\\mathbf{x}$ 乘以矩阵 $\\mathbf{A}$ 后得到一个 2 维向量 $\\mathbf{b}$。 以这个观点来看的话，**矩阵 $\\mathbf{A}$ 就相当于一个从一个向量集映射到另一个向量集的函数！**。\n\n假设 $\\mathbf{x}$ 是 $n$ 维向量，$\\mathbf{b}$ 是 $m$ 维向量，则 $\\mathbf{A}$ 就是一个 $R^n$ 到 $R^m$ 的变换。这个变换的`定义域`是 $R^n$，`上域`是 $R^m$，记作 $T: R^n \\rightarrow R^m$。$\\mathbf{x}$ 是 $R^n$ 空间中的一个向量，$T(\\mathbf{x})$ 就是其变换到 $R^m$ 空间中的`像`，而全体像 $T(\\mathbf{x})$ 的集合就称为变换 $T$ 的`值域`。图示如下：\n\n{% asset_img linear-transformation.png 变换$T$ %}\n\n从这种观点来看，矩阵就是一个函数：$\\mathbf{x}\\mapsto\\mathbf{A}{x}$！矩阵既可看作是数据的表示，又可看作是表示变换的函数，这不禁让我联想起了 lisp 里的“同像性”，也就是“代码即数据”。我不知道他们之间有没有更深一层的联系，不过从这一层面再来看矩阵，感觉又多了一层趣味……\n\n除此之外，以动态的眼光来看待矩阵，也有助于我们理解为什么一些随时间变化的系统可以用线性代数来建模。比如马尔科夫链中的转移矩阵，就是用静态的矩阵来表示一个变换的过程。\n\n不难发现，当变换 $T$ 为 $\\mathbf{x}\\mapsto\\mathbf{A}\\mathbf{x}$ ，向量 $\\mathbf{x}$ 若有 n 维，则变换的定义域就是 $R^n$，$\\mathbf{A}$ 就有 n 列；向量 $\\mathbf{b}$  若有 m 维，则变换的上域就是 $R^m$，$\\mathbf{A}$ 就有 m 行（$\\mathbf{A}$ 每一列有 m 个元素）。而变换的值域就是 $\\mathbf{A}$ 中列的所有线性组合组成的集合。\n\n也就是说，像 {% math %}\\begin{bmatrix}1 & -3 \\\\ 3 & 5 \\\\ -1 & 7\\end{bmatrix}{% endmath %}这样的矩阵，所表达的变换就是一个二维到三维的映射 $T:R^2\\rightarrow R^3$。\n\n再例如，矩阵{% math %}\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}{% endmath %} 所表达的变换就是一个投影：把 $R^3$ 中的点投影到 $x_1 x_2$平面，因为：\n{% math %}\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\n=\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ 0 \\end{bmatrix}\n{% endmath %} \n\n# 二、线性变换\n线性变换是一类满足线性条件的变换。所谓的线性条件就是：\n$$T(\\mathbf{u}+\\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) \\\\ \\text{和}\\\\ T(c \\mathbf{u}) = c T(\\mathbf{u})$$\n \n注意到，向量的加法和数乘运算在变换前和变换后的效果是一样的，也就是所谓的线性变换*保持了*向量的加法和数乘运算。\n\n我们假设有一个二维向量 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2 \\end{bmatrix}= x_1 \\mathbf{e}_1 + x_2 \\mathbf{e}_2$，其中 {% math %}\\mathbf{e}_1=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}, \\mathbf{e}_2=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}{% endmath %} 是 2$\\times$ 2 单位矩阵 $\\mathbf{I}_n$ 的列向量。由于线性变换保持加法和数乘运算，所以\n\n{% math %}\n\\begin{equation}\nT(\\mathbf{x})=x_1 T(\\mathbf{e}_1) + x_2 T(\\mathbf{e}_2) = \\begin{bmatrix}T(\\mathbf{e}_1) & T(\\mathbf{e}_2)\\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = \\mathbf{A}\\mathbf{x} \\nonumber\n\\end{equation}\n{% endmath %}\n\n这也就是说，对于每一个线性变换$T: R^n \\rightarrow R^m$，都有唯一一个矩阵 $\\mathbf{A}$，使得 $T(\\mathbf{x})=\\mathbf{A}\\mathbf{x}$，其中 $ \\mathbf{A} = [ T(\\mathbf{e}_1) \\cdots T(\\mathbf{e}_1) ] $。$ \\mathbf{A} $ 被称为是线性变换 $T$ 的`标准矩阵`。\n\n总结一下，线性变换是满足线性条件的变换，所谓线性条件就要求变换前后的加法和数乘运算不变（变换前 a+b 等于 c，则变换后 a'+b' 也等于 c'）。 线性变换有两种描述形式：$T:R^n \\rightarrow R^m$ 和 $ \\mathbf{x} \\mapsto \\mathbf{A}\\mathbf{x} $，后者也被称为`矩阵变换`\n\n> 线性变换强调它作为映射的性质，而矩阵变换则描述了映射是怎样实现的。\n\n# 三、几何中的线性变换\n借助上面线性变换的性质，我们就很容易理解图形学中一些专门用于变换的矩阵了，比如 2 维平面上的旋转矩阵：\n{% math %}\n\\begin{equation}\n\\mathbf{A}=\n\\begin{bmatrix}\n\\cos\\varphi & -\\sin\\varphi \\\\\n\\sin\\varphi & \\cos\\varphi\n\\end{bmatrix} \\nonumber\n\\end{equation}\n{% endmath %}\n\n把它的列向量拆开，就是 {% math %}T(\\mathbf{e}_1) = \\begin{bmatrix}\\cos\\varphi \\\\ \\sin\\varphi \\end{bmatrix}{% endmath %}，{% math %}T(\\mathbf{e}_2) = \\begin{bmatrix}-\\sin\\varphi \\\\ \\cos\\varphi \\end{bmatrix}{% endmath %}也就是 {% math %}\\begin{bmatrix}1\\\\ 0\\end{bmatrix}{% endmath %} 旋转到 {% math %}\\begin{bmatrix}\\cos\\varphi \\\\ \\sin\\varphi\\end{bmatrix}{% endmath %} ，{% math %}\\begin{bmatrix}0\\\\ 1\\end{bmatrix}{% endmath %} 旋转到 {% math %}\\begin{bmatrix}-\\sin\\varphi \\\\ \\cos\\varphi\\end{bmatrix}{% endmath %} 。\n\n旋转变换如下图所示：\n \n{% asset_img rotation.png 旋转变换 %}\n\n# 四、存在性和唯一性问题\n有了线性变换的概念，我们再来回顾之前两章讨论的解的存在性和唯一性的问题。\n\n## 4.1 解的存在性\n\n非线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 可以看做是一个 $ \\mathbf{x} $ 所在空间到 $ \\mathbf{b} $ 所在空间的映射。\n对映射 $T=R^n\\rightarrow R^m$ ，如果 $R^n$ 中任意向量 $\\mathbf{b}$ 都是 $R^n$ 中至少一个 $\\mathbf{x}$ 的像，则称 $T$ 是 $R^n$ 到 $R^m$ 上的映射（或叫`满射`），这时，非线性方程组对于任意的 $ \\mathbf{b} $ 都有解。反过来，如果存在 $ \\mathbf{b} $ 使得非线性方程组无解，那么 $T$ 就不是 $R^n$ 到 $R^m$ 上的满射。它们的几何表示如下图所示：\n\n{% asset_img existence.png 满射 %}\n \n## 4.2 解的唯一性\n如果任意的 $ \\mathbf{b}\\in R^m $ 都是 $R^n$ 中最多一个向量 $ \\mathbf{x} $ 的像，那么就称 $T$ 是`一对一映射`。\n\n一对一映射也就是非线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 对任意 $ \\mathbf{b} $ 要么无解，要么有唯一解。也就是说，当 方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有无穷多解时（即方程含有自由变量，即不满秩，即各列线性相关） ，$T$ 就不是一对一映射，这时齐次方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 只有平凡解。\n\n{% asset_img uniqueness.png 一对一映射 %}\n \n# 参考文献:\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/linear-algebra-3.md","raw":"---\ntitle: 线性代数拾遗（三）：线性变换以及矩阵的意义\ntags:\n  - 数学\n  - 线性代数拾遗\n  - 线性代数\n  - 基础概念\ncategory: 线性代数\nmathjax: true\ndate: 2016-05-20 21:11:00\n---\n\n{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-2 上一章 %}我们讨论了齐次和非齐次两种线性方程组的解集，以及它们的几何意义。由齐次线性方程组，我们引入了零空间的概念；而由非齐次线性方程组，我们引入了列空间的概念。这两个空间目前是我们理解线性方程组的桥梁，未来还会对这些空间进行更进一步的讨论。在这之前，让我们先来研究一下矩阵的意义。 \n\n之前的两章中，矩阵是在矩阵方程中出现的，当时我们理解它的意义为“对向量的一种封装”，也就是一种“数据”的形式理解矩阵的。这一章，我们引入矩阵的另一层意义：`线性变换`。\n\n<!-- more -->\n\n# 一、变换 \n假如有如 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 形式的方程：\n{% math %}\n\\begin{bmatrix}\n4 & -3 & 1 & 3 \\\\\n2 & 0 & 5 & 1\n\\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n=\n\\begin{bmatrix} 5 \\\\ 8 \\end{bmatrix}\n{% endmath %} \n以往我们都是将其看成是几个列向量的线性组合，即{% math %}1\\begin{bmatrix}4 \\\\ 2\\end{bmatrix} + 1\\begin{bmatrix}-3 \\\\ 0\\end{bmatrix} + 1\\begin{bmatrix}1 \\\\ 5\\end{bmatrix} + 1\\begin{bmatrix}3 \\\\ 1\\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 8 \\end{bmatrix}{% endmath %}，这次我们换个角度，把 $\\mathbf{A}$ 看作一个整体，整个方程就是一个 4 维向量 $\\mathbf{x}$ 乘以矩阵 $\\mathbf{A}$ 后得到一个 2 维向量 $\\mathbf{b}$。 以这个观点来看的话，**矩阵 $\\mathbf{A}$ 就相当于一个从一个向量集映射到另一个向量集的函数！**。\n\n假设 $\\mathbf{x}$ 是 $n$ 维向量，$\\mathbf{b}$ 是 $m$ 维向量，则 $\\mathbf{A}$ 就是一个 $R^n$ 到 $R^m$ 的变换。这个变换的`定义域`是 $R^n$，`上域`是 $R^m$，记作 $T: R^n \\rightarrow R^m$。$\\mathbf{x}$ 是 $R^n$ 空间中的一个向量，$T(\\mathbf{x})$ 就是其变换到 $R^m$ 空间中的`像`，而全体像 $T(\\mathbf{x})$ 的集合就称为变换 $T$ 的`值域`。图示如下：\n\n{% asset_img linear-transformation.png 变换$T$ %}\n\n从这种观点来看，矩阵就是一个函数：$\\mathbf{x}\\mapsto\\mathbf{A}{x}$！矩阵既可看作是数据的表示，又可看作是表示变换的函数，这不禁让我联想起了 lisp 里的“同像性”，也就是“代码即数据”。我不知道他们之间有没有更深一层的联系，不过从这一层面再来看矩阵，感觉又多了一层趣味……\n\n除此之外，以动态的眼光来看待矩阵，也有助于我们理解为什么一些随时间变化的系统可以用线性代数来建模。比如马尔科夫链中的转移矩阵，就是用静态的矩阵来表示一个变换的过程。\n\n不难发现，当变换 $T$ 为 $\\mathbf{x}\\mapsto\\mathbf{A}\\mathbf{x}$ ，向量 $\\mathbf{x}$ 若有 n 维，则变换的定义域就是 $R^n$，$\\mathbf{A}$ 就有 n 列；向量 $\\mathbf{b}$  若有 m 维，则变换的上域就是 $R^m$，$\\mathbf{A}$ 就有 m 行（$\\mathbf{A}$ 每一列有 m 个元素）。而变换的值域就是 $\\mathbf{A}$ 中列的所有线性组合组成的集合。\n\n也就是说，像 {% math %}\\begin{bmatrix}1 & -3 \\\\ 3 & 5 \\\\ -1 & 7\\end{bmatrix}{% endmath %}这样的矩阵，所表达的变换就是一个二维到三维的映射 $T:R^2\\rightarrow R^3$。\n\n再例如，矩阵{% math %}\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}{% endmath %} 所表达的变换就是一个投影：把 $R^3$ 中的点投影到 $x_1 x_2$平面，因为：\n{% math %}\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\n=\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ 0 \\end{bmatrix}\n{% endmath %} \n\n# 二、线性变换\n线性变换是一类满足线性条件的变换。所谓的线性条件就是：\n$$T(\\mathbf{u}+\\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) \\\\ \\text{和}\\\\ T(c \\mathbf{u}) = c T(\\mathbf{u})$$\n \n注意到，向量的加法和数乘运算在变换前和变换后的效果是一样的，也就是所谓的线性变换*保持了*向量的加法和数乘运算。\n\n我们假设有一个二维向量 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2 \\end{bmatrix}= x_1 \\mathbf{e}_1 + x_2 \\mathbf{e}_2$，其中 {% math %}\\mathbf{e}_1=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}, \\mathbf{e}_2=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}{% endmath %} 是 2$\\times$ 2 单位矩阵 $\\mathbf{I}_n$ 的列向量。由于线性变换保持加法和数乘运算，所以\n\n{% math %}\n\\begin{equation}\nT(\\mathbf{x})=x_1 T(\\mathbf{e}_1) + x_2 T(\\mathbf{e}_2) = \\begin{bmatrix}T(\\mathbf{e}_1) & T(\\mathbf{e}_2)\\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = \\mathbf{A}\\mathbf{x} \\nonumber\n\\end{equation}\n{% endmath %}\n\n这也就是说，对于每一个线性变换$T: R^n \\rightarrow R^m$，都有唯一一个矩阵 $\\mathbf{A}$，使得 $T(\\mathbf{x})=\\mathbf{A}\\mathbf{x}$，其中 $ \\mathbf{A} = [ T(\\mathbf{e}_1) \\cdots T(\\mathbf{e}_1) ] $。$ \\mathbf{A} $ 被称为是线性变换 $T$ 的`标准矩阵`。\n\n总结一下，线性变换是满足线性条件的变换，所谓线性条件就要求变换前后的加法和数乘运算不变（变换前 a+b 等于 c，则变换后 a'+b' 也等于 c'）。 线性变换有两种描述形式：$T:R^n \\rightarrow R^m$ 和 $ \\mathbf{x} \\mapsto \\mathbf{A}\\mathbf{x} $，后者也被称为`矩阵变换`\n\n> 线性变换强调它作为映射的性质，而矩阵变换则描述了映射是怎样实现的。\n\n# 三、几何中的线性变换\n借助上面线性变换的性质，我们就很容易理解图形学中一些专门用于变换的矩阵了，比如 2 维平面上的旋转矩阵：\n{% math %}\n\\begin{equation}\n\\mathbf{A}=\n\\begin{bmatrix}\n\\cos\\varphi & -\\sin\\varphi \\\\\n\\sin\\varphi & \\cos\\varphi\n\\end{bmatrix} \\nonumber\n\\end{equation}\n{% endmath %}\n\n把它的列向量拆开，就是 {% math %}T(\\mathbf{e}_1) = \\begin{bmatrix}\\cos\\varphi \\\\ \\sin\\varphi \\end{bmatrix}{% endmath %}，{% math %}T(\\mathbf{e}_2) = \\begin{bmatrix}-\\sin\\varphi \\\\ \\cos\\varphi \\end{bmatrix}{% endmath %}也就是 {% math %}\\begin{bmatrix}1\\\\ 0\\end{bmatrix}{% endmath %} 旋转到 {% math %}\\begin{bmatrix}\\cos\\varphi \\\\ \\sin\\varphi\\end{bmatrix}{% endmath %} ，{% math %}\\begin{bmatrix}0\\\\ 1\\end{bmatrix}{% endmath %} 旋转到 {% math %}\\begin{bmatrix}-\\sin\\varphi \\\\ \\cos\\varphi\\end{bmatrix}{% endmath %} 。\n\n旋转变换如下图所示：\n \n{% asset_img rotation.png 旋转变换 %}\n\n# 四、存在性和唯一性问题\n有了线性变换的概念，我们再来回顾之前两章讨论的解的存在性和唯一性的问题。\n\n## 4.1 解的存在性\n\n非线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 可以看做是一个 $ \\mathbf{x} $ 所在空间到 $ \\mathbf{b} $ 所在空间的映射。\n对映射 $T=R^n\\rightarrow R^m$ ，如果 $R^n$ 中任意向量 $\\mathbf{b}$ 都是 $R^n$ 中至少一个 $\\mathbf{x}$ 的像，则称 $T$ 是 $R^n$ 到 $R^m$ 上的映射（或叫`满射`），这时，非线性方程组对于任意的 $ \\mathbf{b} $ 都有解。反过来，如果存在 $ \\mathbf{b} $ 使得非线性方程组无解，那么 $T$ 就不是 $R^n$ 到 $R^m$ 上的满射。它们的几何表示如下图所示：\n\n{% asset_img existence.png 满射 %}\n \n## 4.2 解的唯一性\n如果任意的 $ \\mathbf{b}\\in R^m $ 都是 $R^n$ 中最多一个向量 $ \\mathbf{x} $ 的像，那么就称 $T$ 是`一对一映射`。\n\n一对一映射也就是非线性方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 对任意 $ \\mathbf{b} $ 要么无解，要么有唯一解。也就是说，当 方程 $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ 有无穷多解时（即方程含有自由变量，即不满秩，即各列线性相关） ，$T$ 就不是一对一映射，这时齐次方程组 $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ 只有平凡解。\n\n{% asset_img uniqueness.png 一对一映射 %}\n \n# 参考文献:\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"linear-algebra-3","published":1,"updated":"2016-06-21T15:15:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8d000n6jod7olbpkcp"},{"title":"线性代数拾遗（四）：线性方程组的应用","date":"2016-06-19T16:00:00.000Z","_content":"\n{% asset_img banner.jpeg %}\n\n由于这段时间科研任务较重，加上 hexo 升级后总出现一些奇怪的问题，所以有一段时间没更新这个系列了，今天忙里偷闲补上一篇。\n\n前面几章，我们回顾了一遍线性方程组和矩阵的一些概念。线性代数的最原始问题是解线性方程组，为了解决这个问题，我们引入了向量和矩阵，继而对矩阵的一些特性也进行了一番分析，然后又发现矩阵不但可以表示数据，也可以表示变换。然而，这些概念是如何应用于现实生活呢，实际生活中有哪些线性方程组的例子？这一章我们来介绍一些线性代数的实际应用。\n\n总体上来说，牵涉到多个变量的相互约束，而且这些约束是“线性”的问题时，就有可能通过建立线性方程组从而得到解。\n\n# 一、经济学例子\n这是来自《线性代数及其应用》中的一个例子，很好地展示了线性代数在经济学中的应用：\n\n比如一个国家包括煤炭、电力、钢铁三个部门，各部门都产出一定的资源，同时也消耗一定的资源（为方便讨论，本例中只考虑煤炭、电力、钢铁这三种资源，并且假设所有产出的资源都会被消耗）。比如，煤炭部门生产的每 100 份煤炭中，60 份被电力部门消耗，40 份被钢铁部门消耗；电力部门每生产 100 份煤炭，40 份被煤炭部门消耗，10 份被自己消耗，还有 50 份被钢铁部门消耗；钢铁部门每生产 100 份钢铁，60 份被煤炭部门消耗，20 份被电力部门消耗，还有 20 份被自己消耗。那么，如何给这三种资源定价，使得各部门的收支达到平衡？\n\n<!-- more -->\n\n首先，上面所述的各部门产出与消耗情况可以用一个表格来表示：\n\n|            | 煤炭产出|电力产出|钢铁产出|\n|-----------:|:------:|:-----:|:-----:|\n|煤炭部门的消耗| 0      | 0.4   |0.6    |\n|电力部门的消耗|0.6     | 0.1   |0.2    |\n|钢铁部门的消耗|0.4     | 0.5   |0.2    |\n\n表中每一行表示某部们消耗各资源的情况，各列表示某资源被各部门消耗的情况。例如，煤炭部门每生产 1 单位的煤炭，就有 0.6 单位被电力部门消耗，0.4 单位被钢铁部门消耗；同时，煤炭部门也会消耗 0.4 单位的电力和 0.6 单位的钢铁来保证生产。\n\n当然我们可以考虑用向量来表示这个表格。将表格中的各行`向量化`，得到：\n\n{% math %}\n\\begin{align}\nO_c &= [0, 0.4, 0.6] \\\\\nO_e &= [0.6, 0.1, 0.2] \\\\\nO_s &= [0.4, 0.5, 0.2]\n\\end{align}\n{% endmath %}\n\n其中，$O_c$, $O_e$, $O_s$ 分别表示煤炭、电力、钢铁各个部门消耗三种资源的量。\n\n各种资源的单位价格也可以用符号定义。例如用 $p_c$, $p_e$, $p_s$ 分别来表示煤炭、电力、钢铁三种资源的价格，那么煤炭部门的总支出就是 {% math %}0 \\cdot p_c+0.6 p_e+0.4 p_s = O_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}{% endmath %}. 同理，电力部门和钢铁部门的总收入是 {% math %}O_e \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}, O_s \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}{% endmath %}. \n\n也就是说，煤炭部门每生产出 1 单位价值为 $p_c$ 的煤炭，它就需要消耗价值为 $O_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}$ 的资源。要使煤炭部门收支平衡，就需要：\n\n{% math %}\n\\begin{equation}\nO_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} = p_c\n\\end{equation}\n{% endmath %}\n\n同理，要使三个部门都达到收支平衡，需要：\n\n{% math %}\n\\begin{align}\nO_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0  , 0.4, 0.6] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_c \\\\\nO_e \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0.6, 0.1, 0.2] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_e \\\\\nO_s \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0.4, 0.5, 0.2] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_s\n\\end{align}\n{% endmath %}\n\n借助矩阵的封装，我们可以把这三个式子合并为一个式子：\n{% math %}\n\\begin{eqnarray}\n\\mathbf{A} \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} \\\\\n\\left( \\mathbf{A} - \\mathbf{I} \\right) \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= \\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix} \\label{homogeneous}\\\\\n\\end{eqnarray}\n{% endmath %}\n其中，3 x 3 矩阵 $\\mathbf{A}$ 就是上面的那个表格。\n\n式子 $\\eqref{homogeneous}$ 是我们熟悉的齐次线性方程组的形式。按照套路，我们化简增广矩阵：\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}\n-1 & 0.4 & 0.6 & 0 \\\\\n0.6 & -0.9 & 0.2 & 0 \\\\\n0.4 & 0.5 & -0.8 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & -0.4 & -0.6 & 0 \\\\\n0 & 1 & -0.85 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -0.94 & 0 \\\\\n0 & 1 & -0.85 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n由此得到通解：$p_c = 0.94 p_s$, $p_e = 0.85 p_s$，$p_s$ 为自由变量。\n\n所以，各部门达到收支平衡时的平衡价格向量为：\n{% math %}\n\\begin{equation}\n\\mathbf{p} = p_s\n\\begin{bmatrix}\n0.94 \\\\\n0.85 \\\\\n1\n\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n也就是说，如果钢铁价格为100元，那么煤炭和电的价格分别为94元和和85元时，整个经济系统可以达到平衡。\n\n# 二、总结\n从上面的例子，我们可以发现：\n\n- 当一个系统中各个部分之间存在**线性约束**时（例如化学方程式的配平，各元素之间相互存在约束关系），就有可能借助线性方程组来建模。\n- 当一个问题描述的是一张简单的表格时，我们可以将其`向量化`为一系列向量，或是一个矩阵，进而进行“批量”计算。\n\n这里，我们再次发现了矩阵“封装”计算的特点。借助`向量化`和`矩阵化`，我们可以将传统的数学问题转化为线性代数问题（如本文的例子就转化为了齐次线性方程组）。\n\n# 参考文献\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/linear-algebra-4.md","raw":"---\ntitle: 线性代数拾遗（四）：线性方程组的应用\ndate: 2016-06-20\ntags:\n  - 线性代数拾遗\n  - 线性代数\n  - 数学\n  - 基础概念\ncategories: 线性代数\n---\n\n{% asset_img banner.jpeg %}\n\n由于这段时间科研任务较重，加上 hexo 升级后总出现一些奇怪的问题，所以有一段时间没更新这个系列了，今天忙里偷闲补上一篇。\n\n前面几章，我们回顾了一遍线性方程组和矩阵的一些概念。线性代数的最原始问题是解线性方程组，为了解决这个问题，我们引入了向量和矩阵，继而对矩阵的一些特性也进行了一番分析，然后又发现矩阵不但可以表示数据，也可以表示变换。然而，这些概念是如何应用于现实生活呢，实际生活中有哪些线性方程组的例子？这一章我们来介绍一些线性代数的实际应用。\n\n总体上来说，牵涉到多个变量的相互约束，而且这些约束是“线性”的问题时，就有可能通过建立线性方程组从而得到解。\n\n# 一、经济学例子\n这是来自《线性代数及其应用》中的一个例子，很好地展示了线性代数在经济学中的应用：\n\n比如一个国家包括煤炭、电力、钢铁三个部门，各部门都产出一定的资源，同时也消耗一定的资源（为方便讨论，本例中只考虑煤炭、电力、钢铁这三种资源，并且假设所有产出的资源都会被消耗）。比如，煤炭部门生产的每 100 份煤炭中，60 份被电力部门消耗，40 份被钢铁部门消耗；电力部门每生产 100 份煤炭，40 份被煤炭部门消耗，10 份被自己消耗，还有 50 份被钢铁部门消耗；钢铁部门每生产 100 份钢铁，60 份被煤炭部门消耗，20 份被电力部门消耗，还有 20 份被自己消耗。那么，如何给这三种资源定价，使得各部门的收支达到平衡？\n\n<!-- more -->\n\n首先，上面所述的各部门产出与消耗情况可以用一个表格来表示：\n\n|            | 煤炭产出|电力产出|钢铁产出|\n|-----------:|:------:|:-----:|:-----:|\n|煤炭部门的消耗| 0      | 0.4   |0.6    |\n|电力部门的消耗|0.6     | 0.1   |0.2    |\n|钢铁部门的消耗|0.4     | 0.5   |0.2    |\n\n表中每一行表示某部们消耗各资源的情况，各列表示某资源被各部门消耗的情况。例如，煤炭部门每生产 1 单位的煤炭，就有 0.6 单位被电力部门消耗，0.4 单位被钢铁部门消耗；同时，煤炭部门也会消耗 0.4 单位的电力和 0.6 单位的钢铁来保证生产。\n\n当然我们可以考虑用向量来表示这个表格。将表格中的各行`向量化`，得到：\n\n{% math %}\n\\begin{align}\nO_c &= [0, 0.4, 0.6] \\\\\nO_e &= [0.6, 0.1, 0.2] \\\\\nO_s &= [0.4, 0.5, 0.2]\n\\end{align}\n{% endmath %}\n\n其中，$O_c$, $O_e$, $O_s$ 分别表示煤炭、电力、钢铁各个部门消耗三种资源的量。\n\n各种资源的单位价格也可以用符号定义。例如用 $p_c$, $p_e$, $p_s$ 分别来表示煤炭、电力、钢铁三种资源的价格，那么煤炭部门的总支出就是 {% math %}0 \\cdot p_c+0.6 p_e+0.4 p_s = O_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}{% endmath %}. 同理，电力部门和钢铁部门的总收入是 {% math %}O_e \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}, O_s \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}{% endmath %}. \n\n也就是说，煤炭部门每生产出 1 单位价值为 $p_c$ 的煤炭，它就需要消耗价值为 $O_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix}$ 的资源。要使煤炭部门收支平衡，就需要：\n\n{% math %}\n\\begin{equation}\nO_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} = p_c\n\\end{equation}\n{% endmath %}\n\n同理，要使三个部门都达到收支平衡，需要：\n\n{% math %}\n\\begin{align}\nO_c \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0  , 0.4, 0.6] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_c \\\\\nO_e \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0.6, 0.1, 0.2] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_e \\\\\nO_s \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= [0.4, 0.5, 0.2] \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= p_s\n\\end{align}\n{% endmath %}\n\n借助矩阵的封装，我们可以把这三个式子合并为一个式子：\n{% math %}\n\\begin{eqnarray}\n\\mathbf{A} \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} \\\\\n\\left( \\mathbf{A} - \\mathbf{I} \\right) \\cdot \\begin{bmatrix}p_c\\\\ p_e\\\\ p_s\\end{bmatrix} &= \\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix} \\label{homogeneous}\\\\\n\\end{eqnarray}\n{% endmath %}\n其中，3 x 3 矩阵 $\\mathbf{A}$ 就是上面的那个表格。\n\n式子 $\\eqref{homogeneous}$ 是我们熟悉的齐次线性方程组的形式。按照套路，我们化简增广矩阵：\n{% math %}\n\\begin{equation}\n\\begin{bmatrix}\n-1 & 0.4 & 0.6 & 0 \\\\\n0.6 & -0.9 & 0.2 & 0 \\\\\n0.4 & 0.5 & -0.8 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & -0.4 & -0.6 & 0 \\\\\n0 & 1 & -0.85 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\sim\n\\begin{bmatrix}\n1 & 0 & -0.94 & 0 \\\\\n0 & 1 & -0.85 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n由此得到通解：$p_c = 0.94 p_s$, $p_e = 0.85 p_s$，$p_s$ 为自由变量。\n\n所以，各部门达到收支平衡时的平衡价格向量为：\n{% math %}\n\\begin{equation}\n\\mathbf{p} = p_s\n\\begin{bmatrix}\n0.94 \\\\\n0.85 \\\\\n1\n\\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n也就是说，如果钢铁价格为100元，那么煤炭和电的价格分别为94元和和85元时，整个经济系统可以达到平衡。\n\n# 二、总结\n从上面的例子，我们可以发现：\n\n- 当一个系统中各个部分之间存在**线性约束**时（例如化学方程式的配平，各元素之间相互存在约束关系），就有可能借助线性方程组来建模。\n- 当一个问题描述的是一张简单的表格时，我们可以将其`向量化`为一系列向量，或是一个矩阵，进而进行“批量”计算。\n\n这里，我们再次发现了矩阵“封装”计算的特点。借助`向量化`和`矩阵化`，我们可以将传统的数学问题转化为线性代数问题（如本文的例子就转化为了齐次线性方程组）。\n\n# 参考文献\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"linear-algebra-4","published":1,"updated":"2016-06-21T10:40:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8f000o6jodirzw8ey2"},{"title":"线性代数拾遗（五）：矩阵变换的应用","mathjax":true,"_content":"{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-2 上一章 %}用了一个经济学的例子，介绍了现实中的线性方程组，那个例子里，我们借助矩阵“封装”的作用，将解三个方程组的问题转换为解{% math %}\\mathbf{A}\\mathbf{x}=\\mathbf{0}{% endmath %}。而我们知道，矩阵不仅可以封装数据，还可以表示线性变换，那这一章就来介绍一下矩阵变换在现实生活中的应用。\n\n<!-- more -->\n\n# 一、社会学例子\n这个例子同样来自于《线性代数及其应用》这本书。\n\n假如我们要研究一个城市的人口迁入、迁出的问题。用 $c_i$ 和 $r_i$ 分别表示第 $i$ 年该城市市区和郊区的人口数，$c_0$和$r_0$就是初始年（最开始进行观测的那一年）市区和郊区的人口数。再用 $\\mathbf{x}_i$ 表示第 $i$ 年的人口向量：{% math %} \\mathbf{x}_i = \\begin{bmatrix}c_i \\\\ r_i \\end{bmatrix} {% endmath %}。\n\n设人口统计学研究表明，每年有 5% 的城市人口迁移到郊区（其余 95% 继续留在城市），有 3% 的郊区人口移居城市（其余 97% 继续留在郊区），如下图所示：\n\n{% diagram \"城市人口迁移示意图\" %}\ndigraph {\n    city [label=\"城区\"];\n    rural [label=\"郊区\"];\n    city -> rural [label=\"5%\"];\n    city -> city [label=\"95%\"];\n    rural -> city [label=\"3%\"];\n    rural -> rural [label=\"97%\"];\n}\n{% enddiagram %}\n\n这个研究结果，用数学表示就是，第 $i$ 年的城区人口经过一年后，在城区和郊区的分布为：\n\n{% math %}\n\\begin{equation}\n\\begin{bmatrix} 0.95\\, c_i \\\\ 0.05\\, c_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n第 $i$ 年的郊区人口经过一年后，在城区和郊区的分布为：\n\n{% math %}\n\\begin{equation}\n\\begin{bmatrix} 0.03\\, r_i \\\\ 0.97\\, r_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n这时（第 $i+1$ 年），城区人口有来自一年前城区人口的 95%，以及一年前郊区人口的 3%；同样，郊区人口有来自一年前城区人口的 5%，以及一年前郊区人口的 97%。用向量表示就是，第 $i+1$ 年的城市人口分布：\n\n{% math %}\n\\begin{equation}\n\\mathbf{x}_{i+1} = \\begin{bmatrix} c_{i+1} \\\\ r_{i+1} \\end{bmatrix}\n= c_i \\begin{bmatrix} 0.95 \\\\ 0.05 \\end{bmatrix} + r_i \\begin{bmatrix} 0.03 \\\\ 0.97 \\end{bmatrix}\n= \\begin{bmatrix} 0.95 & 0.03 \\\\ 0.05 & 0.97 \\end{bmatrix}\\begin{bmatrix} c_i \\\\ r_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n也就是每年人口分布是由上一年人口分布左乘一个矩阵（这个矩阵表示线性变换）得到的，我们用 $\\mathbf{M}$ 表示这个矩阵，就是：\n\n{% math %}\n\\begin{equation}\n\\mathbf{x}_{i+1} = \\mathbf{M} \\mathbf{x}_i\n\\end{equation}\n{% endmath %}\n\n从这个例子，我们可以发现，当我们假设人口迁移的规律比较简单时（每一年的人口只与上一年人口有关，且是线性关系），人口的迁移就可以通过线性变换来表示。又由于矩阵可以表示一个线性变换，所以每一年的人口都可看作是上一年人口左乘一个变换矩阵（又叫`转移矩阵`）。\n\n# 二、马尔可夫链\n\n有同学可能已经意识到了，这个例子实际上就是`马尔可夫链`嘛！对，这个就是马尔可夫链。\n\n{% blockquote 中文维基百科/马尔可夫链 https://zh.wikipedia.org/zh-cn/马尔可夫链 %}\n（马尔可夫链）为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备 “无记忆” 的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。\n{% endblockquote %}\n\n在我们的例子里，城市每一年人口只与上一年人口有关，满足马尔可夫性质，所以人口的迁移可以看作是一个马尔可夫链。这个链中表示状态的节点就是 $\\mathbf{x}_i$，节点中的转换关系就是`转移矩阵` $\\mathbf{M}$。\n\n这个例子展示了我们借助线性变换解决几何之外的问题时的思路：\n- 首先，在我们将每年的人口建模为一个二维向量 $\\mathbf{x}_i$ 的时候，这个实际问题就已经转换到二维的空间中了。从这个空间的角度来看，每年人口的变化情况，就是二维向量不断变换的过程。\n- 由于我们假设人口变化是一个线性变换（比如每年城区人口是上一年城区人口和郊区人口的线性组合），所以这个变换可以用矩阵来描述。\n- 又因为我们进一步简化问题，任务每年的变化是稳定的，所以这个线性变换只需要一个矩阵就可以了。\n\n未来关于这个例子还会进一步展开，对`马尔可夫链`的平稳分布进行更深入的分析。\n\n# 参考文献\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","source":"_posts/linear-algebra-5.md","raw":"---\ntitle: 线性代数拾遗（五）：矩阵变换的应用\ntags:\n  - 线性代数拾遗\n  - 线性代数\n  - 数学\n  - 基础概念\ncategories: 线性代数\nmathjax: true\n---\n{% asset_img banner.jpeg %}\n\n{% post_link linear-algebra-2 上一章 %}用了一个经济学的例子，介绍了现实中的线性方程组，那个例子里，我们借助矩阵“封装”的作用，将解三个方程组的问题转换为解{% math %}\\mathbf{A}\\mathbf{x}=\\mathbf{0}{% endmath %}。而我们知道，矩阵不仅可以封装数据，还可以表示线性变换，那这一章就来介绍一下矩阵变换在现实生活中的应用。\n\n<!-- more -->\n\n# 一、社会学例子\n这个例子同样来自于《线性代数及其应用》这本书。\n\n假如我们要研究一个城市的人口迁入、迁出的问题。用 $c_i$ 和 $r_i$ 分别表示第 $i$ 年该城市市区和郊区的人口数，$c_0$和$r_0$就是初始年（最开始进行观测的那一年）市区和郊区的人口数。再用 $\\mathbf{x}_i$ 表示第 $i$ 年的人口向量：{% math %} \\mathbf{x}_i = \\begin{bmatrix}c_i \\\\ r_i \\end{bmatrix} {% endmath %}。\n\n设人口统计学研究表明，每年有 5% 的城市人口迁移到郊区（其余 95% 继续留在城市），有 3% 的郊区人口移居城市（其余 97% 继续留在郊区），如下图所示：\n\n{% diagram \"城市人口迁移示意图\" %}\ndigraph {\n    city [label=\"城区\"];\n    rural [label=\"郊区\"];\n    city -> rural [label=\"5%\"];\n    city -> city [label=\"95%\"];\n    rural -> city [label=\"3%\"];\n    rural -> rural [label=\"97%\"];\n}\n{% enddiagram %}\n\n这个研究结果，用数学表示就是，第 $i$ 年的城区人口经过一年后，在城区和郊区的分布为：\n\n{% math %}\n\\begin{equation}\n\\begin{bmatrix} 0.95\\, c_i \\\\ 0.05\\, c_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n第 $i$ 年的郊区人口经过一年后，在城区和郊区的分布为：\n\n{% math %}\n\\begin{equation}\n\\begin{bmatrix} 0.03\\, r_i \\\\ 0.97\\, r_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n这时（第 $i+1$ 年），城区人口有来自一年前城区人口的 95%，以及一年前郊区人口的 3%；同样，郊区人口有来自一年前城区人口的 5%，以及一年前郊区人口的 97%。用向量表示就是，第 $i+1$ 年的城市人口分布：\n\n{% math %}\n\\begin{equation}\n\\mathbf{x}_{i+1} = \\begin{bmatrix} c_{i+1} \\\\ r_{i+1} \\end{bmatrix}\n= c_i \\begin{bmatrix} 0.95 \\\\ 0.05 \\end{bmatrix} + r_i \\begin{bmatrix} 0.03 \\\\ 0.97 \\end{bmatrix}\n= \\begin{bmatrix} 0.95 & 0.03 \\\\ 0.05 & 0.97 \\end{bmatrix}\\begin{bmatrix} c_i \\\\ r_i \\end{bmatrix}\n\\end{equation}\n{% endmath %}\n\n也就是每年人口分布是由上一年人口分布左乘一个矩阵（这个矩阵表示线性变换）得到的，我们用 $\\mathbf{M}$ 表示这个矩阵，就是：\n\n{% math %}\n\\begin{equation}\n\\mathbf{x}_{i+1} = \\mathbf{M} \\mathbf{x}_i\n\\end{equation}\n{% endmath %}\n\n从这个例子，我们可以发现，当我们假设人口迁移的规律比较简单时（每一年的人口只与上一年人口有关，且是线性关系），人口的迁移就可以通过线性变换来表示。又由于矩阵可以表示一个线性变换，所以每一年的人口都可看作是上一年人口左乘一个变换矩阵（又叫`转移矩阵`）。\n\n# 二、马尔可夫链\n\n有同学可能已经意识到了，这个例子实际上就是`马尔可夫链`嘛！对，这个就是马尔可夫链。\n\n{% blockquote 中文维基百科/马尔可夫链 https://zh.wikipedia.org/zh-cn/马尔可夫链 %}\n（马尔可夫链）为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备 “无记忆” 的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。\n{% endblockquote %}\n\n在我们的例子里，城市每一年人口只与上一年人口有关，满足马尔可夫性质，所以人口的迁移可以看作是一个马尔可夫链。这个链中表示状态的节点就是 $\\mathbf{x}_i$，节点中的转换关系就是`转移矩阵` $\\mathbf{M}$。\n\n这个例子展示了我们借助线性变换解决几何之外的问题时的思路：\n- 首先，在我们将每年的人口建模为一个二维向量 $\\mathbf{x}_i$ 的时候，这个实际问题就已经转换到二维的空间中了。从这个空间的角度来看，每年人口的变化情况，就是二维向量不断变换的过程。\n- 由于我们假设人口变化是一个线性变换（比如每年城区人口是上一年城区人口和郊区人口的线性组合），所以这个变换可以用矩阵来描述。\n- 又因为我们进一步简化问题，任务每年的变化是稳定的，所以这个线性变换只需要一个矩阵就可以了。\n\n未来关于这个例子还会进一步展开，对`马尔可夫链`的平稳分布进行更深入的分析。\n\n# 参考文献\n---\n- 线性代数及其应用：第3版/（美）莱（Lay, D.C.）著；沈复兴等译. ——北京：人民邮电出版社，2007.7\n\n# 版权声明：\n---\n本文中所有文字、图片版权均属本人所有，如需转载请注明来源。\n","slug":"linear-algebra-5","published":1,"date":"2016-06-22T09:55:26.000Z","updated":"2016-06-22T10:45:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8j000s6jod3klqa123"},{"title":"Logistic 回归","date":"2015-10-05T06:50:00.000Z","mathjax":true,"_content":"\n最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：\n\n## 一、问题的引入\n使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 $h\\_\\theta (x)$ 来决定类别归属。例如：$h\\_\\theta (x) < 0.5$ 时，y = 0；$h_\\theta (x) > 0.5$ 时，y = 1。\n\n然而，线性回归得到的假设值 $h\\_\\theta (x)$ 有可能 >1 或是 <0，而且有可能会超出很多，这种情况下使用线性回归似乎不是很好的选择。\n\n为了解决这个问题，我们引入 Logistic 回归方法，将 $h\\_\\theta (x)$ 限制在 (0,1) 范围内。\n\n注意，Logistic 回归是一种**分类**方法，而不是回归方法，名字中的“回归”是历史原因造成的。\n\n<!--more-->\n## 二、Logistic 函数（Logistic Function）\n线性回归中，假设函数 $h\\_\\theta (x)=\\theta ^\\top x$，这里将截距\"藏\"在了向量中，即$\\theta=[\\theta_0, \\theta_1, \\cdots, \\theta_n]^\\top$，$x=[1, x_1, x_2, \\cdots, x_n]^\\top$。\n\n而在 Logistic 回归中，我们使用一个函数来**限制假设函数的值域**，这个函数就叫做 Logistic 函数（Logistic Function，也叫 Sigmoid Function）。\n\nLogistic Function：$$g(z)=\\frac{1}{1+e^{-z}}$$\n\n它的函数图像：\n{% asset_img Logistic-curve.png 逻辑回归函数图像 %}\n\n从图像中可以看出，逻辑回归函数将输入的$(-\\infty, \\infty)$空间映射到了$(0,1)$空间，即将值域限制在了$(0,1)$之内。 限制后的假设函数为：\n\n$$h\\_\\theta (x)=g(\\theta ^\\top x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$$\n\n注意该假设函数中，只有一个参数：$\\theta$，我们接下来就需要通过优化来求解这个参数，以确定分类模型。\n\n## 三、假设函数的直观解释\n由于假设函数的值域为$(0,1)$，而$h\\_\\theta (x)$值越接近1，就越有可能是 y=1 类；反之$h\\_\\theta (x)$值越接近0，越有可能是 y=0 类。\n\n这样看来，假设函数 $h\\_\\theta (x)$ 可以看做是给定 x，其类别 y=1 的估计概率，即\n\n$$h\\_\\theta (x)=P(y=1 \\mid x;\\theta )$$\n\n## 四、寻求优化参数 $\\theta$\n一般来说，寻找参数的过程就是优化目标函数的过程。\n\n### 4.1 线性回归的目标函数\n\n在线性回归中，我们的目标函数为：\n$$J(\\theta )=\\frac{1}{m} \\sum\\_{i=1}^m \\frac{1}{2} (h\\_\\theta (x^{(i)})-y^{(i)})^2$$\n其中，$\\frac{1}{2} (h\\_\\theta (x^{(i)})-y^{(i)})^2$ 部分就是损失函数，即$Cost(h\\_\\theta (x^{(i)}), y^{(i)})$\n\n线性回归的优化目标就是最小化这个目标函数，即让各个样本点的误差达到最小。\n\n### 4.2 Logistic 回归的目标函数\n\n#### 4.2.1 平方形式的损失函数\n\n我们把 Logistic 回归的假设函数 $h\\_\\theta (x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$ 代入到线性回归的损失函数中，得到：\n\n$$Cost(h\\_\\theta (x), y) = \\frac{1}{2} (h\\_\\theta (x)-y)^2 = \\frac{1}{1+e^{-\\theta ^\\top x}}$$\n\n为简便起见，这里以后，将各个点的误差$h\\_\\theta (x^{(i)})-y^{(i)}$简写为$h\\_\\theta (x)-y$。\n\n然而，这样的损失函数代入$J(\\theta )=\\frac{1}{m} \\sum\\limits\\_{i=1}^m Cost(h\\_\\theta x, y)$ 中，得到的目标函数 $J(\\theta )$ 并非凸函数，其函数图像类似下图的左子图。\n\n{% asset_img convex-function.png 非凸函数和凸函数 %}\n\n只有目标函数是凸函数时，我们才能通过各种优化方法（如梯度下降、牛顿法等）找到极值点，进而得到最优值对应的参数。 因此，Logistic 回归需要调整其损失函数形式，以使得目标函数为凸函数。\n\n#### 4.2.2 对数形式的损失函数\n\nLogistic 回归采用的损失函数为：\n$$Cost(h\\_\\theta (x), y)=\n\\begin{cases} -log(h\\_\\theta (x)) &\\text{if y=1} \\\\\\\n-log(1-h\\_\\theta (x)) &\\text{if y=0} \\end{cases}$$\n\n这两个函数 $-log(h\\_\\theta (x))$，$-log(1-h\\_\\theta (x))$ 的函数图像如下图所示。可以看出，当 y=1 时，随着 $h\\_\\theta (x)$ 逐渐趋近于 0（即趋向于“分错类别”），损失函数将剧烈上升，趋向于 $\\infty$，而当 $h\\_\\theta (x)$ 逐渐趋近于 1（即趋向于“分对类别”） 时，损失函数则会逐渐减小到 0。当 y=0 时，情况类似。\n\n可见，当分错类别时，这个损失函数会得到一个比较大的损失，进而来惩罚分类算法。\n\n{% asset_img cost-function.png 损失函数 %}\n\n##### 简化损失函数\n\n上面对数形式损失函数是分段形式的，我们可以将两个式子压缩成一个式子：\n$$Cost(h\\_\\theta (x), y) = -ylog(h\\_\\theta (x)) -(1-y)log(1-h\\_\\theta (x))$$\n\n当 y=0 时，取后半段；当 y=1 时，取前半段。\n\n由此，我们终于得到了 Logistic 回归的目标函数$J(\\theta)$：\n$$\\begin{align} J(\\theta) & = \\frac{1}{m} \\sum\\_{i=1}^m Cost(h\\_\\theta(x^{(i)}), y^{(i)}) \\\\\\\n& = -\\frac{1}{m} [\\sum\\_{i=1}^m y^{(i)}log h\\_\\theta(x^{(i)}) + (1-y^{(i)})log (1-h\\_\\theta (x^{(i)}))] \\\\\\ \\end{align}$$\n\n### 4.3 优化目标函数求参\n\n优化目标函数：$\\min\\_{\\theta} J(\\theta)$，即可得到参数 $\\theta$\n\n那么，如何优化目标函数呢？优化方法有很多种，这里讲一下“梯度下降法”：\n\n#### 4.3.1 梯度下降法（Gradient Descent）\n\n梯度下降法的原理这里不详细解释了，方法比较直观，网上有很多教程可以参考。\n\n梯度下降法的使用很简单：在目标函数上任找一点开始，让参数 $\\theta$ 不断朝着梯度方向迭代，直到收敛，收敛时函数位于极小值处，此时的 $\\theta$ 即为 $\\min\\_{\\theta} J(\\theta)$。\n\n每一步迭代的形式化定义如下：\n$$\\theta\\_j := \\theta\\_j - \\alpha \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)$$\n\n这里引入了一个新的参数：$\\alpha$，表示迭代速度，在这里作为调控因子。另外式子中 $J(\\theta)$ 关于 $\\theta$ 的梯度可以计算得到：\n$$\\frac{\\partial}{\\partial \\theta\\_j} J(\\theta) = \\frac{1}{m} \\sum\\_{i=1}^m(h\\_\\theta (x^{(i)}) - y^{(i)})x\\_j^{(i)}$$\n\n此外，还要注意的是，梯度下降法迭代时，是所有参数：$\\theta\\_0, \\theta\\_1, \\cdots, \\theta\\_n$ 同时迭代的，这个可以以向量形式进行批量计算。\n\n在梯度下降中，需要计算$\\sum\\_{i=1}^m (h\\_\\theta (x^{(i)}) - y^{(i)})x\\_j^{(i)}$，也就是每一个样本 $x^{(i)}$ 都要参与计算。这样在样本量较大时，难免效率底下。有一些改进的方法来解决这个问题，例如随机梯度下降法等，这里就不展开了。\n\n## 五、用求得的参数进行分类\n\n使用求得的参数 $\\theta$，进而预测新的未知变量 $x$：\n$$h\\_\\theta(x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$$\n之前提过了，这个 $h\\_\\theta(x)$ 直观意义为：给定 x，其类别 y=1 的估计概率，即$h\\_\\theta (x)=P(y=1 \\mid x;\\theta )$ 因此，我们有了 $h\\_\\theta(x)$，就能确定未知样本的分类了。\n","source":"_posts/logistic-regression.md","raw":"---\ntitle: Logistic 回归\ndate: 2015-10-05 14:50\ncategory: 机器学习\ntags:\n- 机器学习\n- Logistic 回归\n- Logistic Regression\nmathjax: true\n---\n\n最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：\n\n## 一、问题的引入\n使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 $h\\_\\theta (x)$ 来决定类别归属。例如：$h\\_\\theta (x) < 0.5$ 时，y = 0；$h_\\theta (x) > 0.5$ 时，y = 1。\n\n然而，线性回归得到的假设值 $h\\_\\theta (x)$ 有可能 >1 或是 <0，而且有可能会超出很多，这种情况下使用线性回归似乎不是很好的选择。\n\n为了解决这个问题，我们引入 Logistic 回归方法，将 $h\\_\\theta (x)$ 限制在 (0,1) 范围内。\n\n注意，Logistic 回归是一种**分类**方法，而不是回归方法，名字中的“回归”是历史原因造成的。\n\n<!--more-->\n## 二、Logistic 函数（Logistic Function）\n线性回归中，假设函数 $h\\_\\theta (x)=\\theta ^\\top x$，这里将截距\"藏\"在了向量中，即$\\theta=[\\theta_0, \\theta_1, \\cdots, \\theta_n]^\\top$，$x=[1, x_1, x_2, \\cdots, x_n]^\\top$。\n\n而在 Logistic 回归中，我们使用一个函数来**限制假设函数的值域**，这个函数就叫做 Logistic 函数（Logistic Function，也叫 Sigmoid Function）。\n\nLogistic Function：$$g(z)=\\frac{1}{1+e^{-z}}$$\n\n它的函数图像：\n{% asset_img Logistic-curve.png 逻辑回归函数图像 %}\n\n从图像中可以看出，逻辑回归函数将输入的$(-\\infty, \\infty)$空间映射到了$(0,1)$空间，即将值域限制在了$(0,1)$之内。 限制后的假设函数为：\n\n$$h\\_\\theta (x)=g(\\theta ^\\top x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$$\n\n注意该假设函数中，只有一个参数：$\\theta$，我们接下来就需要通过优化来求解这个参数，以确定分类模型。\n\n## 三、假设函数的直观解释\n由于假设函数的值域为$(0,1)$，而$h\\_\\theta (x)$值越接近1，就越有可能是 y=1 类；反之$h\\_\\theta (x)$值越接近0，越有可能是 y=0 类。\n\n这样看来，假设函数 $h\\_\\theta (x)$ 可以看做是给定 x，其类别 y=1 的估计概率，即\n\n$$h\\_\\theta (x)=P(y=1 \\mid x;\\theta )$$\n\n## 四、寻求优化参数 $\\theta$\n一般来说，寻找参数的过程就是优化目标函数的过程。\n\n### 4.1 线性回归的目标函数\n\n在线性回归中，我们的目标函数为：\n$$J(\\theta )=\\frac{1}{m} \\sum\\_{i=1}^m \\frac{1}{2} (h\\_\\theta (x^{(i)})-y^{(i)})^2$$\n其中，$\\frac{1}{2} (h\\_\\theta (x^{(i)})-y^{(i)})^2$ 部分就是损失函数，即$Cost(h\\_\\theta (x^{(i)}), y^{(i)})$\n\n线性回归的优化目标就是最小化这个目标函数，即让各个样本点的误差达到最小。\n\n### 4.2 Logistic 回归的目标函数\n\n#### 4.2.1 平方形式的损失函数\n\n我们把 Logistic 回归的假设函数 $h\\_\\theta (x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$ 代入到线性回归的损失函数中，得到：\n\n$$Cost(h\\_\\theta (x), y) = \\frac{1}{2} (h\\_\\theta (x)-y)^2 = \\frac{1}{1+e^{-\\theta ^\\top x}}$$\n\n为简便起见，这里以后，将各个点的误差$h\\_\\theta (x^{(i)})-y^{(i)}$简写为$h\\_\\theta (x)-y$。\n\n然而，这样的损失函数代入$J(\\theta )=\\frac{1}{m} \\sum\\limits\\_{i=1}^m Cost(h\\_\\theta x, y)$ 中，得到的目标函数 $J(\\theta )$ 并非凸函数，其函数图像类似下图的左子图。\n\n{% asset_img convex-function.png 非凸函数和凸函数 %}\n\n只有目标函数是凸函数时，我们才能通过各种优化方法（如梯度下降、牛顿法等）找到极值点，进而得到最优值对应的参数。 因此，Logistic 回归需要调整其损失函数形式，以使得目标函数为凸函数。\n\n#### 4.2.2 对数形式的损失函数\n\nLogistic 回归采用的损失函数为：\n$$Cost(h\\_\\theta (x), y)=\n\\begin{cases} -log(h\\_\\theta (x)) &\\text{if y=1} \\\\\\\n-log(1-h\\_\\theta (x)) &\\text{if y=0} \\end{cases}$$\n\n这两个函数 $-log(h\\_\\theta (x))$，$-log(1-h\\_\\theta (x))$ 的函数图像如下图所示。可以看出，当 y=1 时，随着 $h\\_\\theta (x)$ 逐渐趋近于 0（即趋向于“分错类别”），损失函数将剧烈上升，趋向于 $\\infty$，而当 $h\\_\\theta (x)$ 逐渐趋近于 1（即趋向于“分对类别”） 时，损失函数则会逐渐减小到 0。当 y=0 时，情况类似。\n\n可见，当分错类别时，这个损失函数会得到一个比较大的损失，进而来惩罚分类算法。\n\n{% asset_img cost-function.png 损失函数 %}\n\n##### 简化损失函数\n\n上面对数形式损失函数是分段形式的，我们可以将两个式子压缩成一个式子：\n$$Cost(h\\_\\theta (x), y) = -ylog(h\\_\\theta (x)) -(1-y)log(1-h\\_\\theta (x))$$\n\n当 y=0 时，取后半段；当 y=1 时，取前半段。\n\n由此，我们终于得到了 Logistic 回归的目标函数$J(\\theta)$：\n$$\\begin{align} J(\\theta) & = \\frac{1}{m} \\sum\\_{i=1}^m Cost(h\\_\\theta(x^{(i)}), y^{(i)}) \\\\\\\n& = -\\frac{1}{m} [\\sum\\_{i=1}^m y^{(i)}log h\\_\\theta(x^{(i)}) + (1-y^{(i)})log (1-h\\_\\theta (x^{(i)}))] \\\\\\ \\end{align}$$\n\n### 4.3 优化目标函数求参\n\n优化目标函数：$\\min\\_{\\theta} J(\\theta)$，即可得到参数 $\\theta$\n\n那么，如何优化目标函数呢？优化方法有很多种，这里讲一下“梯度下降法”：\n\n#### 4.3.1 梯度下降法（Gradient Descent）\n\n梯度下降法的原理这里不详细解释了，方法比较直观，网上有很多教程可以参考。\n\n梯度下降法的使用很简单：在目标函数上任找一点开始，让参数 $\\theta$ 不断朝着梯度方向迭代，直到收敛，收敛时函数位于极小值处，此时的 $\\theta$ 即为 $\\min\\_{\\theta} J(\\theta)$。\n\n每一步迭代的形式化定义如下：\n$$\\theta\\_j := \\theta\\_j - \\alpha \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)$$\n\n这里引入了一个新的参数：$\\alpha$，表示迭代速度，在这里作为调控因子。另外式子中 $J(\\theta)$ 关于 $\\theta$ 的梯度可以计算得到：\n$$\\frac{\\partial}{\\partial \\theta\\_j} J(\\theta) = \\frac{1}{m} \\sum\\_{i=1}^m(h\\_\\theta (x^{(i)}) - y^{(i)})x\\_j^{(i)}$$\n\n此外，还要注意的是，梯度下降法迭代时，是所有参数：$\\theta\\_0, \\theta\\_1, \\cdots, \\theta\\_n$ 同时迭代的，这个可以以向量形式进行批量计算。\n\n在梯度下降中，需要计算$\\sum\\_{i=1}^m (h\\_\\theta (x^{(i)}) - y^{(i)})x\\_j^{(i)}$，也就是每一个样本 $x^{(i)}$ 都要参与计算。这样在样本量较大时，难免效率底下。有一些改进的方法来解决这个问题，例如随机梯度下降法等，这里就不展开了。\n\n## 五、用求得的参数进行分类\n\n使用求得的参数 $\\theta$，进而预测新的未知变量 $x$：\n$$h\\_\\theta(x)=\\frac{1}{1+e^{-\\theta ^\\top x}}$$\n之前提过了，这个 $h\\_\\theta(x)$ 直观意义为：给定 x，其类别 y=1 的估计概率，即$h\\_\\theta (x)=P(y=1 \\mid x;\\theta )$ 因此，我们有了 $h\\_\\theta(x)$，就能确定未知样本的分类了。\n","slug":"logistic-regression","published":1,"updated":"2016-05-20T18:14:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8n000t6jodij3yx69y"},{"title":"《统计思维：程序员数学之概率统计》读书摘录","date":"2015-10-03T07:20:00.000Z","updated":"2015-10-03T07:20:00.000Z","mathjax":true,"_content":"## 第一章 探索性数据分析\n\n### 重编码（recode）\n不是调查收集的原始数据，而是使用原始数据计算得到的变量。\n可用来检查数据的一致性和准确性\n\n---\n## 第二章 分布\n\n### 直方图\n直方图能判断分布的形状，容易发现最常出现的值，但不一定能看到很少出现的值（离群值）\n<!--more-->\n### 变量分布\n\n- 集中趋势 —— 均值、中位数\n    变量值是否聚集在某个值附近？\n\n- 众数\n    是否有多个聚集点？\n\n- 展布 —— 方差\n    变量的变化性如何？\n\n- 尾部\n    当值偏离众数时，其概率降低多快？\n\n- 离群值\n    是否有远离众数的极端值？\n\n### 汇总统计量\n\n- 均值\n    描述分布的“集中趋势”\n\n\t$$x=\\frac{1}{n} \\sum\\_i{x\\_i}$$\n\n- 方差\n    - 描述分布的变化性，或“展布”\n\n    $$S^2=\\frac{1}{n} \\sum\\_i{(x\\_i -\\bar{x} )^2}$$\n\n- 效应量\n    - 均值的差值：$\\bar{x\\_1} -\\bar{x\\_2} $\n    - 均值差/合并标准差：\n    $$d=\\bar{x\\_1} -\\bar{x\\_2} \\over s$$\n    其中，$s= \\frac{n\\_1 \\cdot s\\_1 + n\\_2 \\cdot s\\_2}{n\\_1+n\\_2}$，该量描述均值差相对标准差的**倍数**\n    \n## 第三章 概率质量函数\n### 概率质量函数（Probability Mass Function，PMF）\n概率质量函数将每个值映射到其概率，$概率=\\frac{频数}{样本量} $\n\n* PMF 适用于变量值数量较少的情况。随着值的数量增加，每个值对应的概率会变得越来越小，随机噪音的影响就会变大。\n\n## 第四章 累积分布函数（cumulative distribution function，CDF）\n### 百分位秩（percentile rank）\n在标准化考试成绩中，百分位秩是**比你成绩低（或相同）的人的比例**\n\n### CDF\nCDF将一个值映射到百分位秩\nCDF(x) = 小于或等于x的值在分布中所占的比例\n\n* 不同于百分位秩，CDF范围为0到1\n* 第 50 百分位就是**中位数**\n* 基于百分位数的统计量\n\t* 中位数（median）\n\t* 四分位矩（interquartile range，IQR）\n\t* 等份点——分位数（quantile）\n\n### 利用CDF生成随机样本\n* 无论CDF的形状如何，其百分位秩的分布都是均匀的\n利用这一点，可以使用给定的CDF生成随机数：\n\n1. 从0到100中均匀地选择一个百分位秩\n2. 使用cdf.Percentile，得到分布中对应所选百分位秩的值\n\n## 第五章 分布建模\n基于有限样本的经验观察，被称为*经验分布*\n*分析分布*试图建立一个简化的分布模型，来描述真实世界的分布，可以用作经验分布的**建模**\n\n* 现实世界的很多现象都可以用分析分布进行建模\n* 分析分布是一种**抽象**，也是一种**数据压缩形式**：如果模型很好地拟合了一个数据集，那么只需几个参数便可对大量数据进行概括\n* 有时候，我们会发现一种自然现象符合某个分析分布，进而这种认识可以帮助我们更好地了解物理系统：如 Pareto 分布经常是由带有正反馈的生成型过程导致的。\n* 分析分布可以方便地进行数学分析\n* 现实世界的数据**永远不会**完美地符合一个分析分布，现实世界和数学模型之间总是存在着差异。\n* 什么是“相关”的，什么是“不必要”的，这取决于你将这个模型用于何种用途。\n\n## 第六章 概率密度函数（Probability density function, PDF）\n概率密度度量单位 x 的概率。\n\n### 核密度估计（kernel density estimation, KDE）\n核密度估计可以对一个样本寻找符合样本数据的适当平滑的 PDF\n\nKDE 估计密度函数可用于：\n\n1. 可视化\n\t在项目探索阶段，可通过 CDF 展现分布，继而判断估计 PDF 是否为该分布的适宜模型。\n2. 插值\n\t如果相信总体分布是平滑的，那么就可以使用KDE为样本中不存在的值插入相应的密度。\n3. 模拟\n\t我们可以使用 KDE，对样本分布做平滑处理，使得模拟可以探索可能性更高的结果。\n\n### PMF、CDF、PDF 的关系\n\n{% asset_img pmf-cdf-pdf.jpg 分布函数的关系框架 %}\n\nPMF：一组**离散值**的概率。PDF 累加得到 CDF。\nCDF：累积概率。\nPDF：**连续性** CDF 的导数。\n\n离散型分布 -> 连续型分布：平滑处理\n1. 假设数据来自连续的分析分布，然后估计这个分布的参数\n2. 核密度估计\n\n### 矩（moment）\n原始矩是一个统计量。对一组值为$x\\_i$的样本，第 k 个原始矩为：\n$$m'\\_k=\\frac{1}{n} \\sum{\\_i} x\\_i^k$$ 当 k=1 时，原始矩即为样本均值 $\\bar{x} $。\n\n第 k 个中心矩（central moment）计算公式：\n$$m\\_k=\\frac{1}{n} \\sum{\\_i} (x\\_i -\\bar{x} )^k$$\n当 k=2 时，中心矩即为方差。\n\n#### 为什么称为“矩”\n> 如果我们在直尺的不同位置 $x\\_i$ 附加一个重物，然后将直尺围绕这些值的均值旋转，旋转重物的[惯性力矩](http://en.wikipedia.org/wiki/Moment_of_inertia)就是这些值的方差。\n\n#### 统计量的单位\n如果值$x\\_i$的单位是厘米，那么第一原始矩的单位是*厘米*，第二原始矩的单位是*平方厘米*，第三原始矩的单位是*立方厘米*\n\n### 偏度（skewness）\n如果分布是以集中趋势为中心对称的，那么这个分布就是*非偏斜的（unskewed）*。\n如果分布中的值向右**延伸更多**，那么这个分布就是*右偏的（right skewed）*；\n如果分布中的值向左**延伸更多**，那么这个分布就是*左偏的（left skewed）*；\n\n* *偏斜（skewed）*与*有偏（biased）*并无关系。偏度只是描述了分布的形状。\n\n#### 偏度的计算\n对给定的值序列 $x\\_i$，样本偏度 $$g\\_1= \\frac{ \\tfrac{1}{n} \\sum{\\_i} (x\\_i-\\bar{x} )^3}{方差^3} $$\n\n偏度为负值代表分布左偏，正值代表分布右偏。$g\\_1$的大小代表偏斜的程度。\n\n#### 衡量分布对称性的另一个方法\n实际应用中，样本偏度容易受分布中离群值的影响，因此，计算样本偏度通常并非好主意。\n\n衡量分布对称性的另一个方法是**检查均值和中位数的关系**：极端值对均值的影响比对中位数影响更大\n* 在左偏分布中，均值 < 中位数\n* 在右偏分布中，均值 > 中位数\n\n##### Pearson 中位数偏度系数（Pearson's median skewness coefficient）\n$$g\\_p=3(\\bar{x} -m)/S$$\n\nPearson 中位数偏度系数更加稳健（robust）。\n\n## 第七章 变量之间的关系\n\n> 如果能够从一个变量的信息得到另一个变量的信息，那么这两个变量就是**相关的**。\n\n### 散点图\n\n研究两个变量之间关系最简单方法就是散点图（scatter plot）。\n\n#### 抖动技术（jittering）\n当获取数据由于某种原因而丢失部分信息，例如数据经过四舍五入后丢失了更高精度的小数位信息，变得“更离散”而导致散点图不美观，这时可以通过加入随机噪音（例如服从均匀分布的随机值）来填补各离散值之间的空隙。\n\n**抖动技术只是为了图像的美观，应只用于视觉呈现，而不能通过它来分析数据。**\n\n#### 饱和（saturation）\n由于数据在一定区域内过于密集，甚至出现重叠，而离群值显得特别突出。这种情况称为“饱和”。\n\n解决办法：\n将图中的点以半透明显示（例如 alpha=0.2），这样重叠的点只会导致那一片的数据点颜色更深。\n（我的理解：**以半透明显示的办法，相当于把数据正则化（Normalized）了**，原先数据密度值的域为二值情况（0 or 1），**重叠区域密度大于 1 无法在图上体现，相当于上溢出。通过规范化，将密度值域打散在（0，1）连续域内了**，重叠区域密度只要密度不超过1，就能在散点图中较好地体现出来）\n\n### 相关性（correlation）\n相关性是一个**量化两个变量之间关系强弱**的统计量。\n\n要衡量两个变量时，比较麻烦的是两个变量之间单位不同，无法直接比较。为此，我们需要将两个变量统一到一个量纲上。\n\n常见的相关系数有两种：\n- Pearson乘积矩相关系数：将变量统一为标准分数（standard score），即偏离均值的标准差数：$z\\_i = (x\\_i - \\mu)/\\sigma$。\n- Spearman秩相关系数：将变量转换为秩（rank），就是该变量在其所有值中的排名。\n\n标准分数有以下这些性质：\n- $z\\_i$是无量纲（单位）的，其分布均值为0，方差为1；\n- 如果 $X$ 服从正态分布，则 $Z$ 也服从正态分布；\n- 如果 $X$ 是偏斜的或包含离群值，则 $Z$ 也是偏斜的或包含离群值；\n\n当观察的变量 $X$ 有偏斜，或是含有离群值时，$Z$ 也会受影响。这种情况下，使用百分位秩更加稳健，因为百分位秩总是服从(1, n)的均匀分布：$R~U(1, n)$，其中 $n$ 为样本数。\n\n### 协方差（covariance）\n协方差用于**度量两个变量共同变化的趋势**。\n\n假设现在有两个序列 $X$ 和 $Y$，则两个序列中，值与均值的偏差为：\n$$\\begin{cases}\ndx\\_i = x\\_i - \\bar{x} \\\\\\\ndy\\_i = y\\_i - \\bar{y}\n\\end{cases}$$\n\n如果两个序列变化趋势一致的话，$dx\\_i$ 和 $dy\\_i$ 同号，即$dx\\_i \\cdot dy\\_i > 0$；反之二者异号，即$dx\\_i \\cdot dy\\_i < 0$。\n\n**我们将这一序列所有样本的偏差加到一块，应该就能度量两个序列之间共同变化的趋势**，于是我们就有了“协方差”的定义：\n$$\\operatorname{Cov(X,Y)} = \\frac{1}{n} \\sum dx\\_i dy\\_i$$\n注意到协方差定义中最后乘了一个 $\\frac{1}{n}$ 来正则化（$n$为序列长度，此处要求$X$和$Y$必须长度相同）。\n\n*当$X$和$Y$两个向量正交时，协方差为0*\n\n### Pearson相关系数\n协方差的单位是两个变量单位的乘积，这样会使人对它的意义感到迷惑，因而人们很少将协方差作为摘要统计量。接下来介绍的 Pearson相关系数解决了这个问题。\n\n$$\\rho = \\operatorname{Cov(X,Y)} / S\\_X S\\_Y \\tag{p}\\label{pearson}$$\n可见，Pearson相关系数加入了标准差来正则化协方差，而且是无量纲的。\n\n我们可以把$\\ref{pearson}$式中的$\\operatorname{Cov(X,Y)}$展开，就能发现：\n$$ \\rho = \\frac{1}{n} \\sum{ \\left( \\left( x\\_i-\\bar{x} \\right) / S\\_X \\right) \\left( \\left(y\\_i-\\bar{y}\\right) / S\\_Y \\right) } $$\n也就是说，Pearson相关系数在计算偏差时就将其与标准差相比较，得到一个归一化的结果：标准分数，从而实现无量纲的。\n\nPearson相关系数值的值域为$[-1, +1]$。如果$\\rho >0$则两个变量正相关；反之$\\rho <0$时，两个变量负相关。$\\rho$的大小描述了相关性的强弱程度，当$\\left\\lvert \\rho \\right\\rvert =1$时，两个变量完全相关，这时，只需要一个变量就能准确预测另一个变量。\n\n**注意，Pearson相关系数接近0时，并不能代表变量之间没有相关关系，因为Peason相关系数只度量了线性关系。如果变量之间存在非线性关系，那么用$\\rho$度量相关性就不那么准确了**。下图为一些具有非线性关系的变量的散点图，然而它们的相关系数都为0。\n\n{% asset_img nonlinear-correlation-example.png 非线性关系变量散点图及其相关系数 %}\n\n图片来源：[英文维基百科/Correlation and dependence](http://www.wikiwand.com/en/Correlation_and_dependence)\n\n### Spearman秩相关\n当变量之间呈线性相关，且变量大致符合正态分布时，Pearson相关系数能较好地说明相关性的强弱。然而离群值会影响Pearson相关系数的稳健性（回忆一下，Pearson相关系数定义中，每个样本的标准分数是相同权重加和的，因而离群值能较大程度地影响结果）\n\nSpearman相关系数不仅能描述变量的相关性，还能够缓解离群值及偏斜分布的影响。缓解离群值影响是通过计算各个值的秩(rank)实现的。如在样本[1, 2, 5, 7]中，值5出现在有序列表的第3位，因此5的值为3。\n\n*缓解离群值的常见方法就是用“排名”而非其值来计算。比如取中值（排名中位数）就比取均值面对离群点时更加稳健。*\n\n### 相关性和因果关系\n\n如果变量A和B相关，则有三种可能的解释：A导致B，或B导致A，或其他某种因素导致A和B。这些解释称为“因果关系”。\n\n只有相关性，我们无法证明因果关系。要证明因果关系，有以下三种方法：\n1. 时间\n    如果 A 在 B 之前发生，则 A 可能导致 B，而 B 不可能导致 A。\n2. 随机性\n    类似随机对照试验。\n3.回归分析\n\n# 第八章 估计\n使用样本来**估计**分布，用来估计的统计量叫做**估计量**（estimator）\n\n## 均方误差（mean squared error，MSE）\n均方误差可用来衡量估计量对分布的描述效果\n$$ MSE = \\frac{1}{m} \\sum (\\bar{x} - \\mu)^2 $$\n其中$m$为抽样次数。\n\n### 均方根误差（root mean squared error，RMSE）\n均方根误差就是均方误差的平方根：\n$$ RMSE = \\sqrt{MSE} $$\n\n用均方根误差不一定越小越好，因为估计量在实际情况中不一定会出现。最可能与实际值相符的估计叫做**最大似然估计（maximum likelihood estimator，MLE）**\n\n## 样本方差\n用样本方差作为估计量估计分布的方差是最直观的方法：\n$$S^2 = \\frac{1}{n} \\sum (x\\_i - \\bar{x})^2$$\n\n然而，$$S^2$$是偏倚（biased）估计量，对于小样本，$S^2$通常比分布的方差低很多。\n\n无偏估计量是$$S\\_{n-1}^2$$\n$$ S\\_{n-1}^2 = \\frac{1}{n-1} \\sum (x\\_i - \\bar{x})^2 $$\n其中，减去的“1”是自由度。\n\n## 抽样\n### 抽样误差\n由随机选择导致的估计变化称为**抽样误差（sampling error）**，例如，从数量庞大的大猩猩总体中抽取9只测量体重，然而却运气不佳抽到了最重的9只。\n\n### 抽样分布\n抽样若干次，对每次抽样的估计量进行统计，得到的分布叫做**抽样分布**。抽样分布是对估计量分布的描述，而不是实际的值的分布。\n\n### 抽样分布的描述\n- 标准误差（standard error，SE）\n标准误差度量估计值平均偏离实际值多少。注意标准误差描述的是**估计量**变化的情况，而标准差描述的是**度量值**变化的情况。\n- 置信区间（confidential interval，CI）\n抽样分布中指定比例的范围。例如，90%置信区间表示的是 第5个百分位数 到 第95个百分位数。\n\n### 抽样偏倚\n由于抽样过程导致的误差，叫做抽样偏倚（sampling bias）。比如，通过电话抽样统计女性体重，由于可能不能覆盖到没有电话或是不公布号码的人，实际统计的人群会有偏差。\n\n### 测量误差\n测量误差是得到的结果不准确。比如，统计女性体重时，只是询问而不测量；或是由于调查参与者“美化”自己的数字。\n\n在汇报一个估计值时，可以用标准误差或置信区间。但要记住：抽样误差只是误差来源之一，而且通常不是最大的误差来源。\n","source":"_posts/think-statistics-note.md","raw":"---\ntitle: 《统计思维：程序员数学之概率统计》读书摘录\ndate: 2015-10-03 15:20:00\nupdated: 2015-10-03 15:20:00\ncategory: statistics\ntags:\n- 统计\n- 读书笔记\nmathjax: true\n---\n## 第一章 探索性数据分析\n\n### 重编码（recode）\n不是调查收集的原始数据，而是使用原始数据计算得到的变量。\n可用来检查数据的一致性和准确性\n\n---\n## 第二章 分布\n\n### 直方图\n直方图能判断分布的形状，容易发现最常出现的值，但不一定能看到很少出现的值（离群值）\n<!--more-->\n### 变量分布\n\n- 集中趋势 —— 均值、中位数\n    变量值是否聚集在某个值附近？\n\n- 众数\n    是否有多个聚集点？\n\n- 展布 —— 方差\n    变量的变化性如何？\n\n- 尾部\n    当值偏离众数时，其概率降低多快？\n\n- 离群值\n    是否有远离众数的极端值？\n\n### 汇总统计量\n\n- 均值\n    描述分布的“集中趋势”\n\n\t$$x=\\frac{1}{n} \\sum\\_i{x\\_i}$$\n\n- 方差\n    - 描述分布的变化性，或“展布”\n\n    $$S^2=\\frac{1}{n} \\sum\\_i{(x\\_i -\\bar{x} )^2}$$\n\n- 效应量\n    - 均值的差值：$\\bar{x\\_1} -\\bar{x\\_2} $\n    - 均值差/合并标准差：\n    $$d=\\bar{x\\_1} -\\bar{x\\_2} \\over s$$\n    其中，$s= \\frac{n\\_1 \\cdot s\\_1 + n\\_2 \\cdot s\\_2}{n\\_1+n\\_2}$，该量描述均值差相对标准差的**倍数**\n    \n## 第三章 概率质量函数\n### 概率质量函数（Probability Mass Function，PMF）\n概率质量函数将每个值映射到其概率，$概率=\\frac{频数}{样本量} $\n\n* PMF 适用于变量值数量较少的情况。随着值的数量增加，每个值对应的概率会变得越来越小，随机噪音的影响就会变大。\n\n## 第四章 累积分布函数（cumulative distribution function，CDF）\n### 百分位秩（percentile rank）\n在标准化考试成绩中，百分位秩是**比你成绩低（或相同）的人的比例**\n\n### CDF\nCDF将一个值映射到百分位秩\nCDF(x) = 小于或等于x的值在分布中所占的比例\n\n* 不同于百分位秩，CDF范围为0到1\n* 第 50 百分位就是**中位数**\n* 基于百分位数的统计量\n\t* 中位数（median）\n\t* 四分位矩（interquartile range，IQR）\n\t* 等份点——分位数（quantile）\n\n### 利用CDF生成随机样本\n* 无论CDF的形状如何，其百分位秩的分布都是均匀的\n利用这一点，可以使用给定的CDF生成随机数：\n\n1. 从0到100中均匀地选择一个百分位秩\n2. 使用cdf.Percentile，得到分布中对应所选百分位秩的值\n\n## 第五章 分布建模\n基于有限样本的经验观察，被称为*经验分布*\n*分析分布*试图建立一个简化的分布模型，来描述真实世界的分布，可以用作经验分布的**建模**\n\n* 现实世界的很多现象都可以用分析分布进行建模\n* 分析分布是一种**抽象**，也是一种**数据压缩形式**：如果模型很好地拟合了一个数据集，那么只需几个参数便可对大量数据进行概括\n* 有时候，我们会发现一种自然现象符合某个分析分布，进而这种认识可以帮助我们更好地了解物理系统：如 Pareto 分布经常是由带有正反馈的生成型过程导致的。\n* 分析分布可以方便地进行数学分析\n* 现实世界的数据**永远不会**完美地符合一个分析分布，现实世界和数学模型之间总是存在着差异。\n* 什么是“相关”的，什么是“不必要”的，这取决于你将这个模型用于何种用途。\n\n## 第六章 概率密度函数（Probability density function, PDF）\n概率密度度量单位 x 的概率。\n\n### 核密度估计（kernel density estimation, KDE）\n核密度估计可以对一个样本寻找符合样本数据的适当平滑的 PDF\n\nKDE 估计密度函数可用于：\n\n1. 可视化\n\t在项目探索阶段，可通过 CDF 展现分布，继而判断估计 PDF 是否为该分布的适宜模型。\n2. 插值\n\t如果相信总体分布是平滑的，那么就可以使用KDE为样本中不存在的值插入相应的密度。\n3. 模拟\n\t我们可以使用 KDE，对样本分布做平滑处理，使得模拟可以探索可能性更高的结果。\n\n### PMF、CDF、PDF 的关系\n\n{% asset_img pmf-cdf-pdf.jpg 分布函数的关系框架 %}\n\nPMF：一组**离散值**的概率。PDF 累加得到 CDF。\nCDF：累积概率。\nPDF：**连续性** CDF 的导数。\n\n离散型分布 -> 连续型分布：平滑处理\n1. 假设数据来自连续的分析分布，然后估计这个分布的参数\n2. 核密度估计\n\n### 矩（moment）\n原始矩是一个统计量。对一组值为$x\\_i$的样本，第 k 个原始矩为：\n$$m'\\_k=\\frac{1}{n} \\sum{\\_i} x\\_i^k$$ 当 k=1 时，原始矩即为样本均值 $\\bar{x} $。\n\n第 k 个中心矩（central moment）计算公式：\n$$m\\_k=\\frac{1}{n} \\sum{\\_i} (x\\_i -\\bar{x} )^k$$\n当 k=2 时，中心矩即为方差。\n\n#### 为什么称为“矩”\n> 如果我们在直尺的不同位置 $x\\_i$ 附加一个重物，然后将直尺围绕这些值的均值旋转，旋转重物的[惯性力矩](http://en.wikipedia.org/wiki/Moment_of_inertia)就是这些值的方差。\n\n#### 统计量的单位\n如果值$x\\_i$的单位是厘米，那么第一原始矩的单位是*厘米*，第二原始矩的单位是*平方厘米*，第三原始矩的单位是*立方厘米*\n\n### 偏度（skewness）\n如果分布是以集中趋势为中心对称的，那么这个分布就是*非偏斜的（unskewed）*。\n如果分布中的值向右**延伸更多**，那么这个分布就是*右偏的（right skewed）*；\n如果分布中的值向左**延伸更多**，那么这个分布就是*左偏的（left skewed）*；\n\n* *偏斜（skewed）*与*有偏（biased）*并无关系。偏度只是描述了分布的形状。\n\n#### 偏度的计算\n对给定的值序列 $x\\_i$，样本偏度 $$g\\_1= \\frac{ \\tfrac{1}{n} \\sum{\\_i} (x\\_i-\\bar{x} )^3}{方差^3} $$\n\n偏度为负值代表分布左偏，正值代表分布右偏。$g\\_1$的大小代表偏斜的程度。\n\n#### 衡量分布对称性的另一个方法\n实际应用中，样本偏度容易受分布中离群值的影响，因此，计算样本偏度通常并非好主意。\n\n衡量分布对称性的另一个方法是**检查均值和中位数的关系**：极端值对均值的影响比对中位数影响更大\n* 在左偏分布中，均值 < 中位数\n* 在右偏分布中，均值 > 中位数\n\n##### Pearson 中位数偏度系数（Pearson's median skewness coefficient）\n$$g\\_p=3(\\bar{x} -m)/S$$\n\nPearson 中位数偏度系数更加稳健（robust）。\n\n## 第七章 变量之间的关系\n\n> 如果能够从一个变量的信息得到另一个变量的信息，那么这两个变量就是**相关的**。\n\n### 散点图\n\n研究两个变量之间关系最简单方法就是散点图（scatter plot）。\n\n#### 抖动技术（jittering）\n当获取数据由于某种原因而丢失部分信息，例如数据经过四舍五入后丢失了更高精度的小数位信息，变得“更离散”而导致散点图不美观，这时可以通过加入随机噪音（例如服从均匀分布的随机值）来填补各离散值之间的空隙。\n\n**抖动技术只是为了图像的美观，应只用于视觉呈现，而不能通过它来分析数据。**\n\n#### 饱和（saturation）\n由于数据在一定区域内过于密集，甚至出现重叠，而离群值显得特别突出。这种情况称为“饱和”。\n\n解决办法：\n将图中的点以半透明显示（例如 alpha=0.2），这样重叠的点只会导致那一片的数据点颜色更深。\n（我的理解：**以半透明显示的办法，相当于把数据正则化（Normalized）了**，原先数据密度值的域为二值情况（0 or 1），**重叠区域密度大于 1 无法在图上体现，相当于上溢出。通过规范化，将密度值域打散在（0，1）连续域内了**，重叠区域密度只要密度不超过1，就能在散点图中较好地体现出来）\n\n### 相关性（correlation）\n相关性是一个**量化两个变量之间关系强弱**的统计量。\n\n要衡量两个变量时，比较麻烦的是两个变量之间单位不同，无法直接比较。为此，我们需要将两个变量统一到一个量纲上。\n\n常见的相关系数有两种：\n- Pearson乘积矩相关系数：将变量统一为标准分数（standard score），即偏离均值的标准差数：$z\\_i = (x\\_i - \\mu)/\\sigma$。\n- Spearman秩相关系数：将变量转换为秩（rank），就是该变量在其所有值中的排名。\n\n标准分数有以下这些性质：\n- $z\\_i$是无量纲（单位）的，其分布均值为0，方差为1；\n- 如果 $X$ 服从正态分布，则 $Z$ 也服从正态分布；\n- 如果 $X$ 是偏斜的或包含离群值，则 $Z$ 也是偏斜的或包含离群值；\n\n当观察的变量 $X$ 有偏斜，或是含有离群值时，$Z$ 也会受影响。这种情况下，使用百分位秩更加稳健，因为百分位秩总是服从(1, n)的均匀分布：$R~U(1, n)$，其中 $n$ 为样本数。\n\n### 协方差（covariance）\n协方差用于**度量两个变量共同变化的趋势**。\n\n假设现在有两个序列 $X$ 和 $Y$，则两个序列中，值与均值的偏差为：\n$$\\begin{cases}\ndx\\_i = x\\_i - \\bar{x} \\\\\\\ndy\\_i = y\\_i - \\bar{y}\n\\end{cases}$$\n\n如果两个序列变化趋势一致的话，$dx\\_i$ 和 $dy\\_i$ 同号，即$dx\\_i \\cdot dy\\_i > 0$；反之二者异号，即$dx\\_i \\cdot dy\\_i < 0$。\n\n**我们将这一序列所有样本的偏差加到一块，应该就能度量两个序列之间共同变化的趋势**，于是我们就有了“协方差”的定义：\n$$\\operatorname{Cov(X,Y)} = \\frac{1}{n} \\sum dx\\_i dy\\_i$$\n注意到协方差定义中最后乘了一个 $\\frac{1}{n}$ 来正则化（$n$为序列长度，此处要求$X$和$Y$必须长度相同）。\n\n*当$X$和$Y$两个向量正交时，协方差为0*\n\n### Pearson相关系数\n协方差的单位是两个变量单位的乘积，这样会使人对它的意义感到迷惑，因而人们很少将协方差作为摘要统计量。接下来介绍的 Pearson相关系数解决了这个问题。\n\n$$\\rho = \\operatorname{Cov(X,Y)} / S\\_X S\\_Y \\tag{p}\\label{pearson}$$\n可见，Pearson相关系数加入了标准差来正则化协方差，而且是无量纲的。\n\n我们可以把$\\ref{pearson}$式中的$\\operatorname{Cov(X,Y)}$展开，就能发现：\n$$ \\rho = \\frac{1}{n} \\sum{ \\left( \\left( x\\_i-\\bar{x} \\right) / S\\_X \\right) \\left( \\left(y\\_i-\\bar{y}\\right) / S\\_Y \\right) } $$\n也就是说，Pearson相关系数在计算偏差时就将其与标准差相比较，得到一个归一化的结果：标准分数，从而实现无量纲的。\n\nPearson相关系数值的值域为$[-1, +1]$。如果$\\rho >0$则两个变量正相关；反之$\\rho <0$时，两个变量负相关。$\\rho$的大小描述了相关性的强弱程度，当$\\left\\lvert \\rho \\right\\rvert =1$时，两个变量完全相关，这时，只需要一个变量就能准确预测另一个变量。\n\n**注意，Pearson相关系数接近0时，并不能代表变量之间没有相关关系，因为Peason相关系数只度量了线性关系。如果变量之间存在非线性关系，那么用$\\rho$度量相关性就不那么准确了**。下图为一些具有非线性关系的变量的散点图，然而它们的相关系数都为0。\n\n{% asset_img nonlinear-correlation-example.png 非线性关系变量散点图及其相关系数 %}\n\n图片来源：[英文维基百科/Correlation and dependence](http://www.wikiwand.com/en/Correlation_and_dependence)\n\n### Spearman秩相关\n当变量之间呈线性相关，且变量大致符合正态分布时，Pearson相关系数能较好地说明相关性的强弱。然而离群值会影响Pearson相关系数的稳健性（回忆一下，Pearson相关系数定义中，每个样本的标准分数是相同权重加和的，因而离群值能较大程度地影响结果）\n\nSpearman相关系数不仅能描述变量的相关性，还能够缓解离群值及偏斜分布的影响。缓解离群值影响是通过计算各个值的秩(rank)实现的。如在样本[1, 2, 5, 7]中，值5出现在有序列表的第3位，因此5的值为3。\n\n*缓解离群值的常见方法就是用“排名”而非其值来计算。比如取中值（排名中位数）就比取均值面对离群点时更加稳健。*\n\n### 相关性和因果关系\n\n如果变量A和B相关，则有三种可能的解释：A导致B，或B导致A，或其他某种因素导致A和B。这些解释称为“因果关系”。\n\n只有相关性，我们无法证明因果关系。要证明因果关系，有以下三种方法：\n1. 时间\n    如果 A 在 B 之前发生，则 A 可能导致 B，而 B 不可能导致 A。\n2. 随机性\n    类似随机对照试验。\n3.回归分析\n\n# 第八章 估计\n使用样本来**估计**分布，用来估计的统计量叫做**估计量**（estimator）\n\n## 均方误差（mean squared error，MSE）\n均方误差可用来衡量估计量对分布的描述效果\n$$ MSE = \\frac{1}{m} \\sum (\\bar{x} - \\mu)^2 $$\n其中$m$为抽样次数。\n\n### 均方根误差（root mean squared error，RMSE）\n均方根误差就是均方误差的平方根：\n$$ RMSE = \\sqrt{MSE} $$\n\n用均方根误差不一定越小越好，因为估计量在实际情况中不一定会出现。最可能与实际值相符的估计叫做**最大似然估计（maximum likelihood estimator，MLE）**\n\n## 样本方差\n用样本方差作为估计量估计分布的方差是最直观的方法：\n$$S^2 = \\frac{1}{n} \\sum (x\\_i - \\bar{x})^2$$\n\n然而，$$S^2$$是偏倚（biased）估计量，对于小样本，$S^2$通常比分布的方差低很多。\n\n无偏估计量是$$S\\_{n-1}^2$$\n$$ S\\_{n-1}^2 = \\frac{1}{n-1} \\sum (x\\_i - \\bar{x})^2 $$\n其中，减去的“1”是自由度。\n\n## 抽样\n### 抽样误差\n由随机选择导致的估计变化称为**抽样误差（sampling error）**，例如，从数量庞大的大猩猩总体中抽取9只测量体重，然而却运气不佳抽到了最重的9只。\n\n### 抽样分布\n抽样若干次，对每次抽样的估计量进行统计，得到的分布叫做**抽样分布**。抽样分布是对估计量分布的描述，而不是实际的值的分布。\n\n### 抽样分布的描述\n- 标准误差（standard error，SE）\n标准误差度量估计值平均偏离实际值多少。注意标准误差描述的是**估计量**变化的情况，而标准差描述的是**度量值**变化的情况。\n- 置信区间（confidential interval，CI）\n抽样分布中指定比例的范围。例如，90%置信区间表示的是 第5个百分位数 到 第95个百分位数。\n\n### 抽样偏倚\n由于抽样过程导致的误差，叫做抽样偏倚（sampling bias）。比如，通过电话抽样统计女性体重，由于可能不能覆盖到没有电话或是不公布号码的人，实际统计的人群会有偏差。\n\n### 测量误差\n测量误差是得到的结果不准确。比如，统计女性体重时，只是询问而不测量；或是由于调查参与者“美化”自己的数字。\n\n在汇报一个估计值时，可以用标准误差或置信区间。但要记住：抽样误差只是误差来源之一，而且通常不是最大的误差来源。\n","slug":"think-statistics-note","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cipr1ji8q000x6jodxtykp6ua"}],"PostAsset":[{"_id":"source/_drafts/captcha/lsd-line-remove.png","slug":"lsd-line-remove.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/mmedian-width-threshold.png","slug":"mmedian-width-threshold.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_posts/convolution/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-2/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji8b000j6jodmuceunsg","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-3/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji8d000n6jod7olbpkcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/batch-contour.png","slug":"batch-contour.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/batch-median-contour.png","slug":"batch-median-contour.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-1/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji89000i6jod3x7nv2e6","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-4/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji8f000o6jodirzw8ey2","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-5/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji8j000s6jod3klqa123","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-2/r_less_than_n.png","slug":"r_less_than_n.png","post":"cipr1ji8b000j6jodmuceunsg","modified":0,"renderable":0},{"_id":"source/_posts/think-statistics-note/nonlinear-correlation-example.png","slug":"nonlinear-correlation-example.png","post":"cipr1ji8q000x6jodxtykp6ua","modified":0,"renderable":0},{"_id":"source/_posts/think-statistics-note/pmf-cdf-pdf.jpg","slug":"pmf-cdf-pdf.jpg","post":"cipr1ji8q000x6jodxtykp6ua","modified":0,"renderable":0},{"_id":"source/_posts/logistic-regression/Logistic-curve.png","slug":"Logistic-curve.png","post":"cipr1ji8n000t6jodij3yx69y","modified":0,"renderable":0},{"_id":"source/_posts/logistic-regression/convex-function.png","slug":"convex-function.png","post":"cipr1ji8n000t6jodij3yx69y","modified":0,"renderable":0},{"_id":"source/_posts/logistic-regression/cost-function.png","slug":"cost-function.png","post":"cipr1ji8n000t6jodij3yx69y","modified":0,"renderable":0},{"_id":"source/_posts/complex/complex-plane.png","slug":"complex-plane.png","post":"cipr1ji81000b6jodctffzok4","modified":0,"renderable":0},{"_id":"source/_posts/complex/complex-polar-plane.png","slug":"complex-polar-plane.png","post":"cipr1ji81000b6jodctffzok4","modified":0,"renderable":0},{"_id":"source/_posts/complex/complex-polar-transform.png","slug":"complex-polar-transform.png","post":"cipr1ji81000b6jodctffzok4","modified":0,"renderable":0},{"_id":"source/_posts/complex/real-wave-complex-wave.png","slug":"real-wave-complex-wave.png","post":"cipr1ji81000b6jodctffzok4","modified":0,"renderable":0},{"_id":"source/_posts/gabor/2d-gaussian.png","slug":"2d-gaussian.png","post":"cipr1ji87000f6joda7gzav9x","modified":0,"renderable":0},{"_id":"source/_posts/gabor/banner.jpeg","slug":"banner.jpeg","post":"cipr1ji87000f6joda7gzav9x","modified":0,"renderable":0},{"_id":"source/_posts/gabor/gabor-filter-banks.png","slug":"gabor-filter-banks.png","post":"cipr1ji87000f6joda7gzav9x","modified":0,"renderable":0},{"_id":"source/_posts/gabor/gabor-filter-frequency.png","slug":"gabor-filter-frequency.png","post":"cipr1ji87000f6joda7gzav9x","modified":0,"renderable":0},{"_id":"source/_posts/gabor/gabor-filter-spatial.png","slug":"gabor-filter-spatial.png","post":"cipr1ji87000f6joda7gzav9x","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-3/existence.png","slug":"existence.png","post":"cipr1ji8d000n6jod7olbpkcp","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-3/linear-transformation.png","slug":"linear-transformation.png","post":"cipr1ji8d000n6jod7olbpkcp","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-3/rotation.png","slug":"rotation.png","post":"cipr1ji8d000n6jod7olbpkcp","modified":0,"renderable":0},{"_id":"source/_posts/linear-algebra-3/uniqueness.png","slug":"uniqueness.png","post":"cipr1ji8d000n6jod7olbpkcp","modified":0,"renderable":0},{"_id":"source/_posts/convolution/2d-convolution.png","slug":"2d-convolution.png","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/convolution/banner副本.jpeg","slug":"banner副本.jpeg","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/convolution/conv-effect-day1.png","slug":"conv-effect-day1.png","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/convolution/conv-effect-day2.png","slug":"conv-effect-day2.png","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/convolution/conv-effect-day3.png","slug":"conv-effect-day3.png","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_posts/convolution/conv-effect-function.png","slug":"conv-effect-function.png","post":"cipr1ji85000d6jod293rqbis","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/binary.png","slug":"binary.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/contour.png","slug":"contour.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/edge.png","slug":"edge.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/grayscale.png","slug":"grayscale.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/hough-line.png","slug":"hough-line.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/hough.png","slug":"hough.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/kmeans.png","slug":"kmeans.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/lsd-line.png","slug":"lsd-line.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/lsd-post.png","slug":"lsd-post.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/lsd-pre.png","slug":"lsd-pre.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/median-contour.png","slug":"median-contour.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/median.png","slug":"median.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/region-grow.png","slug":"region-grow.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/sample.jpg","slug":"sample.jpg","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0},{"_id":"source/_drafts/captcha/samples.png","slug":"samples.png","post":"cipr1ji6x00016jodywpw1lcp","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cipr1ji7900036jodti4rh0jh","category_id":"cipr1ji7h00076jodzz5x0j46","_id":"cipr1ji87000e6jodl08kpg0c"},{"post_id":"cipr1ji7c00056jodsxir1upb","category_id":"cipr1ji84000c6jodnroyzfv2","_id":"cipr1ji8c000k6jod0braux2d"},{"post_id":"cipr1ji7k00086jodmeqkmfn9","category_id":"cipr1ji7h00076jodzz5x0j46","_id":"cipr1ji8i000p6jodg4nww2wp"},{"post_id":"cipr1ji7s00096jodc895cm2g","category_id":"cipr1ji8c000l6jodlpknwytw","_id":"cipr1ji8p000u6jodvg7jrbmh"},{"post_id":"cipr1ji81000b6jodctffzok4","category_id":"cipr1ji8i000q6jodpsvkvt7k","_id":"cipr1ji8s000z6jodg3vj2a4e"},{"post_id":"cipr1ji8n000t6jodij3yx69y","category_id":"cipr1ji7h00076jodzz5x0j46","_id":"cipr1ji8s00136jod15s72uuu"},{"post_id":"cipr1ji85000d6jod293rqbis","category_id":"cipr1ji8i000q6jodpsvkvt7k","_id":"cipr1ji8t00156jodm8r668sx"},{"post_id":"cipr1ji87000f6joda7gzav9x","category_id":"cipr1ji8i000q6jodpsvkvt7k","_id":"cipr1ji8w001a6jodjp4b9ji0"},{"post_id":"cipr1ji89000i6jod3x7nv2e6","category_id":"cipr1ji8u00176jodlrryuoeu","_id":"cipr1ji8y001f6jod77t3x0j2"},{"post_id":"cipr1ji8b000j6jodmuceunsg","category_id":"cipr1ji8u00176jodlrryuoeu","_id":"cipr1ji8z001i6jod1zo0m27t"},{"post_id":"cipr1ji8d000n6jod7olbpkcp","category_id":"cipr1ji8u00176jodlrryuoeu","_id":"cipr1ji92001m6jod9sd7bmil"},{"post_id":"cipr1ji8f000o6jodirzw8ey2","category_id":"cipr1ji8u00176jodlrryuoeu","_id":"cipr1ji94001r6jodbwom7tv6"},{"post_id":"cipr1ji8j000s6jod3klqa123","category_id":"cipr1ji8u00176jodlrryuoeu","_id":"cipr1ji95001u6jodoe39tqp6"},{"post_id":"cipr1ji8q000x6jodxtykp6ua","category_id":"cipr1ji94001s6jod62i5s9vd","_id":"cipr1ji96001y6jodhi6d4kbm"}],"PostTag":[{"post_id":"cipr1ji7900036jodti4rh0jh","tag_id":"cipr1ji7g00066jodp38i0jfp","_id":"cipr1ji8r000y6jodacigeeff"},{"post_id":"cipr1ji7900036jodti4rh0jh","tag_id":"cipr1ji80000a6jod63adp2qz","_id":"cipr1ji8s00106jodsum3ugxg"},{"post_id":"cipr1ji7900036jodti4rh0jh","tag_id":"cipr1ji88000g6jodlx8ixzl7","_id":"cipr1ji8s00146jod66q04wpz"},{"post_id":"cipr1ji7900036jodti4rh0jh","tag_id":"cipr1ji8c000m6jods50taans","_id":"cipr1ji8u00166jodk9nst8ey"},{"post_id":"cipr1ji7900036jodti4rh0jh","tag_id":"cipr1ji8i000r6jodw0bbb8p6","_id":"cipr1ji8v00196jodwztkpql9"},{"post_id":"cipr1ji7c00056jodsxir1upb","tag_id":"cipr1ji8p000w6jod2uois2vq","_id":"cipr1ji8w001b6jodbmgszio7"},{"post_id":"cipr1ji7c00056jodsxir1upb","tag_id":"cipr1ji8s00126jodhnssq9cs","_id":"cipr1ji8x001e6jodqeqq11qj"},{"post_id":"cipr1ji7k00086jodmeqkmfn9","tag_id":"cipr1ji7g00066jodp38i0jfp","_id":"cipr1ji92001l6jod60t6uxbk"},{"post_id":"cipr1ji7k00086jodmeqkmfn9","tag_id":"cipr1ji8w001d6jodrqjv1wmv","_id":"cipr1ji93001o6jodq5176mn2"},{"post_id":"cipr1ji7k00086jodmeqkmfn9","tag_id":"cipr1ji8y001h6jod941yk18j","_id":"cipr1ji93001q6jodi9sgjqek"},{"post_id":"cipr1ji7s00096jodc895cm2g","tag_id":"cipr1ji90001k6jodg8ilelx6","_id":"cipr1ji95001v6jodjr6qkm1l"},{"post_id":"cipr1ji7s00096jodc895cm2g","tag_id":"cipr1ji93001p6jodrprq3x1b","_id":"cipr1ji95001w6jodgkpie26h"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji94001t6jode0tfr72w","_id":"cipr1ji9800256jodqx4d69m1"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji96001x6jodwj75g97v","_id":"cipr1ji9800266jod1479m9py"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji96001z6jodvsc4gfi0","_id":"cipr1ji9900286jod4i7kzt1x"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji9700206jodv8fg1uqy","_id":"cipr1ji9900296jodw00bvr3j"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji9700216jod9t1uuu5y","_id":"cipr1ji9a002b6jod7hyzhvnw"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9a002c6jod7t0t845r"},{"post_id":"cipr1ji81000b6jodctffzok4","tag_id":"cipr1ji9800236jodnzk1qpdi","_id":"cipr1ji9b002e6joduxk2o9yy"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1ji9d002h6jodyfmxa1pa"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji9800276jodt15kaib4","_id":"cipr1ji9d002i6jodydmmzmsa"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji9700216jod9t1uuu5y","_id":"cipr1ji9d002k6jod3czivaha"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji7g00066jodp38i0jfp","_id":"cipr1ji9d002l6jodo37d84sl"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9d002n6jodkyfz63qx"},{"post_id":"cipr1ji85000d6jod293rqbis","tag_id":"cipr1ji9800236jodnzk1qpdi","_id":"cipr1ji9d002o6jodgb1sxlma"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji9700216jod9t1uuu5y","_id":"cipr1ji9g002s6jodc5agkj9i"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji7g00066jodp38i0jfp","_id":"cipr1ji9g002t6jodd6jjtdtg"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji9d002j6jodqe3f37jk","_id":"cipr1ji9g002v6jodqodbqtx4"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji94001t6jode0tfr72w","_id":"cipr1ji9g002w6jod2u7k63nv"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji9e002p6jody0kavb2q","_id":"cipr1ji9g002y6jodsssd7nj3"},{"post_id":"cipr1ji87000f6joda7gzav9x","tag_id":"cipr1ji9800236jodnzk1qpdi","_id":"cipr1ji9g002z6jodsovr11pf"},{"post_id":"cipr1ji89000i6jod3x7nv2e6","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1ji9h00326jodg9e8rl6u"},{"post_id":"cipr1ji89000i6jod3x7nv2e6","tag_id":"cipr1ji9g002u6jodo2qksqh0","_id":"cipr1ji9i00336jodhv6coi4h"},{"post_id":"cipr1ji89000i6jod3x7nv2e6","tag_id":"cipr1ji9g002x6jodx2ef4lq0","_id":"cipr1ji9i00356jod5su7ijd0"},{"post_id":"cipr1ji89000i6jod3x7nv2e6","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9i00366jodihh2uf6b"},{"post_id":"cipr1ji8b000j6jodmuceunsg","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1ji9l003a6jod8kzzph3q"},{"post_id":"cipr1ji8b000j6jodmuceunsg","tag_id":"cipr1ji9g002u6jodo2qksqh0","_id":"cipr1ji9l003b6jodsuok536e"},{"post_id":"cipr1ji8b000j6jodmuceunsg","tag_id":"cipr1ji9g002x6jodx2ef4lq0","_id":"cipr1ji9n003d6jodda9sej7x"},{"post_id":"cipr1ji8b000j6jodmuceunsg","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9n003e6jod82sy7tnd"},{"post_id":"cipr1ji8d000n6jod7olbpkcp","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1ji9p003i6jodhne8pnan"},{"post_id":"cipr1ji8d000n6jod7olbpkcp","tag_id":"cipr1ji9g002x6jodx2ef4lq0","_id":"cipr1ji9p003j6jod561i3emx"},{"post_id":"cipr1ji8d000n6jod7olbpkcp","tag_id":"cipr1ji9g002u6jodo2qksqh0","_id":"cipr1ji9p003l6jodfvgdbi1a"},{"post_id":"cipr1ji8d000n6jod7olbpkcp","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9q003m6jodkujjagt6"},{"post_id":"cipr1ji8f000o6jodirzw8ey2","tag_id":"cipr1ji9g002x6jodx2ef4lq0","_id":"cipr1ji9w003q6jod0cxe5j31"},{"post_id":"cipr1ji8f000o6jodirzw8ey2","tag_id":"cipr1ji9g002u6jodo2qksqh0","_id":"cipr1ji9w003r6jodx7krw9tq"},{"post_id":"cipr1ji8f000o6jodirzw8ey2","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1ji9y003t6jodfsidoedr"},{"post_id":"cipr1ji8f000o6jodirzw8ey2","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1ji9y003u6jodn37mpo8b"},{"post_id":"cipr1ji8j000s6jod3klqa123","tag_id":"cipr1ji9g002x6jodx2ef4lq0","_id":"cipr1ji9z003y6jod905ofemt"},{"post_id":"cipr1ji8j000s6jod3klqa123","tag_id":"cipr1ji9g002u6jodo2qksqh0","_id":"cipr1jia0003z6jodyopd67rz"},{"post_id":"cipr1ji8j000s6jod3klqa123","tag_id":"cipr1ji9800246jodxfe9h38r","_id":"cipr1jia000416jodcmduoud7"},{"post_id":"cipr1ji8j000s6jod3klqa123","tag_id":"cipr1ji9700226jodotfw7zal","_id":"cipr1jia000426jod6o5p895h"},{"post_id":"cipr1ji8n000t6jodij3yx69y","tag_id":"cipr1ji7g00066jodp38i0jfp","_id":"cipr1jia000446jodxlmqab76"},{"post_id":"cipr1ji8n000t6jodij3yx69y","tag_id":"cipr1ji9z003x6jodhrsqt7bj","_id":"cipr1jia000456jods80p7nyj"},{"post_id":"cipr1ji8n000t6jodij3yx69y","tag_id":"cipr1jia000406jod5s29jxuy","_id":"cipr1jia900466jodxcx071bc"},{"post_id":"cipr1ji8q000x6jodxtykp6ua","tag_id":"cipr1jia000436jodnpwioejg","_id":"cipr1jia900476jodpo6fbv5m"},{"post_id":"cipr1ji8q000x6jodxtykp6ua","tag_id":"cipr1ji88000g6jodlx8ixzl7","_id":"cipr1jia900486jodsl39izon"}],"Tag":[{"name":"机器学习","_id":"cipr1ji7g00066jodp38i0jfp"},{"name":"统计学习","_id":"cipr1ji80000a6jod63adp2qz"},{"name":"读书笔记","_id":"cipr1ji88000g6jodlx8ixzl7"},{"name":"统计学习基础","_id":"cipr1ji8c000m6jods50taans"},{"name":"ESL","_id":"cipr1ji8i000r6jodw0bbb8p6"},{"name":"编程","_id":"cipr1ji8p000w6jod2uois2vq"},{"name":"重构","_id":"cipr1ji8s00126jodhnssq9cs"},{"name":"Machine Learning","_id":"cipr1ji8w001d6jodrqjv1wmv"},{"name":"个人思考","_id":"cipr1ji8y001h6jod941yk18j"},{"name":"随笔","_id":"cipr1ji90001k6jodg8ilelx6"},{"name":"机器学习感悟","_id":"cipr1ji93001p6jodrprq3x1b"},{"name":"傅里叶变换","_id":"cipr1ji94001t6jode0tfr72w"},{"name":"欧拉公式","_id":"cipr1ji96001x6jodwj75g97v"},{"name":"复数","_id":"cipr1ji96001z6jodvsc4gfi0"},{"name":"数字信号处理","_id":"cipr1ji9700206jodv8fg1uqy"},{"name":"图像处理","_id":"cipr1ji9700216jod9t1uuu5y"},{"name":"基础概念","_id":"cipr1ji9700226jodotfw7zal"},{"name":"Gabor 特征","_id":"cipr1ji9800236jodnzk1qpdi"},{"name":"数学","_id":"cipr1ji9800246jodxfe9h38r"},{"name":"卷积","_id":"cipr1ji9800276jodt15kaib4"},{"name":"特征提取","_id":"cipr1ji9d002j6jodqe3f37jk"},{"name":"Gabor","_id":"cipr1ji9e002p6jody0kavb2q"},{"name":"线性代数","_id":"cipr1ji9g002u6jodo2qksqh0"},{"name":"线性代数拾遗","_id":"cipr1ji9g002x6jodx2ef4lq0"},{"name":"Logistic 回归","_id":"cipr1ji9z003x6jodhrsqt7bj"},{"name":"Logistic Regression","_id":"cipr1jia000406jod5s29jxuy"},{"name":"统计","_id":"cipr1jia000436jodnpwioejg"}]}}