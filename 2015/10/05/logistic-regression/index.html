<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/apple-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/favicon-16x16.png"><link rel="mask-icon" href="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/favicon-16x16.png" color="#222"><link rel="manifest" href="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/manifest.json"><meta name="msapplication-config" content="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/favicon/browserconfig.xml"><meta name="google-site-verification" content="4Fw5av3MAK9VoBQQanNGeiDwK9jEJjQBL6jk3PSY-B4"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"mengqi92.github.io",root:"/",scheme:"Mist",version:"7.8.0",exturl:!0,sidebar:{position:"right",display:"hide",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：一、问题的引入使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 $h\theta (x)$ 来决定类别归属。例如：$h\theta (x) &amp;lt; 0.5$ 时，y &#x3D; 0；$h_\theta (x) &amp;gt; 0."><meta name="keywords" content="机器学习,Logistic 回归,Logistic Regression"><meta property="og:type" content="article"><meta property="og:title" content="Logistic 回归"><meta property="og:url" content="http:&#x2F;&#x2F;mengqi92.github.io&#x2F;2015&#x2F;10&#x2F;05&#x2F;logistic-regression&#x2F;index.html"><meta property="og:site_name" content="Mengqi&#39;s blog"><meta property="og:description" content="最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：一、问题的引入使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 $h\theta (x)$ 来决定类别归属。例如：$h\theta (x) &amp;lt; 0.5$ 时，y &#x3D; 0；$h_\theta (x) &amp;gt; 0."><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;blog.mengqi.life&#x2F;images&#x2F;logistic-regression&#x2F;Logistic-curve.png"><meta property="og:image" content="https:&#x2F;&#x2F;blog.mengqi.life&#x2F;images&#x2F;logistic-regression&#x2F;convex-function.png"><meta property="og:image" content="https:&#x2F;&#x2F;blog.mengqi.life&#x2F;images&#x2F;logistic-regression&#x2F;cost-function.png"><meta property="og:updated_time" content="2024-01-25T11:24:24.637Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;blog.mengqi.life&#x2F;images&#x2F;logistic-regression&#x2F;Logistic-curve.png"><link rel="canonical" href="http://mengqi92.github.io/2015/10/05/logistic-regression/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Logistic 回归 | Mengqi's blog</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-68396368-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-68396368-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8800f71adfa983d80f6c943ceb1ee330";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Mengqi's blog" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Mengqi's blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">a paranoid android.</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a></li><li class="menu-item menu-item-quotes"><a href="/analects/" rel="section"><i class="fa fa-scroll fa-fw"></i>古文乱选</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://mengqi92.github.io/2015/10/05/logistic-regression/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/astronaut.png"><meta itemprop="name" content="Mengqi"><meta itemprop="description" content="Happiness only real when shared."></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Mengqi's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Logistic 回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2015-10-05 14:50:00" itemprop="dateCreated datePublished" datetime="2015-10-05T14:50:00+00:00">2015-10-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2024-01-25 11:24:24" itemprop="dateModified" datetime="2024-01-25T11:24:24+00:00">2024-01-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：</p><h2 id="一、问题的引入"><a href="#一、问题的引入" class="headerlink" title="一、问题的引入"></a>一、问题的引入</h2><p>使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 $h<em>\theta (x)$ 来决定类别归属。例如：$h</em>\theta (x) &lt; 0.5$ 时，y = 0；$h_\theta (x) &gt; 0.5$ 时，y = 1。</p><p>然而，线性回归得到的假设值 $h_\theta (x)$ 有可能 &gt;1 或是 &lt;0，而且有可能会超出很多，这种情况下使用线性回归似乎不是很好的选择。</p><p>为了解决这个问题，我们引入 Logistic 回归方法，将 $h_\theta (x)$ 限制在 (0,1) 范围内。</p><p>注意，Logistic 回归是一种<strong>分类</strong>方法，而不是回归方法，名字中的「回归」是历史原因造成的。</p><a id="more"></a><h2 id="二、Logistic-函数（Logistic-Function）"><a href="#二、Logistic-函数（Logistic-Function）" class="headerlink" title="二、Logistic 函数（Logistic Function）"></a>二、Logistic 函数（Logistic Function）</h2><p>线性回归中，假设函数 $h_\theta (x)=\theta ^\top x$，这里将截距”藏”在了向量中，即$\theta=[\theta_0, \theta_1, \cdots, \theta_n]^\top$，$x=[1, x_1, x_2, \cdots, x_n]^\top$。</p><p>而在 Logistic 回归中，我们使用一个函数来<strong>限制假设函数的值域</strong>，这个函数就叫做 Logistic 函数（Logistic Function，也叫 Sigmoid Function）。</p><p>Logistic Function：</p><script type="math/tex;mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>它的函数图像：<br><img data-src="https://blog.mengqi.life/images/logistic-regression/Logistic-curve.png" title="逻辑回归函数图像"></p><p>从图像中可以看出，逻辑回归函数将输入的$(-\infty, \infty)$空间映射到了$(0,1)$空间，即将值域限制在了$(0,1)$之内。 限制后的假设函数为：</p><script type="math/tex;mode=display">h_\theta (x)=g(\theta ^\top x)=\frac{1}{1+e^{-\theta ^\top x}}</script><p>注意该假设函数中，只有一个参数：$\theta$，我们接下来就需要通过优化来求解这个参数，以确定分类模型。</p><h2 id="三、假设函数的直观解释"><a href="#三、假设函数的直观解释" class="headerlink" title="三、假设函数的直观解释"></a>三、假设函数的直观解释</h2><p>由于假设函数的值域为$(0,1)$，而$h<em>\theta (x)$值越接近1，就越有可能是 y=1 类；反之$h</em>\theta (x)$值越接近0，越有可能是 y=0 类。</p><p>这样看来，假设函数 $h_\theta (x)$ 可以看做是给定 x，其类别 y=1 的估计概率，即</p><script type="math/tex;mode=display">h_\theta (x)=P(y=1 \mid x;\theta )</script><h2 id="四、寻求优化参数-theta"><a href="#四、寻求优化参数-theta" class="headerlink" title="四、寻求优化参数 $\theta$"></a>四、寻求优化参数 $\theta$</h2><p>一般来说，寻找参数的过程就是优化目标函数的过程。</p><h3 id="4-1-线性回归的目标函数"><a href="#4-1-线性回归的目标函数" class="headerlink" title="4.1 线性回归的目标函数"></a>4.1 线性回归的目标函数</h3><p>在线性回归中，我们的目标函数为：</p><script type="math/tex;mode=display">J(\theta )=\frac{1}{m} \sum_{i=1}^m \frac{1}{2} (h_\theta (x^{(i)})-y^{(i)})^2</script><p>其中，$\frac{1}{2} (h<em>\theta (x^{(i)})-y^{(i)})^2$ 部分就是损失函数，即$Cost(h</em>\theta (x^{(i)}), y^{(i)})$</p><p>线性回归的优化目标就是最小化这个目标函数，即让各个样本点的误差达到最小。</p><h3 id="4-2-Logistic-回归的目标函数"><a href="#4-2-Logistic-回归的目标函数" class="headerlink" title="4.2 Logistic 回归的目标函数"></a>4.2 Logistic 回归的目标函数</h3><h4 id="4-2-1-平方形式的损失函数"><a href="#4-2-1-平方形式的损失函数" class="headerlink" title="4.2.1 平方形式的损失函数"></a>4.2.1 平方形式的损失函数</h4><p>我们把 Logistic 回归的假设函数 $h_\theta (x)=\frac{1}{1+e^{-\theta ^\top x}}$ 代入到线性回归的损失函数中，得到：</p><script type="math/tex;mode=display">Cost(h_\theta (x), y) = \frac{1}{2} (h_\theta (x)-y)^2 = \frac{1}{1+e^{-\theta ^\top x}}</script><p>为简便起见，这里以后，将各个点的误差$h<em>\theta (x^{(i)})-y^{(i)}$简写为$h</em>\theta (x)-y$。</p><p>然而，这样的损失函数代入$J(\theta )=\frac{1}{m} \sum\limits<em>{i=1}^m Cost(h</em>\theta x, y)$ 中，得到的目标函数 $J(\theta )$ 并非凸函数，其函数图像类似下图的左子图。</p><img data-src="https://blog.mengqi.life/images/logistic-regression/convex-function.png" title="非凸函数和凸函数"><p>只有目标函数是凸函数时，我们才能通过各种优化方法（如梯度下降、牛顿法等）找到极值点，进而得到最优值对应的参数。 因此，Logistic 回归需要调整其损失函数形式，以使得目标函数为凸函数。</p><h4 id="4-2-2-对数形式的损失函数"><a href="#4-2-2-对数形式的损失函数" class="headerlink" title="4.2.2 对数形式的损失函数"></a>4.2.2 对数形式的损失函数</h4><p>Logistic 回归采用的损失函数为：</p><script type="math/tex;mode=display">Cost(h_\theta (x), y)=
\begin{cases} -log(h_\theta (x)) &\text{if y=1} \\
-log(1-h_\theta (x)) &\text{if y=0} \end{cases}</script><p>这两个函数 $-log(h<em>\theta (x))$，$-log(1-h</em>\theta (x))$ 的函数图像如下图所示。可以看出，当 y=1 时，随着 $h<em>\theta (x)$ 逐渐趋近于 0（即趋向于「分错类别」），损失函数将剧烈上升，趋向于 $\infty$，而当 $h</em>\theta (x)$ 逐渐趋近于 1（即趋向于「分对类别」） 时，损失函数则会逐渐减小到 0。当 y=0 时，情况类似。</p><p>可见，当分错类别时，这个损失函数会得到一个比较大的损失，进而来惩罚分类算法。</p><img data-src="https://blog.mengqi.life/images/logistic-regression/cost-function.png" title="损失函数"><h5 id="简化损失函数"><a href="#简化损失函数" class="headerlink" title="简化损失函数"></a>简化损失函数</h5><p>上面对数形式损失函数是分段形式的，我们可以将两个式子压缩成一个式子：</p><script type="math/tex;mode=display">Cost(h_\theta (x), y) = -ylog(h_\theta (x)) -(1-y)log(1-h_\theta (x))</script><p>当 y=0 时，取后半段；当 y=1 时，取前半段。</p><p>由此，我们终于得到了 Logistic 回归的目标函数$J(\theta)$：</p><script type="math/tex;mode=display">\begin{aligned} J(\theta) & = \frac{1}{m} \sum_{i=1}^m Cost(h_\theta(x^{(i)}), y^{(i)}) \\
& = -\frac{1}{m} [\sum_{i=1}^m y^{(i)}log h_\theta(x^{(i)}) + (1-y^{(i)})log (1-h_\theta (x^{(i)}))] \\\ \end{aligned}</script><h3 id="4-3-优化目标函数求参"><a href="#4-3-优化目标函数求参" class="headerlink" title="4.3 优化目标函数求参"></a>4.3 优化目标函数求参</h3><p>优化目标函数：$\min_{\theta} J(\theta)$，即可得到参数 $\theta$</p><p>那么，如何优化目标函数呢？优化方法有很多种，这里讲一下「梯度下降法」：</p><h4 id="4-3-1-梯度下降法（Gradient-Descent）"><a href="#4-3-1-梯度下降法（Gradient-Descent）" class="headerlink" title="4.3.1 梯度下降法（Gradient Descent）"></a>4.3.1 梯度下降法（Gradient Descent）</h4><p>梯度下降法的原理这里不详细解释了，方法比较直观，网上有很多教程可以参考。</p><p>梯度下降法的使用很简单：在目标函数上任找一点开始，让参数 $\theta$ 不断朝着梯度方向迭代，直到收敛，收敛时函数位于极小值处，此时的 $\theta$ 即为 $\min_{\theta} J(\theta)$。</p><p>每一步迭代的形式化定义如下：</p><script type="math/tex;mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)</script><p>这里引入了一个新的参数：$\alpha$，表示迭代速度，在这里作为调控因子。另外式子中 $J(\theta)$ 关于 $\theta$ 的梯度可以计算得到：</p><script type="math/tex;mode=display">\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^m(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>此外，还要注意的是，梯度下降法迭代时，是所有参数：$\theta_0, \theta_1, \cdots, \theta_n$ 同时迭代的，这个可以以向量形式进行批量计算。</p><p>在梯度下降中，需要计算$\sum<em>{i=1}^m (h</em>\theta (x^{(i)}) - y^{(i)})x_j^{(i)}$，也就是每一个样本 $x^{(i)}$ 都要参与计算。这样在样本量较大时，难免效率底下。有一些改进的方法来解决这个问题，例如随机梯度下降法等，这里就不展开了。</p><h2 id="五、用求得的参数进行分类"><a href="#五、用求得的参数进行分类" class="headerlink" title="五、用求得的参数进行分类"></a>五、用求得的参数进行分类</h2><p>使用求得的参数 $\theta$，进而预测新的未知变量 $x$：</p><script type="math/tex;mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta ^\top x}}</script><p>之前提过了，这个 $h<em>\theta(x)$ 直观意义为：给定 x，其类别 y=1 的估计概率，即$h</em>\theta (x)=P(y=1 \mid x;\theta )$ 因此，我们有了 $h_\theta(x)$，就能确定未知样本的分类了。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Mengqi</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="/http:/mengqi92.github.io/2015/10/05/logistic-regression/" title="Logistic 回归">http://mengqi92.github.io/2015/10/05/logistic-regression/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/Logistic-%E5%9B%9E%E5%BD%92/" rel="tag"># Logistic 回归</a> <a href="/tags/Logistic-Regression/" rel="tag"># Logistic Regression</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2015/10/03/think-statistics-note/" rel="prev" title="《统计思维：程序员数学之概率统计》读书摘录"><i class="fa fa-chevron-left"></i> 《统计思维：程序员数学之概率统计》读书摘录</a></div><div class="post-nav-item"><a href="/2015/10/06/complex/" rel="next" title="复数的几种表示形式">复数的几种表示形式 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、问题的引入"><span class="nav-number">1.</span> <span class="nav-text">一、问题的引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、Logistic-函数（Logistic-Function）"><span class="nav-number">2.</span> <span class="nav-text">二、Logistic 函数（Logistic Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、假设函数的直观解释"><span class="nav-number">3.</span> <span class="nav-text">三、假设函数的直观解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、寻求优化参数-theta"><span class="nav-number">4.</span> <span class="nav-text">四、寻求优化参数 $\theta$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-线性回归的目标函数"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 线性回归的目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Logistic-回归的目标函数"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Logistic 回归的目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-平方形式的损失函数"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 平方形式的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-对数形式的损失函数"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 对数形式的损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#简化损失函数"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">简化损失函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-优化目标函数求参"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 优化目标函数求参</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-梯度下降法（Gradient-Descent）"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.3.1 梯度下降法（Gradient Descent）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、用求得的参数进行分类"><span class="nav-number">5.</span> <span class="nav-text">五、用求得的参数进行分类</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Mengqi" src="https://d1aeqfcn0xwy4s.cloudfront.net/avatar/astronaut.png"><p class="site-author-name" itemprop="name">Mengqi</p><div class="site-description" itemprop="description">Happiness only real when shared.</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbmdxaTky" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;mengqi92"><i class="fab fa-github fa-fw"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOm1lbmdxaXBlaUBnbWFpbC5jb20=" title="E-Mail → mailto:mengqipei@gmail.com"><i class="fa fa-envelope fa-fw"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9tZW5ncWlwZWk=" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;mengqipei"><i class="fab fa-twitter fa-fw"></i></span> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><span class="exturl" data-url="aHR0cHM6Ly9jdGV4dC5vcmcvemhz" title="https:&#x2F;&#x2F;ctext.org&#x2F;zhs">中国哲学书电子化计划</span></li><li class="links-of-blogroll-item"><span class="exturl" data-url="aHR0cDovL3d3dy5mdXppaGFvLm9yZw==" title="http:&#x2F;&#x2F;www.fuzihao.org">切问录</span></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Mengqi</span></div><div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly9taXN0LnRoZW1lLW5leHQub3Jn">NexT.Mist</span> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css"><script src="//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css"></body></html>