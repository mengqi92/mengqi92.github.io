<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/apple-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/favicon-16x16.png"><link rel="mask-icon" href="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/favicon-16x16.png" color="#222"><link rel="manifest" href="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/manifest.json"><meta name="msapplication-config" content="https://mengqistatic.azureedge.net/staticfiles/avatar/favicon/browserconfig.xml"><meta name="google-site-verification" content="4Fw5av3MAK9VoBQQanNGeiDwK9jEJjQBL6jk3PSY-B4"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://mengqi92.github.io").hostname,root:"/",scheme:"Mist",version:"7.5.0",exturl:!0,sidebar:{position:"right",display:"hide",offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!0,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},sidebarPadding:40}</script><meta name="description" content="最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：一、问题的引入使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 \(h_\theta (x)\) 来决定类别归属。例如：\(h_\theta (x) &amp;lt; 0.5\) 时，y &#x3D; 0；\(h_\theta (x)"><meta name="keywords" content="机器学习,Logistic 回归,Logistic Regression"><meta property="og:type" content="article"><meta property="og:title" content="Logistic 回归"><meta property="og:url" content="http:&#x2F;&#x2F;mengqi92.github.io&#x2F;2015&#x2F;10&#x2F;05&#x2F;logistic-regression&#x2F;index.html"><meta property="og:site_name" content="Mengqi&#39;s blog"><meta property="og:description" content="最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：一、问题的引入使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 \(h_\theta (x)\) 来决定类别归属。例如：\(h_\theta (x) &amp;lt; 0.5\) 时，y &#x3D; 0；\(h_\theta (x)"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;mengqistatic.azureedge.net&#x2F;staticfiles&#x2F;logistic-regression&#x2F;Logistic-curve.png"><meta property="og:image" content="https:&#x2F;&#x2F;mengqistatic.azureedge.net&#x2F;staticfiles&#x2F;logistic-regression&#x2F;convex-function.png"><meta property="og:image" content="https:&#x2F;&#x2F;mengqistatic.azureedge.net&#x2F;staticfiles&#x2F;logistic-regression&#x2F;cost-function.png"><meta property="og:updated_time" content="2020-07-18T04:42:41.196Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;mengqistatic.azureedge.net&#x2F;staticfiles&#x2F;logistic-regression&#x2F;Logistic-curve.png"><link rel="canonical" href="http://mengqi92.github.io/2015/10/05/logistic-regression/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>Logistic 回归 | Mengqi's blog</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-68396368-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-68396368-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8800f71adfa983d80f6c943ceb1ee330";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Mengqi's blog" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Mengqi's blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">a paranoid android.</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://mengqi92.github.io/2015/10/05/logistic-regression/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://mengqistatic.azureedge.net/staticfiles/avatar/astronaut.png"><meta itemprop="name" content="Mengqi"><meta itemprop="description" content="机器学习、图像处理、统计分析、函数式编程"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Mengqi's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Logistic 回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2015-10-05 14:50:00" itemprop="dateCreated datePublished" datetime="2015-10-05T14:50:00+00:00">2015-10-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-07-18 04:42:41" itemprop="dateModified" datetime="2020-07-18T04:42:41+00:00">2020-07-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span> </a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近项目需要，用到了 Logistic 回归（Logistic Regression），因此又跟着 Andrew Ng 的机器学习课程复习了一遍相关知识，整理如下：</p><h2 id="一问题的引入">一、问题的引入</h2><p>使用线性回归方法是可以引申来处理分类问题的，一般是用回归得到假设值 <span class="math inline">\(h_\theta (x)\)</span> 来决定类别归属。例如：<span class="math inline">\(h_\theta (x) &lt; 0.5\)</span> 时，y = 0；<span class="math inline">\(h_\theta (x) &gt; 0.5\)</span> 时，y = 1。</p><p>然而，线性回归得到的假设值 <span class="math inline">\(h_\theta (x)\)</span> 有可能 &gt;1 或是 &lt;0，而且有可能会超出很多，这种情况下使用线性回归似乎不是很好的选择。</p><p>为了解决这个问题，我们引入 Logistic 回归方法，将 <span class="math inline">\(h_\theta (x)\)</span> 限制在 (0,1) 范围内。</p><p>注意，Logistic 回归是一种<strong>分类</strong>方法，而不是回归方法，名字中的“回归”是历史原因造成的。</p><a id="more"></a><h2 id="二logistic-函数logistic-function">二、Logistic 函数（Logistic Function）</h2><p>线性回归中，假设函数 <span class="math inline">\(h_\theta (x)=\theta ^\top x\)</span>，这里将截距"藏"在了向量中，即<span class="math inline">\(\theta=[\theta_0, \theta_1, \cdots, \theta_n]^\top\)</span>，<span class="math inline">\(x=[1, x_1, x_2, \cdots, x_n]^\top\)</span>。</p><p>而在 Logistic 回归中，我们使用一个函数来<strong>限制假设函数的值域</strong>，这个函数就叫做 Logistic 函数（Logistic Function，也叫 Sigmoid Function）。</p><p>Logistic Function：<span class="math display">\[g(z)=\frac{1}{1+e^{-z}}\]</span></p><p>它的函数图像： <img title="逻辑回归函数图像" data-src="https://mengqistatic.azureedge.net/staticfiles/logistic-regression/Logistic-curve.png"></p><p>从图像中可以看出，逻辑回归函数将输入的<span class="math inline">\((-\infty, \infty)\)</span>空间映射到了<span class="math inline">\((0,1)\)</span>空间，即将值域限制在了<span class="math inline">\((0,1)\)</span>之内。 限制后的假设函数为：</p><p><span class="math display">\[h_\theta (x)=g(\theta ^\top x)=\frac{1}{1+e^{-\theta ^\top x}}\]</span></p><p>注意该假设函数中，只有一个参数：<span class="math inline">\(\theta\)</span>，我们接下来就需要通过优化来求解这个参数，以确定分类模型。</p><h2 id="三假设函数的直观解释">三、假设函数的直观解释</h2><p>由于假设函数的值域为<span class="math inline">\((0,1)\)</span>，而<span class="math inline">\(h_\theta (x)\)</span>值越接近1，就越有可能是 y=1 类；反之<span class="math inline">\(h_\theta (x)\)</span>值越接近0，越有可能是 y=0 类。</p><p>这样看来，假设函数 <span class="math inline">\(h_\theta (x)\)</span> 可以看做是给定 x，其类别 y=1 的估计概率，即</p><p><span class="math display">\[h_\theta (x)=P(y=1 \mid x;\theta )\]</span></p><h2 id="四寻求优化参数-theta">四、寻求优化参数 <span class="math inline">\(\theta\)</span></h2><p>一般来说，寻找参数的过程就是优化目标函数的过程。</p><h3 id="线性回归的目标函数">4.1 线性回归的目标函数</h3><p>在线性回归中，我们的目标函数为： <span class="math display">\[J(\theta )=\frac{1}{m} \sum_{i=1}^m \frac{1}{2} (h_\theta (x^{(i)})-y^{(i)})^2\]</span> 其中，<span class="math inline">\(\frac{1}{2} (h_\theta (x^{(i)})-y^{(i)})^2\)</span> 部分就是损失函数，即<span class="math inline">\(Cost(h_\theta (x^{(i)}), y^{(i)})\)</span></p><p>线性回归的优化目标就是最小化这个目标函数，即让各个样本点的误差达到最小。</p><h3 id="logistic-回归的目标函数">4.2 Logistic 回归的目标函数</h3><h4 id="平方形式的损失函数">4.2.1 平方形式的损失函数</h4><p>我们把 Logistic 回归的假设函数 <span class="math inline">\(h_\theta (x)=\frac{1}{1+e^{-\theta ^\top x}}\)</span> 代入到线性回归的损失函数中，得到：</p><p><span class="math display">\[Cost(h_\theta (x), y) = \frac{1}{2} (h_\theta (x)-y)^2 = \frac{1}{1+e^{-\theta ^\top x}}\]</span></p><p>为简便起见，这里以后，将各个点的误差<span class="math inline">\(h_\theta (x^{(i)})-y^{(i)}\)</span>简写为<span class="math inline">\(h_\theta (x)-y\)</span>。</p><p>然而，这样的损失函数代入<span class="math inline">\(J(\theta )=\frac{1}{m} \sum\limits_{i=1}^m Cost(h_\theta x, y)\)</span> 中，得到的目标函数 <span class="math inline">\(J(\theta )\)</span> 并非凸函数，其函数图像类似下图的左子图。</p><img title="非凸函数和凸函数" data-src="https://mengqistatic.azureedge.net/staticfiles/logistic-regression/convex-function.png"><p>只有目标函数是凸函数时，我们才能通过各种优化方法（如梯度下降、牛顿法等）找到极值点，进而得到最优值对应的参数。 因此，Logistic 回归需要调整其损失函数形式，以使得目标函数为凸函数。</p><h4 id="对数形式的损失函数">4.2.2 对数形式的损失函数</h4><p>Logistic 回归采用的损失函数为： <span class="math display">\[Cost(h_\theta (x), y)= \begin{cases} -log(h_\theta (x)) &amp;\text{if y=1} \\\ -log(1-h_\theta (x)) &amp;\text{if y=0} \end{cases}\]</span></p><p>这两个函数 <span class="math inline">\(-log(h_\theta (x))\)</span>，<span class="math inline">\(-log(1-h_\theta (x))\)</span> 的函数图像如下图所示。可以看出，当 y=1 时，随着 <span class="math inline">\(h_\theta (x)\)</span> 逐渐趋近于 0（即趋向于“分错类别”），损失函数将剧烈上升，趋向于 <span class="math inline">\(\infty\)</span>，而当 <span class="math inline">\(h_\theta (x)\)</span> 逐渐趋近于 1（即趋向于“分对类别”） 时，损失函数则会逐渐减小到 0。当 y=0 时，情况类似。</p><p>可见，当分错类别时，这个损失函数会得到一个比较大的损失，进而来惩罚分类算法。</p><img title="损失函数" data-src="https://mengqistatic.azureedge.net/staticfiles/logistic-regression/cost-function.png"><h5 id="简化损失函数">简化损失函数</h5><p>上面对数形式损失函数是分段形式的，我们可以将两个式子压缩成一个式子： <span class="math display">\[Cost(h_\theta (x), y) = -ylog(h_\theta (x)) -(1-y)log(1-h_\theta (x))\]</span></p><p>当 y=0 时，取后半段；当 y=1 时，取前半段。</p><p>由此，我们终于得到了 Logistic 回归的目标函数<span class="math inline">\(J(\theta)\)</span>： <span class="math display">\[\begin{align} J(\theta) &amp; = \frac{1}{m} \sum_{i=1}^m Cost(h_\theta(x^{(i)}), y^{(i)}) \\\ &amp; = -\frac{1}{m} [\sum_{i=1}^m y^{(i)}log h_\theta(x^{(i)}) + (1-y^{(i)})log (1-h_\theta (x^{(i)}))] \\\ \end{align}\]</span></p><h3 id="优化目标函数求参">4.3 优化目标函数求参</h3><p>优化目标函数：<span class="math inline">\(\min_{\theta} J(\theta)\)</span>，即可得到参数 <span class="math inline">\(\theta\)</span></p><p>那么，如何优化目标函数呢？优化方法有很多种，这里讲一下“梯度下降法”：</p><h4 id="梯度下降法gradient-descent">4.3.1 梯度下降法（Gradient Descent）</h4><p>梯度下降法的原理这里不详细解释了，方法比较直观，网上有很多教程可以参考。</p><p>梯度下降法的使用很简单：在目标函数上任找一点开始，让参数 <span class="math inline">\(\theta\)</span> 不断朝着梯度方向迭代，直到收敛，收敛时函数位于极小值处，此时的 <span class="math inline">\(\theta\)</span> 即为 <span class="math inline">\(\min_{\theta} J(\theta)\)</span>。</p><p>每一步迭代的形式化定义如下： <span class="math display">\[\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)\]</span></p><p>这里引入了一个新的参数：<span class="math inline">\(\alpha\)</span>，表示迭代速度，在这里作为调控因子。另外式子中 <span class="math inline">\(J(\theta)\)</span> 关于 <span class="math inline">\(\theta\)</span> 的梯度可以计算得到： <span class="math display">\[\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^m(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}\]</span></p><p>此外，还要注意的是，梯度下降法迭代时，是所有参数：<span class="math inline">\(\theta_0, \theta_1, \cdots, \theta_n\)</span> 同时迭代的，这个可以以向量形式进行批量计算。</p><p>在梯度下降中，需要计算<span class="math inline">\(\sum_{i=1}^m (h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}\)</span>，也就是每一个样本 <span class="math inline">\(x^{(i)}\)</span> 都要参与计算。这样在样本量较大时，难免效率底下。有一些改进的方法来解决这个问题，例如随机梯度下降法等，这里就不展开了。</p><h2 id="五用求得的参数进行分类">五、用求得的参数进行分类</h2><p>使用求得的参数 <span class="math inline">\(\theta\)</span>，进而预测新的未知变量 <span class="math inline">\(x\)</span>： <span class="math display">\[h_\theta(x)=\frac{1}{1+e^{-\theta ^\top x}}\]</span> 之前提过了，这个 <span class="math inline">\(h_\theta(x)\)</span> 直观意义为：给定 x，其类别 y=1 的估计概率，即<span class="math inline">\(h_\theta (x)=P(y=1 \mid x;\theta )\)</span> 因此，我们有了 <span class="math inline">\(h_\theta(x)\)</span>，就能确定未知样本的分类了。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Mengqi</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="/http:/mengqi92.github.io/2015/10/05/logistic-regression/" title="Logistic 回归">http://mengqi92.github.io/2015/10/05/logistic-regression/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLzQuMC9udWxs"><i class="fa fa-fw fa-creative-commons"></i>BY-NC</span> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/Logistic-%E5%9B%9E%E5%BD%92/" rel="tag"># Logistic 回归</a> <a href="/tags/Logistic-Regression/" rel="tag"># Logistic Regression</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2015/10/03/think-statistics-note/" rel="prev" title="《统计思维：程序员数学之概率统计》读书摘录"><i class="fa fa-chevron-left"></i> 《统计思维：程序员数学之概率统计》读书摘录</a></div><div class="post-nav-item"><a href="/2015/10/06/complex/" rel="next" title="复数的几种表示形式">复数的几种表示形式 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一问题的引入"><span class="nav-number">1.</span> <span class="nav-text">一、问题的引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二logistic-函数logistic-function"><span class="nav-number">2.</span> <span class="nav-text">二、Logistic 函数（Logistic Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三假设函数的直观解释"><span class="nav-number">3.</span> <span class="nav-text">三、假设函数的直观解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四寻求优化参数-theta"><span class="nav-number">4.</span> <span class="nav-text">四、寻求优化参数 \(\theta\)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归的目标函数"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 线性回归的目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-回归的目标函数"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Logistic 回归的目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#平方形式的损失函数"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 平方形式的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对数形式的损失函数"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 对数形式的损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#简化损失函数"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">简化损失函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化目标函数求参"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 优化目标函数求参</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法gradient-descent"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.3.1 梯度下降法（Gradient Descent）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五用求得的参数进行分类"><span class="nav-number">5.</span> <span class="nav-text">五、用求得的参数进行分类</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Mengqi" src="https://mengqistatic.azureedge.net/staticfiles/avatar/astronaut.png"><p class="site-author-name" itemprop="name">Mengqi</p><div class="site-description" itemprop="description">机器学习、图像处理、统计分析、函数式编程</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">29</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbmdxaTky" title="Github → https:&#x2F;&#x2F;github.com&#x2F;mengqi92"><i class="fa fa-fw fa-github"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOm1lbmdxaXBlaUBnbWFpbC5jb20=" title="E-Mail → mailto:mengqipei@gmail.com"><i class="fa fa-fw fa-envelope"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9tZW5ncWlwZWk=" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;mengqipei"><i class="fa fa-fw fa-twitter"></i></span> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLzQuMC9udWxs"><img src="/images/cc-by-nc.svg" alt="Creative Commons"></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><span class="exturl" data-url="aHR0cDovL3d3dy5mdXppaGFvLm9yZw==" title="http:&#x2F;&#x2F;www.fuzihao.org">切问录</span></li><li class="links-of-blogroll-item"><span class="exturl" data-url="aHR0cHM6Ly9idXdlbnFpLmdpdGh1Yi5pbw==" title="https:&#x2F;&#x2F;buwenqi.github.io">Wenqi's Blog</span></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Mengqi</span></div><div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> 强力驱动 v4.0.0</div><span class="post-meta-divider">|</span><div class="theme-info">主题 – <span class="exturl theme-link" data-url="aHR0cHM6Ly9taXN0LnRoZW1lLW5leHQub3Jn">NexT.Mist</span> v7.5.0</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script></body></html>