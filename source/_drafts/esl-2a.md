title: 《统计学习基础》读书总结 - 第二章（上）
tags:
  - 机器学习
  - 统计学习
  - 读书笔记
  - 统计学习基础
  - ESL
category: 机器学习
mathjax: true
date: 2015-10-13 15:59:45
---

# 前言

这本《统计学习基础（Elements of statistical Learning）》是我购买的第一本全英文技术书籍，书中的插图丰富而且美观，讲解有了图示之后也变得明白易懂……李航博士的《统计学习方法》一书中，引用到了这本《统计学习基础》然而由于各种原因，之前只是零碎地读过几章，并没有完整系统地阅读过。这次终于下定决心要详细阅读一遍，并将学习的心得总结在这里。

<!-- more -->

# 一、监督学习总览

监督学习中，从输出的类别来看，我们可以将学习问题分为以下几类：
- 输出为连续的数量（quantitative），此类问题被称为回归问题（regression）；
- 输出为离散的类别（qualitative），此类问题被称为分类问题（classification）；
- 输出为有序类别（ordered categorical）
    - 二类问题，可将输出编码为 $\{ 0, 1 \}$；
    - 多类问题，例如{小, 中, 大}三类，此时可将输出按“虚拟变量（dummy variables）”形式编码：
    $$\begin{cases}
    100 &: \text{小} \\\
    010 &: \text{中} \\\
    001 &: \text{大} \\\
    \end{cases}$$

# 二、最小二乘法和最近邻法

这节从两个基本预测算法开始介绍：最小二乘法和最近邻法。

## 2.1 最小二乘法
线性模型与最小二乘法（Linear Models and Least Squares）
给定输入 $X = (X\_1, X\_2, \cdots, X\_p)$，我们可以建立线性模型来预测输出 $Y$：
$$ \hat{Y} = \hat{\beta} + \sum\_{j=1}^p{X\_j \hat{\beta\_j} } \tag{1}\label{leastsquare}$$
其中，$\hat{\beta\_0}$就是截距，在机器学习中也被称为“偏差（bias）”。为了方便，我们常把 $\hat{\beta\_0}$ 包含进 $\hat{\beta}$中，相应地，在 $X$ 中添加一个常量 1（$\ref{leastsquare}$式子右边展开是$ 1 \times \hat{\beta\_0} + x\_1 \hat{\beta\_1} + \cdots + x\_p \hat{\beta\_p} $），这样就能把整个线性模型写为一个内积的形式了：
$$ \hat{Y} = X^\top \hat{\beta} $$

注意，这里线性模型不只有一维情况，$\hat{Y}$可以是一个K维向量，此时 $\beta$ 是一个 p\times K 的协方差矩阵，（X, \hat{Y}）表示一个超平面。

从 $p$ 维输入空间来看，$f(X)=X^\top \beta$ 是线性的，而$f'(X) = \beta$将指向坡度最陡的方向。

接下来用最小平方误差法来拟合模型：
$$RSS(\beta) = \sum\_{i=1}^N (y\_i - x\_i^\top \beta)^2$$
用矩阵形式描述就是：
$$RSS(\beta) = (\mathbf{y}-\mathbf{X}\beta)^\top (\mathbf{y}-\mathbf{X}\beta)$$
其中，$\mathbf{X}$为$N \times p$的矩阵，每一行为一个输入向量，$\mathbf{y}$为$N$维输出向量。

我们拟合模型就需要最小化误差：$RSS(\beta)$，令$\frac{\partial RSS(\beta)}{\beta}=0$得到：
$$\mathbf{X}^\top (\mathbf{y}-\mathbf{X}\beta)$$

继而解出误差最小时的$\beta$：
$$\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

有了$\beta$，我们的模型也就确定了，从而就可以预测任意输入变量 $x\_0$ 的类别：
$$\hat{y}(x\_0) = x\_0^\top \hat{\beta}$$

在二分类问题中，只有$y=0 或 y=1$两种情况，而用线性模型得到的输出向量范围是连续的值，此时可以定义当$\hat{y}>0.5$时，分类为$y=1$类；而当$\hat{y}<0.5$时，分类为$y=0$类。

## 2.2 最近邻法
最近邻法使用训练集中，$x$的最近邻观察点来估计分类输出$\hat{Y}$：
$$ \hat{Y} = \frac{1}{n} \sum\_{x\_i \in N\_k(x)}{y\_i} $$
其中，$N\_k(x)$是$x$的$k$近邻点。计算出$\hat{Y}$后即可判定类别：当 $\hat{Y} > 0.5$ 时判为一类；当 $\hat{Y} < 0.5$ 时判为另一类。

近邻的定义需要使用距离。距离一般有欧氏距离、马氏距离、曼哈顿距离、切比雪夫距离等等，这里不再展开。

最近邻法分类中，随着 k 的增大，其分类面将更“泛化”，在训练集上的错误率也会提高；相反，当 k=1 时，其训练集错误率为 0（每类就取其自身的类别）。

另外，最近邻法中只需要 1 个参数：$k$。*参数的有效数量(effective number of parameters)*为 $N/k$，并随着 $k$ 增大而减小（参数的有效数量**我暂时还没有很好地理解**。书上是这样介绍的：当 $k$ 个类别互不相交时，邻域数可为 $N/k$，从而每个邻域取一个均值即可代表这个类别）。

> 参数的有效数量

最近邻法中，不能用训练集上的均方误差来训练得到 $k$，因为这样的话我们将总会得到 $k=1$！

## 2.3 最小二乘法和最近邻法的比较
* 最小二乘法得到的线性决策面较为平滑，而且与数据的拟合更为稳定；然而它要求数据本身应该是适合线性决策面的，也就是说，它会有较小的variance，而可能有较大的 bia也就是说，它会有较小的方差（variance），而可能有较大的偏差（bias）。

    这里需要指出，方差（variance）是指模型假设与训练样本之间的差别；而偏差（bias）是指模型假设与真实情况之间的差别。比如说我们现在有一组样本，用线性决策面能很好地拟合，这时，variance就比较小；而如果样本所代表的数据其实并非线性，那么这个决策面就不能很好地预测样本外的真实数据，bias就比较大了。

* K近邻法对数据的分布要求不高，没有很强的假设，因而适用于各种情况。然而，这个方法得到的决策面受输入样本点的分布影响较大，因而较为不稳定。即variance较大而bias较小。

## 2.4 最近邻法的几种改进

* 通过核方法为样本点的邻居赋予不同的权重，离样本点越远，权重越小。
* 在高维空间中，对距离核进行一些调整，以强调某些变量。
* 局部回归：通过局部加权最小二乘法拟合，而非局部拟合常量（fitting constants locally？）
* 对原始输入点进行扩展以使得线性模型能拟合任意复杂模型。
* Projection pursuit 以及神经网络模型由非线性转换而来的线性模型组合而成。

# 三、统计决策理论
统计决策理论为统计机器学习提供了一个框架：

## 3.1 连续形式的输出（quantitative output）
首先，用 $X\in \mathbb{R}^p$ 表示随机输入向量，$Y\in \mathbb{R}$表示随机输出变量，$X$和$Y$联合概率：$Pr(X,Y)$。
我们的目标就是，寻找一个函数$f(X)$，使得从输入$X$的值能预测出$Y$。
这个理论需要一个损失函数（loss function）来惩罚预测的错误：$L(Y, f(X))$。

由以上的铺垫，我们就有函数$f(x)$的解：
$$ f(x) = E(Y \mid X=x) $$
也就是说，$f(x)$是样本$X=x$条件下，输出值的期望（这也叫回归函数）。

下面我们结合之前学习的两个方法来看一下这个理论框架是如何应用的。

### 3.1.1 K-近邻方法

从这个理论来看K-近邻方法，就会发现，K-近邻实际上就是直接用训练数据来得$f(x)$的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行分类决策：
$$ \hat{f}(x) = Ave(y\_i \mid x\_i \in N\_k(x)) $$
从这个理论再来看K-近邻方法，就能发现，K-近邻实际上就是直接用训练数据来得到f(x)的。由于每个点最多只对应一个输出，为了算法的稳定性，K-近邻选择了一个邻域范围，从这个范围中的条件期望来进行类别决策：

总结一下K-近邻的两个特点：
- **K-近邻是通过样本来估计期望的，是基于样本的方法**；
- K-近邻是把一个点上的条件概率放宽到了一个邻域上面。

理论上，当样本数$N$和类别数$k$都趋向于无穷大时，$k/N \rightarrow 0$，$\hat{f}(x) \rightarrow E(Y\mid X=x)$。然而实际上，随着维数$p$增大，K-近邻中对“近邻”的测量复杂度也会随之增大，虽然$Ave(y\_i \mid x\_i \in N\_k(x))$仍会继续收敛，但收敛的速度会随着维数增高而变慢。

### 3.1.2 线性回归方法

再来看线性回归，它实际上做了一个假设，假设$f(x)$是近似线性的：
$$f(x) \approx x^\top \beta$$
也就是说**线性回归是基于模型的方法**。

比如一个用平方误差作为损失函数的线性回归，它的期望预测误差（Expected Prediction Error）为：
$$\begin{align}
EPE(f)  & = & E(Y - f(X))^2 \\\
        & = & \int (y-f(x))^2 Pr(dx, dy)
\end{align}
\tag{2}\label{2}$$
我们可以将 $f(x)$ 代入，解得：
$$ \theta = [E(XX^\top)]^{-1} E(XY) $$
而最小二乘法实际上就是将$\ref{2}$式中的期望用样本的均值代替。

我们将$\ref{2}$式的联合概率改写为在$X$上的条件概率，得到：
$$EPE(f) = E\_X E\_Y ([Y - f(X)]^2 \mid X)$$

上式是采用平方误差（$L\_2$）作为损失函数的结果，那如果我们改用$L\_1$，结果就会是中值形式了：
$$ \hat{f}(x) = \operatorname{median}(Y \mid X=x) $$

**中值形式相较于平方形式，对离群点更加稳健。然而 $L\_1$ 导数有不连续情况，因而没有得到广泛应用。**

## 3.2 不连续形式的输出（categorical output）
预测不连续形式输出的基本模式和连续形式的差不多，只是需要用不同的损失函数来惩罚预测误差。

用$G$表示输出的随机变量，$\mathcal{G}$表示所有可能的输出，$\hat{G}$表示估计的可能类别集合。我们的损失函数可以用一个 $K\times K$ 矩阵 $L$ 描述，这里的 $K = \lvert \mathcal{G} \rvert$，$L$上的元素$L(k,l)$表示将$\mathcal{G}\_k$分类为$\mathcal{G}\_l$的代价。

根据这个损失函数，我们有：
$$EPE = E[L(G, \hat{G}(X))]$$
改写为条件概率的形式就是：
$$EPE = E\_X \sum\_{k=1}^{K} L[\mathcal{G}\_k, \hat{G}(X)] Pr(\mathcal{G}\_k \mid X)$$
逐点最小化EPE，就得到了估计输出：
$$\hat{G}(x) = \operatorname{argmin}\_{g\in \mathcal{G}} \sum\_{k=1}^K L(\mathcal{G}\_k, g) Pr(\mathcal{G}\_k \mid X = x)$$

### 3.2.1 贝叶斯分类器

如果我们采用 0-1 损失函数（即分错代价为1，分对为0）的话，可以进一步将上式简化为：
$$\hat{G}(x) = \operatorname{argmin}\_{g\in \mathcal{G}} [1-Pr(g \mid X=x)]$$

上面的式子等价于 $\operatorname{argmax}\_{g\in \mathcal{G}} Pr(g \mid X=x)$。也就是说在给定 $X=x$ 条件下，类别$G\_k$概率最大，那就预测输出$G\_k$。这个实际上就是贝叶斯分类器了。

### 3.2.2 K-近邻与贝叶斯分类器的关系

K-近邻分类类似于贝叶斯分类。他们的区别只在于：
- 贝叶斯分类是考虑某一个点的条件概率，而K-近邻将其放宽到一个邻域了；
- 贝叶斯分类器中的概率在K-近邻中用邻域中训练样本的比例来得到。
